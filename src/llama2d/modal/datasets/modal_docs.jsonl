{"text": "# Hyperparameter search\nThis example showcases a simple grid search in one dimension, where we try different\nparameters for a model and pick the one with the best results on a holdout set.\n## Defining the image\nFirst, let's build a custom image and install scikit-learn in it.\n```python\nimport modal\nstub = modal.Stub(\n    \"example-basic-grid-search\",\n    image=modal.Image.debian_slim().pip_install(\"scikit-learn~=1.2.2\"),\n)\n```\n## The Modal function\nNext, define the function. Note that we use the custom image with scikit-learn in it.\nWe also take the hyperparameter `k`, which is how many nearest neighbors we use.\n```python\n@stub.function()\ndef fit_knn(k):\n    from sklearn.datasets import load_digits\n    from sklearn.model_selection import train_test_split\n    from sklearn.neighbors import KNeighborsClassifier\n    X, y = load_digits(return_X_y=True)\n    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n    clf = KNeighborsClassifier(k)\n    clf.fit(X_train, y_train)\n    score = float(clf.score(X_test, y_test))\n    print(\"k = %3d, score = %.4f\" % (k, score))\n    return score, k\n```\n## Parallel search\nTo do a hyperparameter search, let's map over this function with different values\nfor `k`, and then select for the best score on the holdout set:\n```python\n@stub.local_entrypoint()\ndef main():\n    # Do a basic hyperparameter search\n    best_score, best_k = max(fit_knn.map(range(1, 100)))\n    print(\"Best k = %3d, score = %.4f\" % (best_k, best_score))\n```\n"}
{"text": "\n# Finetuning Flan-T5\nExample by [@anishpdalal](https://github.com/anishpdalal)\n[Flan-T5](https://huggingface.co/docs/transformers/model_doc/flan-t5) is a highly versatile model that's been instruction-tuned to\nperform well on a variety of text-based tasks such as question answering and summarization. There are smaller model variants available which makes\nFlan-T5 a great base model to use for finetuning on a specific instruction dataset with just a single GPU. In this example, we'll\nfinetune Flan-T5 on the [Extreme Sum (\"XSum\")](https://huggingface.co/datasets/xsum) dataset to summarize news articles.\n## Defining dependencies\nThe example uses the `dataset` package from HuggingFace to load the xsum dataset. It also uses the `transformers`\nand `accelerate` packages with a PyTorch backend to finetune and serve the model. Finally, we also\ninstall `tensorboard` and serve it via a web app. All packages are installed into a Debian Slim base image\nusing the `pip_install` function.\n```python\nfrom pathlib import Path\nimport modal\nfrom modal import Image, Stub, Volume, method, wsgi_app\nVOL_MOUNT_PATH = Path(\"/vol\")\n```\nOther Flan-T5 models can be found [here](https://huggingface.co/docs/transformers/model_doc/flan-t5)\n```python\nBASE_MODEL = \"google/flan-t5-base\"\nimage = Image.debian_slim().pip_install(\n    \"accelerate\",\n    \"transformers\",\n    \"torch\",\n    \"datasets\",\n    \"tensorboard\",\n)\nstub = Stub(name=\"example-news-summarizer\", image=image)\noutput_vol = Volume.persisted(\"finetune-volume\")\nstub.volume = output_vol\n```\n### Handling preemption\nAs this finetuning job is long-running it's possible that it experiences a preemption.\nThe training code is robust to pre-emption events by periodically saving checkpoints and restoring\nfrom checkpoint on restart. But it's also helpful to observe in logs when a preemption restart has occurred,\nso we track restarts with a `modal.Dict`.\nSee the [guide on preemptions](/docs/guide/preemption#preemption) for more details on preemption handling.\n```python\nstub.restart_tracker_dict = modal.Dict.new()\ndef track_restarts(restart_tracker: modal.Dict):\n    if not restart_tracker.contains(\"count\"):\n        preemption_count = 0\n        print(f\"Starting first time. {preemption_count=}\")\n        restart_tracker[\"count\"] = preemption_count = 0\n    else:\n        preemption_count = restart_tracker.get(\"count\") + 1\n        print(f\"Restarting after pre-emption. {preemption_count=}\")\n        restart_tracker[\"count\"] = preemption_count\n```\n## Finetuning Flan-T5 on XSum dataset\nEach row in the dataset has a `document` (input news article) and `summary` column.\n```python\n@stub.function(\n    gpu=\"A10g\",\n    timeout=7200,\n    volumes={VOL_MOUNT_PATH: output_vol},\n)\ndef finetune(num_train_epochs: int = 1, size_percentage: int = 10):\n    from datasets import load_dataset\n    from transformers import (\n        AutoModelForSeq2SeqLM,\n        AutoTokenizer,\n        DataCollatorForSeq2Seq,\n        Seq2SeqTrainer,\n        Seq2SeqTrainingArguments,\n        TrainerCallback,\n    )\n    track_restarts(stub.restart_tracker_dict)\n    # Use size percentage to retrieve subset of the dataset to iterate faster\n    if size_percentage:\n        xsum_train = load_dataset(\"xsum\", split=f\"train[:{size_percentage}%]\")\n        xsum_test = load_dataset(\"xsum\", split=f\"test[:{size_percentage}%]\")\n    # Load the whole dataset\n    else:\n        xsum = load_dataset(\"xsum\")\n        xsum_train = xsum[\"train\"]\n        xsum_test = xsum[\"test\"]\n    # Load the tokenizer and model\n    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n    model = AutoModelForSeq2SeqLM.from_pretrained(BASE_MODEL)\n    # Replace all padding tokens with a large negative number so that the loss function ignores them in\n    # its calculation\n    padding_token_id = -100\n    batch_size = 8\n    def preprocess(batch):\n        # prepend summarize: prefix to document to convert the example to a summarization instruction\n        inputs = [\"summarize: \" + doc for doc in batch[\"document\"]]\n        model_inputs = tokenizer(\n            inputs, max_length=512, truncation=True, padding=\"max_length\"\n        )\n        labels = tokenizer(\n            text_target=batch[\"summary\"],\n            max_length=128,\n            truncation=True,\n            padding=\"max_length\",\n        )\n        labels[\"input_ids\"] = [\n            [\n                l if l != tokenizer.pad_token_id else padding_token_id\n                for l in label\n            ]\n            for label in labels[\"input_ids\"]\n        ]\n        model_inputs[\"labels\"] = labels[\"input_ids\"]\n        return model_inputs\n    tokenized_xsum_train = xsum_train.map(\n        preprocess, batched=True, remove_columns=[\"document\", \"summary\", \"id\"]\n    )\n    tokenized_xsum_test = xsum_test.map(\n        preprocess, batched=True, remove_columns=[\"document\", \"summary\", \"id\"]\n    )\n    data_collator = DataCollatorForSeq2Seq(\n        tokenizer,\n        model=model,\n        label_pad_token_id=padding_token_id,\n        pad_to_multiple_of=batch_size,\n    )\n    class CheckpointCallback(TrainerCallback):\n        def __init__(self, volume):\n            self.volume = volume\n        def on_save(self, args, state, control, **kwargs):\n            \"\"\"\n            Event called after a checkpoint save.\n            \"\"\"\n            if state.is_world_process_zero:\n                print(\"running commit on modal.Volume after model checkpoint\")\n                self.volume.commit()\n    training_args = Seq2SeqTrainingArguments(\n        # Save checkpoints to the mounted volume\n        output_dir=str(VOL_MOUNT_PATH / \"model\"),\n        per_device_train_batch_size=batch_size,\n        per_device_eval_batch_size=batch_size,\n        predict_with_generate=True,\n        learning_rate=3e-5,\n        num_train_epochs=num_train_epochs,\n        logging_strategy=\"steps\",\n        logging_steps=100,\n        evaluation_strategy=\"steps\",\n        save_strategy=\"steps\",\n        save_steps=750,\n        save_total_limit=2,\n        load_best_model_at_end=True,\n    )\n    trainer = Seq2SeqTrainer(\n        model=model,\n        args=training_args,\n        callbacks=[CheckpointCallback(stub.volume)],\n        data_collator=data_collator,\n        train_dataset=tokenized_xsum_train,\n        eval_dataset=tokenized_xsum_test,\n    )\n    try:\n        trainer.train(resume_from_checkpoint=True)\n    except KeyboardInterrupt:  # handle possible preemption\n        print(\"received interrupt; saving state and model\")\n        trainer.save_state()\n        trainer.save_model()\n        raise\n    # Save the trained model and tokenizer to the mounted volume\n    model.save_pretrained(str(VOL_MOUNT_PATH / \"model\"))\n    tokenizer.save_pretrained(str(VOL_MOUNT_PATH / \"tokenizer\"))\n    stub.volume.commit()\n    print(\"\u2705 done\")\n```\n## Monitoring Finetuning with Tensorboard\nTensorboard is an application for visualizing training loss. In this example we\nserve it as a Modal WSGI app.\n```python\n@stub.function(volumes={VOL_MOUNT_PATH: output_vol})\n@wsgi_app()\ndef monitor():\n    import tensorboard\n    board = tensorboard.program.TensorBoard()\n    board.configure(logdir=f\"{VOL_MOUNT_PATH}/logs\")\n    (data_provider, deprecated_multiplexer) = board._make_data_provider()\n    wsgi_app = tensorboard.backend.application.TensorBoardWSGIApp(\n        board.flags,\n        board.plugin_loaders,\n        data_provider,\n        board.assets_zip_provider,\n        deprecated_multiplexer,\n    )\n    return wsgi_app\n```\n## Model Inference\n```python\n@stub.cls(volumes={VOL_MOUNT_PATH: output_vol})\nclass Summarizer:\n    def __enter__(self):\n        from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, pipeline\n        # Load saved tokenizer and finetuned from training run\n        tokenizer = AutoTokenizer.from_pretrained(\n            BASE_MODEL, cache_dir=VOL_MOUNT_PATH / \"tokenizer/\"\n        )\n        model = AutoModelForSeq2SeqLM.from_pretrained(\n            BASE_MODEL, cache_dir=VOL_MOUNT_PATH / \"model/\"\n        )\n        self.summarizer = pipeline(\n            \"summarization\", tokenizer=tokenizer, model=model\n        )\n    @method()\n    def generate(self, input: str) -> str:\n        return self.summarizer(input)[0][\"summary_text\"]\n@stub.local_entrypoint()\ndef main():\n    input = \"\"\"\n    The 14-time major champion, playing in his first full PGA Tour event for almost 18 months,\n    carded a level-par second round of 72, but missed the cut by four shots after his first-round 76.\n    World number one Jason Day and US Open champion Dustin Johnson also missed the cut at Torrey Pines in San Diego.\n    Overnight leader Rose carded a one-under 71 to put him on eight under. Canada's\n    Adam Hadwin and USA's Brandt Snedeker are tied in second on seven under, while US PGA champion\n    Jimmy Walker missed the cut as he finished on three over. Woods is playing in just his\n    second tournament since 15 months out with a back injury. \"It's frustrating not being\n    able to have a chance to win the tournament,\" said the 41-year-old, who won his last major,\n    the US Open, at the same course in 2008. \"Overall today was a lot better than yesterday.\n    I hit it better, I putted well again. I hit a lot of beautiful putts that didn't go in, but\n    I hit it much better today, which was nice.\" Scotland's Martin Laird and England's Paul Casey\n    are both on two under, while Ireland's Shane Lowry is on level par.\n    \"\"\"\n    model = Summarizer()\n    response = model.generate.remote(input)\n    print(response)\n```\n## Run via the CLI\nInvoke model finetuning use the provided command below\n```bash\nmodal run --detach flan_t5_finetune.py::finetune --num-train-epochs=1 --size-percentage=10\nView the tensorboard logs at https://<username>--example-news-summarizer-monitor-dev.modal.run\n```\nInvoke finetuned model inference via local entrypoint\n```bash\nmodal run flan_t5_finetune.py\nWorld number one Tiger Woods missed the cut at the US Open as he failed to qualify for the final round of the event in Los Angeles.\n```\n"}
{"text": "\n# Question-answering with LangChain\nIn this example we create a large-language-model (LLM) powered question answering\nweb endpoint and CLI. Only a single document is used as the knowledge-base of the application,\nthe 2022 USA State of the Union address by President Joe Biden. However, this same application structure\ncould be extended to do question-answering over all State of the Union speeches, or other large text corpuses.\nIt's the [LangChain](https://github.com/hwchase17/langchain) library that makes this all so easy. This demo is only around 100 lines of code!\n## Defining dependencies\nThe example uses three PyPi packages to make scraping easy, and three to build and run the question-answering functionality.\nThese are installed into a Debian Slim base image using the `pip_install` function.\nBecause OpenAI's API is used, we also specify the `openai-secret` Modal Secret, which contains an OpenAI API key.\nA `docsearch` global variable is also declared to facilitate caching a slow operation in the code below.\n```python\nfrom pathlib import Path\nfrom modal import Image, Secret, Stub, web_endpoint\nimage = Image.debian_slim().pip_install(\n    # scraping pkgs\n    \"beautifulsoup4~=4.11.1\",\n    \"httpx~=0.23.3\",\n    \"lxml~=4.9.2\",\n    # langchain pkgs\n    \"faiss-cpu~=1.7.3\",\n    \"langchain~=0.0.138\",\n    \"openai~=0.27.4\",\n    \"tiktoken==0.3.0\",\n)\nstub = Stub(\n    name=\"example-langchain-qanda\",\n    image=image,\n    secrets=[Secret.from_name(\"openai-secret\")],\n)\ndocsearch = None  # embedding index that's relatively expensive to compute, so caching with global var.\n```\n## Scraping the speech from whitehouse.gov\nIt's super easy to scrape the transcipt of Biden's speech using `httpx` and `BeautifulSoup`.\nThis speech is just one document and it's relatively short, but it's enough to demonstrate\nthe question-answering capability of the LLM chain.\n```python\ndef scrape_state_of_the_union() -> str:\n    import httpx\n    from bs4 import BeautifulSoup\n    url = \"https://www.whitehouse.gov/state-of-the-union-2022/\"\n    # fetch article; simulate desktop browser\n    headers = {\n        \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_2) AppleWebKit/601.3.9 (KHTML, like Gecko) Version/9.0.2 Safari/601.3.9\"\n    }\n    response = httpx.get(url, headers=headers)\n    soup = BeautifulSoup(response.text, \"lxml\")\n    # get all text paragraphs & construct string of article text\n    speech_text = \"\"\n    speech_section = soup.find_all(\n        \"div\", {\"class\": \"sotu-annotations__content\"}\n    )\n    if speech_section:\n        paragraph_tags = speech_section[0].find_all(\"p\")\n        speech_text = \"\".join([p.get_text() for p in paragraph_tags])\n    return speech_text.replace(\"\\t\", \"\")\n```\n## Constructing the Q&A chain\nAt a high-level, this LLM chain will be able to answer questions asked about Biden's speech and provide\nreferences to which parts of the speech contain the evidence for given answers.\nThe chain combines a text-embedding index over parts of Biden's speech with OpenAI's [GPT-3 LLM](https://openai.com/blog/chatgpt/).\nThe index is used to select the most likely relevant parts of the speech given the question, and these\nare used to build a specialized prompt for the OpenAI language model.\nFor more information on this, see [LangChain's \"Question Answering\" notebook](https://langchain.readthedocs.io/en/latest/use_cases/evaluation/question_answering.html).\n```python\ndef retrieve_sources(sources_refs: str, texts: list[str]) -> list[str]:\n    \"\"\"\n    Map back from the references given by the LLM's output to the original text parts.\n    \"\"\"\n    clean_indices = [\n        r.replace(\"-pl\", \"\").strip() for r in sources_refs.split(\",\")\n    ]\n    numeric_indices = (int(r) if r.isnumeric() else None for r in clean_indices)\n    return [\n        texts[i] if i is not None else \"INVALID SOURCE\" for i in numeric_indices\n    ]\ndef qanda_langchain(query: str) -> tuple[str, list[str]]:\n    from langchain.chains.qa_with_sources import load_qa_with_sources_chain\n    from langchain.embeddings.openai import OpenAIEmbeddings\n    from langchain.llms import OpenAI\n    from langchain.text_splitter import CharacterTextSplitter\n    from langchain.vectorstores.faiss import FAISS\n    # Support caching speech text on disk.\n    speech_file_path = Path(\"state-of-the-union.txt\")\n    if speech_file_path.exists():\n        state_of_the_union = speech_file_path.read_text()\n    else:\n        print(\"scraping the 2022 State of the Union speech\")\n        state_of_the_union = scrape_state_of_the_union()\n        speech_file_path.write_text(state_of_the_union)\n    # We cannot send the entire speech to the model because OpenAI's model\n    # has a maximum limit on input tokens. So we split up the speech\n    # into smaller chunks.\n    text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n    print(\"splitting speech into text chunks\")\n    texts = text_splitter.split_text(state_of_the_union)\n    # Embedding-based query<->text similarity comparison is used to select\n    # a small subset of the speech text chunks.\n    # Generating the `docsearch` index is too slow to re-run on every request,\n    # so we do rudimentary caching using a global variable.\n    global docsearch\n    if not docsearch:\n        # New OpenAI accounts have a very low rate-limit for their first 48 hrs.\n        # It's too low to embed even just this single Biden speech.\n        # The `chunk_size` parameter is set to a low number, and internally LangChain\n        # will retry the embedding requests, which should be enough to handle the rate-limiting.\n        #\n        # Ref: https://platform.openai.com/docs/guides/rate-limits/overview.\n        print(\"generating docsearch indexer\")\n        docsearch = FAISS.from_texts(\n            texts,\n            OpenAIEmbeddings(chunk_size=5),\n            metadatas=[{\"source\": i} for i in range(len(texts))],\n        )\n    print(\"selecting text parts by similarity to query\")\n    docs = docsearch.similarity_search(query)\n    chain = load_qa_with_sources_chain(\n        OpenAI(temperature=0), chain_type=\"stuff\"\n    )\n    print(\"running query against Q&A chain.\\n\")\n    result = chain(\n        {\"input_documents\": docs, \"question\": query}, return_only_outputs=True\n    )\n    output: str = result[\"output_text\"]\n    parts = output.split(\"SOURCES: \")\n    if len(parts) == 2:\n        answer, sources_refs = parts\n        sources = retrieve_sources(sources_refs, texts)\n    elif len(parts) == 1:\n        answer = parts[0]\n        sources = []\n    else:\n        raise RuntimeError(\n            f\"Expected to receive an answer with a single 'SOURCES' block, got:\\n{output}\"\n        )\n    return answer.strip(), sources\n```\n## Modal Functions\nWith our application's functionality implemented we can hook it into Modal.\nAs said above, we're implementing a web endpoint, `web`, and a CLI command, `cli`.\n```python\n@stub.function()\n@web_endpoint(method=\"GET\")\ndef web(query: str, show_sources: bool = False):\n    answer, sources = qanda_langchain(query)\n    if show_sources:\n        return {\n            \"answer\": answer,\n            \"sources\": sources,\n        }\n    else:\n        return {\n            \"answer\": answer,\n        }\n@stub.function()\ndef cli(query: str, show_sources: bool = False):\n    answer, sources = qanda_langchain(query)\n    # Terminal codes for pretty-printing.\n    bold, end = \"\\033[1m\", \"\\033[0m\"\n    print(f\"\ud83e\udd9c {bold}ANSWER:{end}\")\n    print(answer)\n    if show_sources:\n        print(f\"\ud83d\udd17 {bold}SOURCES:{end}\")\n        for text in sources:\n            print(text)\n            print(\"----\")\n```\n## Test run the CLI\n```bash\nmodal run potus_speech_qanda.py --query \"What did the president say about Justice Breyer\"\n\ud83e\udd9c ANSWER:\nThe president thanked Justice Breyer for his service and mentioned his legacy of excellence. He also nominated Ketanji Brown Jackson to continue in Justice Breyer's legacy.\n```\nTo see the text of the sources the model chain used to provide the answer, set the `--show-sources` flag.\n```bash\nmodal run potus_speech_qanda.py \\\n   --query \"How many oil barrels were released from reserves\" \\\n   --show-sources=True\n```\n## Test run the web endpoint\nModal makes it trivially easy to ship LangChain chains to the web. We can test drive this app's web endpoint\nby running `modal serve potus_speech_qanda.py` and then hitting the endpoint with `curl`:\n```bash\ncurl --get \\\n  --data-urlencode \"query=What did the president say about Justice Breyer\" \\\n  https://modal-labs--example-langchain-qanda-web.modal.run\n```\n```json\n{\n  \"answer\": \"The president thanked Justice Breyer for his service and mentioned his legacy of excellence. He also nominated Ketanji Brown Jackson to continue in Justice Breyer's legacy.\"\n}\n```\n"}
{"text": "\n```python\nimport fastapi\nfrom modal import Image, Stub, asgi_app\nfrom modal.functions import FunctionCall\nfrom starlette.responses import HTMLResponse, RedirectResponse\nstub = Stub(\"example-poll\")\nweb_app = fastapi.FastAPI()\n@stub.function(image=Image.debian_slim().pip_install(\"primefac\"))\ndef factor_number(number):\n    import primefac\n    return list(primefac.primefac(number))  # could take a long time\n@web_app.get(\"/\")\nasync def index():\n    return HTMLResponse(\n        \"\"\"\n    <form method=\"get\" action=\"/factors\">\n        Enter a number: <input name=\"number\" />\n        <input type=\"submit\" value=\"Factorize!\"/>\n    </form>\n    \"\"\"\n    )\n@web_app.get(\"/factors\")\nasync def web_submit(request: fastapi.Request, number: int):\n    call = factor_number.spawn(\n        number\n    )  # returns a FunctionCall without waiting for result\n    polling_url = request.url.replace(\n        path=\"/result\", query=f\"function_id={call.object_id}\"\n    )\n    return RedirectResponse(polling_url)\n@web_app.get(\"/result\")\nasync def web_poll(function_id: str):\n    function_call = FunctionCall.from_id(function_id)\n    try:\n        result = function_call.get(timeout=0)\n    except TimeoutError:\n        result = \"not ready\"\n    return result\n@stub.function()\n@asgi_app()\ndef fastapi_app():\n    return web_app\n```\n"}
{"text": "\n# Document OCR web app\nThis tutorial shows you how to use Modal to deploy a fully serverless\n[React](https://reactjs.org/) + [FastAPI](https://fastapi.tiangolo.com/) application.\nWe're going to build a simple \"Receipt Parser\" web app that submits OCR transcription\ntasks to a separate Modal app defined in the [Job Queue\ntutorial](/docs/guide/ex/doc_ocr_jobs), polls until the task is completed, and displays\nthe results. Try it out for yourself\n[here](https://modal-labs-example-doc-ocr-webapp-wrapper.modal.run/).\n![receipt parser frontend](./receipt_parser_frontend.jpg)\n## Basic setup\nLet's get the imports out of the way and define a [`Stub`](/docs/reference/modal.Stub).\n```python\nfrom pathlib import Path\nimport fastapi\nimport fastapi.staticfiles\nfrom modal import Function, Mount, Stub, asgi_app\nstub = Stub(\"example-doc-ocr-webapp\")\n```\nModal works with any [ASGI](/docs/guide/webhooks#serving-asgi-and-wsgi-apps) or\n[WSGI](/docs/guide/webhooks#wsgi) web framework. Here, we choose to use [FastAPI](https://fastapi.tiangolo.com/).\n```python\nweb_app = fastapi.FastAPI()\n```\n## Define endpoints\nWe need two endpoints: one to accept an image and submit it to the Modal job queue,\nand another to poll for the results of the job.\nIn `parse`, we're going to submit tasks to the function defined in the [Job\nQueue tutorial](/docs/guide/ex/doc_ocr_jobs), so we import it first using\n[`Function.lookup`](/docs/reference/modal.Function#lookup).\nWe call [`.spawn()`](/docs/reference/modal.Function#spawn) on the function handle\nwe imported above, to kick off our function without blocking on the results. `spawn` returns\na unique ID for the function call, that we can use later to poll for its result.\n```python\n@web_app.post(\"/parse\")\nasync def parse(request: fastapi.Request):\n    parse_receipt = Function.lookup(\"example-doc-ocr-jobs\", \"parse_receipt\")\n    form = await request.form()\n    receipt = await form[\"receipt\"].read()  # type: ignore\n    call = parse_receipt.spawn(receipt)\n    return {\"call_id\": call.object_id}\n```\n`/result` uses the provided `call_id` to instantiate a `modal.FunctionCall` object, and attempt\nto get its result. If the call hasn't finished yet, we return a `202` status code, which indicates\nthat the server is still working on the job.\n```python\n@web_app.get(\"/result/{call_id}\")\nasync def poll_results(call_id: str):\n    from modal.functions import FunctionCall\n    function_call = FunctionCall.from_id(call_id)\n    try:\n        result = function_call.get(timeout=0)\n    except TimeoutError:\n        return fastapi.responses.JSONResponse(content=\"\", status_code=202)\n    return result\n```\nFinally, we mount the static files for our front-end. We've made [a simple React\napp](https://github.com/modal-labs/modal-examples/tree/main/09_job_queues/doc_ocr_frontend)\nthat hits the two endpoints defined above. To package these files with our app, first\nwe get the local assets path, and then create a modal [`Mount`](/docs/guide/local-data#mounting-directories)\nthat mounts this directory at `/assets` inside our container. Then, we instruct FastAPI to [serve\nthis static file directory](https://fastapi.tiangolo.com/tutorial/static-files/) at our root path.\n```python\nassets_path = Path(__file__).parent / \"doc_ocr_frontend\"\n@stub.function(\n    mounts=[Mount.from_local_dir(assets_path, remote_path=\"/assets\")]\n)\n@asgi_app()\ndef wrapper():\n    web_app.mount(\n        \"/\", fastapi.staticfiles.StaticFiles(directory=\"/assets\", html=True)\n    )\n    return web_app\n```\n## Running\nYou can run this as an ephemeral app, by running the command\n```shell\nmodal serve doc_ocr_webapp.py\n```\n## Deploy\nThat's all! To deploy your application, run\n```shell\nmodal deploy doc_ocr_webapp.py\n```\nIf successful, this will print a URL for your app, that you can navigate to from\nyour browser \ud83c\udf89 .\n![receipt parser processed](./receipt_parser_frontend_2.jpg)\n## Developing\nIf desired, instead of deploying, we can [serve](/docs/guide/webhooks#developing-with-modal-serve)\nour app ephemerally. In this case, Modal watches all the mounted files, and updates\nthe app if anything changes.\n"}
{"text": "\n# Algolia docsearch crawler\nThis tutorial shows you how to use Modal to run the [Algolia docsearch\ncrawler](https://docsearch.algolia.com/docs/legacy/run-your-own/) to index your\nwebsite and make it searchable. This is not just example code - we run the same\ncode in production to power search on this page (`Ctrl+K` to try it out!).\n## Basic setup\nLet's get the imports out of the way.\n```python\nimport json\nimport os\nimport subprocess\nfrom modal import Image, Secret, Stub, web_endpoint\n```\nModal lets you [use and extend existing Docker images](/docs/guide/custom-container#using-existing-docker-hub-images),\nas long as they have `python` and `pip` available. We'll use the official crawler image built by Algolia, with a small\nadjustment: since this image has `python` symlinked to `python3.6` and Modal is not compatible with Python 3.6, we\ninstall Python 3.8 and symlink that as the `python` executable instead.\n```python\nalgolia_image = Image.from_registry(\n    tag=\"algolia/docsearch-scraper\",\n    setup_dockerfile_commands=[\n        \"RUN apt-get update\",\n        \"RUN apt-get install -y python3.8 python3-distutils wget\",\n        \"RUN wget https://bootstrap.pypa.io/get-pip.py\",\n        \"RUN python3.8 get-pip.py\",\n        \"RUN ln --symbolic --force --no-dereference /usr/bin/python3.8 /usr/bin/python\",\n        \"ENTRYPOINT []\",\n    ],\n)\nstub = Stub(\"example-algolia-indexer\")\n```\n## Configure the crawler\nNow, let's configure the crawler with the website we want to index, and which\nCSS selectors we want to scrape. Complete documentation for crawler configuration is available\n[here](https://docsearch.algolia.com/docs/legacy/config-file).\n```python\nCONFIG = {\n    \"index_name\": \"modal_docs\",\n    \"start_urls\": [\n        {\"url\": \"https://modal.com/docs/guide\", \"page_rank\": 5},\n        {\"url\": \"https://modal.com/docs/reference\", \"page_rank\": 1},\n    ],\n    \"selectors\": {\n        \"lvl0\": \"article h1\",\n        \"lvl1\": \"article h1\",\n        \"lvl2\": \"article h2\",\n        \"lvl3\": \"article h3\",\n        \"lvl4\": \"article h4\",\n        \"text\": \"article p,article ol,article ul,article pre\",\n    },\n}\n```\n## Create an API key\nIf you don't already have one, sign up for an account on [Algolia](https://www.algolia.com/). Set up\na project and create an API key with `write` access to your index, and with the ACL permissions\n`addObject`, `editSettings` and `deleteIndex`. Now, create a secret on the Modal [Secrets](/secrets)\npage with the `API_KEY` and `APPLICATION_ID` you just created. You can name this anything you want,\nwe named it `algolia-secret`.\n## The actual function\nWe want to trigger our crawler from our CI/CD pipeline, so we're serving it as a\n[web endpoint](/docs/guide/webhooks#web_endpoint) that can be triggered by a `GET` request during deploy.\nYou could also consider running the crawler on a [schedule](/docs/guide/cron).\nThe Algolia crawler is written for Python 3.6 and needs to run in the `pipenv` created for it,\nso we're invoking it using a subprocess.\n```python\n@stub.function(\n    image=algolia_image, secrets=[Secret.from_name(\"algolia-secret\")]\n)\ndef crawl():\n    # Installed with a 3.6 venv; Python 3.6 is unsupported by Modal, so use a subprocess instead.\n    subprocess.run(\n        [\"pipenv\", \"run\", \"python\", \"-m\", \"src.index\"],\n        env={**os.environ, \"CONFIG\": json.dumps(CONFIG)},\n    )\n```\nWe want to be able to trigger this function through a webhook.\n```python\n@stub.function()\n@web_endpoint()\ndef crawl_webhook():\n    crawl.remote()\n    return \"Finished indexing docs\"\n```\n## Deploy the indexer\nThat's all the code we need! To deploy your application, run\n```shell\nmodal deploy algolia_indexer.py\n```\nIf successful, this will print a URL for your new webhook, that you can hit using\n`curl` or a browser. Logs from webhook invocations can be found from the [apps](/apps)\npage.\nThe indexed contents can be found at https://www.algolia.com/apps/APP_ID/explorer/browse/, for your\nAPP_ID. Once you're happy with the results, you can [set up the `docsearch` package with your\nwebsite](https://docsearch.algolia.com/docs/docsearch-v3/), and create a search component that uses this index.\n## Entrypoint for development\nTo make it easier to test this, we also have an entrypoint for when you run\n`modal run algolia_indexer.py`\n```python\n@stub.local_entrypoint()\ndef run():\n    crawl.remote()\n```\n"}
{"text": "\n```python\nimport time\nimport modal\nstub = modal.Stub(\n    \"example-tqdm\",\n    image=modal.Image.debian_slim().pip_install(\"tqdm\"),\n)\n@stub.function()\ndef f():\n    from tqdm import tqdm\n    for i in tqdm(range(100)):\n        time.sleep(0.1)\nif __name__ == \"__main__\":\n    with stub.run():\n        f.remote()\n```\n"}
{"text": "\n# Render a video with Blender on GPUs\nThis example shows how you can render an animated 3D scene using [Blender](https://www.blender.org/)'s Python interface.\nWe use Modal's GPU workers for this.\n## Basic setup\n```python\nimport os\nimport tempfile\nimport modal\n```\nThe S3 locations of the assets we want to render, and the frame ranges.\n```python\nSCENE_FILENAME = (\n    \"https://modal-public-assets.s3.amazonaws.com/living_room_cam.blend\"\n)\nMATERIALS_FILENAME = (\n    \"https://modal-public-assets.s3.amazonaws.com/living_room_final.mtl\"\n)\nSTART_FRAME = 32\nEND_FRAME = 34\n```\n## Defining the image\nBlender requires a very custom image in order to run properly.\nIn order to save you some time, we have precompiled the Python packages\nand stored them in a Dockerhub image.\n```python\ndockerfile_commands = [\n    \"RUN export DEBIAN_FRONTEND=noninteractive && \"\n    \"chown root:root /var /etc /usr /var/lib /var/log / && \"  # needed for some weird systemd error\n    '    echo \"deb http://deb.debian.org/debian testing main contrib non-free\" > /etc/apt/sources.list.d/testing.list && '\n    \"    apt update && \"\n    \"    apt install -yq --no-install-recommends libcrypt1 && \"\n    \"    apt install -yq --no-install-recommends\"\n    \"        libgomp1 \"\n    \"        xorg \"\n    \"        openbox \"\n    \"        xvfb \"\n    \"        libxxf86vm1 \"\n    \"        libxfixes3 \"\n    \"        libgl1\",\n    \"COPY --from=akshatb42/bpy:2.93-gpu\"\n    \"     /usr/local/lib/python3.9/dist-packages/\"\n    \"     /usr/local/lib/python3.9/site-packages/\",\n    \"RUN apt install -yq curl\",\n    f\"RUN curl -L -o scene.blend -C - '{SCENE_FILENAME}'\",\n    f\"RUN curl -L -o scene.mtl -C - '{MATERIALS_FILENAME}'\",\n]\nstub = modal.Stub(\n    \"example-blender-video\",\n    image=modal.Image.debian_slim(python_version=\"3.9\").dockerfile_commands(\n        dockerfile_commands\n    ),\n)\n```\n## Setting things up in the containers\nWe need various global configuration that we want to happen inside the containers (but not locally), such as\nenabling the GPU device.\nTo do this, we use the `stub.is_inside()` conditional, which will evaluate to `False` when the script runs\nlocally, but to `True` when imported in the cloud.\n```python\nif stub.is_inside():\n    import bpy\n    # NOTE: Blender segfaults if you try to do this after the other imports.\n    bpy.ops.wm.open_mainfile(filepath=\"/scene.blend\")\n    bpy.data.scenes[\"Scene\"].camera = bpy.data.objects.get(\"Camera.001\")\n    bpy.data.scenes[0].render.engine = \"CYCLES\"\n    # Set the device_type\n    bpy.context.preferences.addons[\n        \"cycles\"\n    ].preferences.compute_device_type = \"CUDA\"\n    # Set the device and feature set\n    bpy.context.scene.cycles.device = \"GPU\"\n    bpy.context.preferences.addons[\"cycles\"].preferences.get_devices()\n    for d in bpy.context.preferences.addons[\"cycles\"].preferences.devices:\n        d[\"use\"] = 1  # Using all devices, include GPU and CPU\n    print(\n        \"Has active device:\",\n        bpy.context.preferences.addons[\n            \"cycles\"\n        ].preferences.has_active_device(),\n    )\n    bpy.data.scenes[0].render.tile_x = 64\n    bpy.data.scenes[0].render.tile_y = 64\n    bpy.data.scenes[0].cycles.samples = 200\n```\n## Use a GPU from a Modal function\nNow, let's define the function that renders each frame in parallel.\nNote the `gpu=\"any\"` argument which tells Modal to use GPU workers.\n```python\n@stub.function(gpu=\"any\")\ndef render_frame(i):\n    print(f\"Using frame {i}\")\n    scn = bpy.context.scene\n    scn.render.resolution_x = 400\n    scn.render.resolution_y = 400\n    scn.render.resolution_percentage = 100\n    scn.frame_set(i)\n    with tempfile.NamedTemporaryFile(suffix=\".png\") as tf:\n        scn.render.filepath = tf.name\n        # Render still frame\n        bpy.ops.render.render(write_still=True)\n        with open(tf.name, \"rb\") as image:\n            img_bytes = bytearray(image.read())\n            return i, img_bytes\n```\n## Entrypoint\nThe code that gets run locally.\nNote that it doesn't require Blender present to run it.\nIn order to render in parallel, we use the `.map` method on the `render_frame` function.\nThis spins up as many workers as are needed\u2014as\nmany as one for each frame, doing everything in parallel.\n```python\nOUTPUT_DIR = \"/tmp/render\"\n@stub.local_entrypoint()\ndef main():\n    os.makedirs(OUTPUT_DIR, exist_ok=True)\n    # Render the frames in parallel using modal, and write them to disk.\n    for idx, frame in render_frame.map(range(START_FRAME, END_FRAME + 1)):\n        with open(os.path.join(OUTPUT_DIR, f\"scene_{idx:03}.png\"), \"wb\") as f:\n            f.write(frame)\n    # Stitch together frames into a gif.\n    import glob\n    from PIL import Image\n    img, *imgs = [\n        Image.open(f)\n        for f in sorted(glob.glob(os.path.join(OUTPUT_DIR, \"scene*.png\")))\n    ]\n    img.save(\n        fp=os.path.join(OUTPUT_DIR, \"scene.gif\"),\n        format=\"GIF\",\n        append_images=imgs,\n        save_all=True,\n        duration=200,\n        loop=0,\n    )\n```\n"}
{"text": "\n```python\nimport modal\nstub = modal.Stub(\"example-generators-async\")\n@stub.function()\ndef f(i):\n    for j in range(i):\n        yield j\n@stub.local_entrypoint()\nasync def run_async():\n    async for r in f.remote_gen.aio(10):\n        print(r)\n    async for r in f.map.aio(range(5)):\n        print(r)\n```\n"}
{"text": "\n```python\nimport modal\nfrom modal import wsgi_app\nstub = modal.Stub(\n    \"example-web-flask-stream\",\n    image=modal.Image.debian_slim().pip_install(\"flask\"),\n)\n@stub.function()\ndef generate_rows():\n    \"\"\"\n    This creates a large CSV file, about 10MB, which will be streaming downloaded\n    by a web client.\n    \"\"\"\n    for i in range(10_000):\n        line = \",\".join(str((j + i) * i) for j in range(128))\n        yield f\"{line}\\n\"\n@stub.function()\n@wsgi_app()\ndef flask_app():\n    from flask import Flask\n    web_app = Flask(__name__)\n    # These web handlers follow the example from\n    # https://flask.palletsprojects.com/en/2.2.x/patterns/streaming/\n    @web_app.route(\"/\")\n    def generate_large_csv():\n        # Run the function locally in the web app's container.\n        return generate_rows(), {\"Content-Type\": \"text/csv\"}\n    @web_app.route(\"/remote\")\n    def generate_large_csv_in_container():\n        # Run the function remotely in a separate container,\n        # which will stream back results to the web app container,\n        # which will stream back to the web client.\n        #\n        # This is less efficient, but demonstrates how web serving\n        # containers can be separated from and cooperate with other\n        # containers.\n        return generate_rows.remote(), {\"Content-Type\": \"text/csv\"}\n    return web_app\n```\n"}
{"text": "\n# Generate synthetic data with Jsonformer\n[Jsonformer](https://github.com/1rgs/jsonformer) is a tool that generates structured synthetic data using LLMs.\nYou provide a JSON spec and it generates a JSON object following the spec. It's a\ngreat tool for developing, benchmarking, and testing applications.\n```python\nfrom typing import Any\nimport modal\n```\nWe will be using one of [Databrick's Dolly](https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm)\nmodels, choosing for the smallest version with 3B parameters. Feel free to use any of the other models\navailable from the [Huggingface Hub Dolly repository](https://huggingface.co/databricks).\n```python\nMODEL_ID: str = \"databricks/dolly-v2-3b\"\nCACHE_PATH: str = \"/root/cache\"\n```\n## Build image and cache model\nWe'll download models from the Huggingface Hub and store them in our image.\nThis skips the downloading of models during inference and reduces cold boot\ntimes.\n```python\ndef download_model():\n    from transformers import AutoModelForCausalLM, AutoTokenizer\n    model = AutoModelForCausalLM.from_pretrained(\n        MODEL_ID, use_cache=True, device_map=\"auto\"\n    )\n    model.save_pretrained(CACHE_PATH, safe_serialization=True)\n    tokenizer = AutoTokenizer.from_pretrained(\n        MODEL_ID, use_fast=True, use_cache=True\n    )\n    tokenizer.save_pretrained(CACHE_PATH, safe_serialization=True)\n```\nDefine our image; install dependencies.\n```python\nimage = (\n    modal.Image.debian_slim(python_version=\"3.10\")\n    .pip_install(\n        \"jsonformer==0.9.0\",\n        \"transformers\",\n        \"torch\",\n        \"accelerate\",\n        \"safetensors\",\n    )\n    .run_function(download_model)\n)\nstub = modal.Stub(\"example-jsonformer\", image=image)\n```\n## Generate examples\nThe generate function takes two arguments `prompt` and `json_schema`, where\n`prompt` is used to describe the domain of your data (for example, \"plants\")\nand the schema contains the JSON schema you want to populate.\n```python\n@stub.function(gpu=modal.gpu.A10G())\ndef generate(prompt: str, json_schema: dict[str, Any]) -> dict[str, Any]:\n    from jsonformer import Jsonformer\n    from transformers import AutoModelForCausalLM, AutoTokenizer\n    model = AutoModelForCausalLM.from_pretrained(\n        CACHE_PATH, use_cache=True, device_map=\"auto\"\n    )\n    tokenizer = AutoTokenizer.from_pretrained(\n        MODEL_ID, use_fast=True, use_cache=True, device_map=\"auto\"\n    )\n    jsonformer = Jsonformer(model, tokenizer, json_schema, prompt)\n    generated_data = jsonformer()\n    return generated_data\n```\nAdd Modal entrypoint for invoking your script, and done!\n```python\n@stub.local_entrypoint()\ndef main():\n    prompt = \"Generate random plant information based on the following schema:\"\n    json_schema = {\n        \"type\": \"object\",\n        \"properties\": {\n            \"height_cm\": {\"type\": \"number\"},\n            \"bearing_fruit\": {\"type\": \"boolean\"},\n            \"classification\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"species\": {\"type\": \"string\"},\n                    \"kingdom\": {\"type\": \"string\"},\n                    \"family\": {\"type\": \"string\"},\n                    \"genus\": {\"type\": \"string\"},\n                },\n            },\n        },\n    }\n    result = generate.remote(prompt, json_schema)\n    print(result)\n```\n"}
{"text": "\nThis example contains a minimal but capable cloud data warehouse.\nIt's comprised of the following:\n- [DuckDB](https://duckdb.org) as the warehouse's OLAP database engine\n- AWS S3 as the data storage provider\n- [DBT](https://docs.getdbt.com/docs/introduction) as the data transformation tool\nMeet your new cloud data warehouse.\n```python\nfrom pathlib import Path\nimport modal\n```\n## Bucket name configuration\nThe only thing in the source code that you must update is the S3 bucket name.\nAWS S3 bucket names are globally unique, and the one in this source is used by Modal.\nUpdate the `BUCKET_NAME` variable below and also any references to the original value\nwithin `sample_proj_duckdb_s3/models/`. The AWS IAM policy below also includes the bucket\nname and that must be updated.\n```python\nBUCKET_NAME = \"modal-example-dbt-duckdb-s3\"\nLOCAL_DBT_PROJECT = Path(__file__).parent / \"sample_proj_duckdb_s3\"\nPROJ_PATH = \"/root/dbt\"\nPROFILES_PATH = \"/root/dbt_profile\"\nTARGET_PATH = \"/root/target\"\ndbt_image = (\n    modal.Image.debian_slim()\n    .pip_install(\n        \"boto3\",\n        \"dbt-duckdb>=1.5.1\",\n        \"pandas\",\n        \"pyarrow\",\n    )\n    .env(\n        {\n            \"DBT_PROJECT_DIR\": PROJ_PATH,\n            \"DBT_PROFILES_DIR\": PROFILES_PATH,\n            \"DBT_TARGET_PATH\": TARGET_PATH,\n        }\n    )\n)\nstub = modal.Stub(name=\"example-dbt-duckdb-s3\", image=dbt_image)\n```\n## DBT Configuration\nMost of the DBT code and configuration is taken directly from the\nhttps://github.com/dbt-labs/jaffle_shop demo and modified to support\nusing dbt-duckdb with an S3 bucket.\nThe DBT profiles.yml configuration is taken from\nhttps://github.com/jwills/dbt-duckdb#configuring-your-profile.\nHere we mount all this local code and configuration into the Modal function\nso that it will be available when we run DBT in the Modal cloud.\n```python\ndbt_project = modal.Mount.from_local_dir(\n    LOCAL_DBT_PROJECT, remote_path=PROJ_PATH\n)\ndbt_profiles = modal.Mount.from_local_file(\n    local_path=LOCAL_DBT_PROJECT / \"profiles.yml\",\n    remote_path=Path(PROFILES_PATH, \"profiles.yml\"),\n)\ndbt_target = modal.NetworkFileSystem.persisted(\"dbt-target\")\n```\nCreate this secret using the \"AWS\" template at https://modal.com/secrets/create.\nBe sure that the AWS user you provide credentials for has permission to\ncreate S3 buckets and read/write data from them.\nThe policy required for this example is the following.\nNot that you *must* update the bucket name listed in the policy to your\nown bucket name.\n```json\n{\n    \"Statement\": [\n        {\n            \"Action\": \"s3:*\",\n            \"Effect\": \"Allow\",\n            \"Resource\": [\n                \"arn:aws:s3:::modal-example-dbt-duckdb-s3/*\",\n                \"arn:aws:s3:::modal-example-dbt-duckdb-s3\"\n            ],\n            \"Sid\": \"duckdbs3access\"\n        }\n    ],\n    \"Version\": \"2012-10-17\"\n}\n```\nBelow we will use this user in a Modal function to create an S3 bucket and\npopulate it with .parquet data.\n```python\ns3_secret = modal.Secret.from_name(\"modal-examples-aws-user\")\n```\n## Seed data\nIn order to provide source data for DBT to ingest and transform,\nwe have this `create_source_data` function which creates an AWS S3 bucket and\npopulates it with .parquet files based of CSV data in the seeds/ directory.\nThis is not the typical way that seeds/ data is used, but it is fine for this\ndemonstration example. See https://docs.getdbt.com/docs/build/seeds for more info.\n```python\n@stub.function(\n    mounts=[dbt_project],\n    secrets=[s3_secret],\n)\ndef create_source_data():\n    import boto3\n    import pandas as pd\n    s3_client = boto3.client(\"s3\")\n    s3_client.create_bucket(Bucket=BUCKET_NAME)\n    for seed_csv_path in Path(PROJ_PATH, \"seeds\").glob(\"*.csv\"):\n        print(f\"found seed file {seed_csv_path}\")\n        name = seed_csv_path.stem\n        df = pd.read_csv(seed_csv_path)\n        parquet_filename = f\"{name}.parquet\"\n        df.to_parquet(parquet_filename)\n        object_key = f\"sources/{parquet_filename}\"\n        print(f\"uploading {object_key=} to S3 bucket '{BUCKET_NAME}'\")\n        s3_client.upload_file(parquet_filename, BUCKET_NAME, object_key)\n```\nThis `daily_build` function runs on a schedule to keep the DuckDB data warehouse\nup-to-date. Currently, the source data for this warehouse is static, so the updates\ndon't really update anything, just re-build. But this example could be extended\nto have sources which continually provide new data across time.\n```python\n@stub.function(\n    schedule=modal.Period(days=1),\n    secrets=[s3_secret],\n    mounts=[dbt_project, dbt_profiles],\n    network_file_systems={TARGET_PATH: dbt_target},\n)\ndef daily_build() -> None:\n    run(\"build\")\n```\n`modal run dbt_duckdb.py::run --command run`\nA successful run will log something a lot like the following:\n```\n03:41:04  Running with dbt=1.5.0\n03:41:05  Found 5 models, 8 tests, 0 snapshots, 0 analyses, 313 macros, 0 operations, 3 seed files, 3 sources, 0 exposures, 0 metrics, 0 groups\n03:41:05\n03:41:06  Concurrency: 1 threads (target='modal')\n03:41:06\n03:41:06  1 of 5 START sql table model main.stg_customers ................................ [RUN]\n03:41:06  1 of 5 OK created sql table model main.stg_customers ........................... [OK in 0.45s]\n03:41:06  2 of 5 START sql table model main.stg_orders ................................... [RUN]\n03:41:06  2 of 5 OK created sql table model main.stg_orders .............................. [OK in 0.34s]\n03:41:06  3 of 5 START sql table model main.stg_payments ................................. [RUN]\n03:41:07  3 of 5 OK created sql table model main.stg_payments ............................ [OK in 0.36s]\n03:41:07  4 of 5 START sql external model main.customers ................................. [RUN]\n03:41:07  4 of 5 OK created sql external model main.customers ............................ [OK in 0.72s]\n03:41:07  5 of 5 START sql table model main.orders ....................................... [RUN]\n03:41:08  5 of 5 OK created sql table model main.orders .................................. [OK in 0.22s]\n03:41:08\n03:41:08  Finished running 4 table models, 1 external model in 0 hours 0 minutes and 3.15 seconds (3.15s).\n03:41:08  Completed successfully\n03:41:08\n03:41:08  Done. PASS=5 WARN=0 ERROR=0 SKIP=0 TOTAL=5\n```\n```python\n@stub.function(\n    secrets=[s3_secret],\n    mounts=[dbt_project, dbt_profiles],\n    network_file_systems={TARGET_PATH: dbt_target},\n)\ndef run(command: str) -> None:\n    from dbt.cli.main import dbtRunner\n    res = dbtRunner().invoke([command])\n    if res.exception:\n        print(res.exception)\n```\nLook for the \"'materialized='external'\" DBT config in the SQL templates\nto see how `dbt-duckdb` is able to write back the transformed data to AWS S3!\nAfter running the 'run' command and seeing it succeed, check what's contained\nunder the bucket's `out/` key prefix. You'll see that DBT has run the transformations\ndefined in `sample_proj_duckdb_s3/models/` and produced output .parquet files.\n"}
{"text": "\n# Machine learning model inference endpoint that uses the webcam\nThis example creates a web endpoint that uses a Huggingface model for object detection.\nThe web endpoint takes an image from their webcam, and sends it to a Modal web endpoint.\nThe Modal web endpoint in turn calls a Modal function that runs the actual model.\nIf you run this, it will look something like this:\n![webcam](./webcam.png)\n## Live demo\n[Take a look at the deployed app](https://modal-labs-example-webcam-object-detection-fastapi-app.modal.run/).\nA couple of caveats:\n* This is not optimized for latency: every prediction takes about 1s, and\n  there's an additional overhead on the first prediction since the containers\n  have to be started and the model initialized.\n* This doesn't work on iPhone unfortunately due to some issues with HTML5\n  webcam components\n## Code\nStarting with imports:\n```python\nimport base64\nimport io\nfrom pathlib import Path\nfrom fastapi import FastAPI, Request, Response\nfrom fastapi.staticfiles import StaticFiles\nfrom modal import (\n    Image,\n    Mount,\n    Stub,\n    asgi_app,\n    method,\n)\n```\nWe need to install [transformers](https://github.com/huggingface/transformers)\nwhich is a package Huggingface uses for all their models, but also\n[Pillow](https://python-pillow.org/) which lets us work with images from Python,\nand a system font for drawing.\nThis example uses the `facebook/detr-resnet-50` pre-trained model, which is downloaded\nonce at image build time using the `download_model` function and saved into the image.\n'Baking' models into the `modal.Image` at build time provided the fastest cold start.\n```python\nmodel_repo_id = \"facebook/detr-resnet-50\"\ndef download_model():\n    from huggingface_hub import snapshot_download\n    snapshot_download(repo_id=model_repo_id, cache_dir=\"/cache\")\nstub = Stub(\"example-webcam-object-detection\")\nimage = (\n    Image.debian_slim()\n    .pip_install(\n        \"huggingface-hub==0.16.4\",\n        \"Pillow\",\n        \"timm\",\n        \"transformers\",\n    )\n    .apt_install(\"fonts-freefont-ttf\")\n    .run_function(download_model)\n)\n```\n## Prediction function\nThe object detection function has a few different features worth mentioning:\n* There's a container initialization step in the `__enter__` method, which\n  runs on every container start. This lets us load the model only once per\n  container, so that it's reused for subsequent function calls.\n* Above we stored the model in the container image. This lets us download the model only\n  when the image is (re)built, and not everytime the function is called.\n* We're running it on multiple CPUs for extra performance\nNote that the function takes an image and returns a new image.\nThe input image is from the webcam\nThe output image is an image with all the bounding boxes and labels on them,\nwith an alpha channel so that most of the image is transparent so that the\nweb interface can render it on top of the webcam view.\n```python\n@stub.cls(\n    cpu=4,\n    image=image,\n)\nclass ObjectDetection:\n    def __enter__(self):\n        from transformers import DetrForObjectDetection, DetrImageProcessor\n        self.feature_extractor = DetrImageProcessor.from_pretrained(\n            model_repo_id,\n            cache_dir=\"/cache\",\n        )\n        self.model = DetrForObjectDetection.from_pretrained(\n            model_repo_id,\n            cache_dir=\"/cache\",\n        )\n    @method()\n    def detect(self, img_data_in):\n        # Based on https://huggingface.co/spaces/nateraw/detr-object-detection/blob/main/app.py\n        from PIL import Image, ImageColor, ImageDraw, ImageFont\n        # Read png from input\n        image = Image.open(io.BytesIO(img_data_in)).convert(\"RGB\")\n        # Make prediction\n        import torch\n        inputs = self.feature_extractor(image, return_tensors=\"pt\")\n        outputs = self.model(**inputs)\n        img_size = torch.tensor([tuple(reversed(image.size))])\n        processed_outputs = (\n            self.feature_extractor.post_process_object_detection(\n                outputs=outputs,\n                target_sizes=img_size,\n                threshold=0,\n            )\n        )\n        output_dict = processed_outputs[0]\n        # Grab boxes\n        keep = output_dict[\"scores\"] > 0.7\n        boxes = output_dict[\"boxes\"][keep].tolist()\n        scores = output_dict[\"scores\"][keep].tolist()\n        labels = output_dict[\"labels\"][keep].tolist()\n        # Plot bounding boxes\n        colors = list(ImageColor.colormap.values())\n        font = ImageFont.truetype(\n            \"/usr/share/fonts/truetype/freefont/FreeMono.ttf\", 18\n        )\n        output_image = Image.new(\"RGBA\", (image.width, image.height))\n        output_image_draw = ImageDraw.Draw(output_image)\n        for _score, box, label in zip(scores, boxes, labels):\n            color = colors[label % len(colors)]\n            text = self.model.config.id2label[label]\n            box = tuple(map(int, box))\n            output_image_draw.rectangle(box, outline=color)\n            output_image_draw.text(\n                box[:2], text, font=font, fill=color, width=3\n            )\n        # Return PNG as bytes\n        with io.BytesIO() as output_buf:\n            output_image.save(output_buf, format=\"PNG\")\n            return output_buf.getvalue()\n```\n## Defining the web interface\nTo keep things clean, we define the web endpoints separate from the prediction\nfunction. This will introduce a tiny bit of extra latency (every web request\ntriggers a Modal function call which will call another Modal function) but in\npractice the overhead is much smaller than the overhead of running the prediction\nfunction etc.\nWe also serve a static html page which contains some tiny bit of Javascript to\ncapture the webcam feed and send it to Modal.\n```python\nweb_app = FastAPI()\nstatic_path = Path(__file__).with_name(\"webcam\").resolve()\n```\nThe endpoint for the prediction function takes an image as a\n[data URI](https://en.wikipedia.org/wiki/Data_URI_scheme)\nand returns another image, also as a data URI:\n```python\n@web_app.post(\"/predict\")\nasync def predict(request: Request):\n    # Takes a webcam image as a datauri, returns a bounding box image as a datauri\n    body = await request.body()\n    img_data_in = base64.b64decode(body.split(b\",\")[1])  # read data-uri\n    img_data_out = ObjectDetection().detect.remote(img_data_in)\n    output_data = b\"data:image/png;base64,\" + base64.b64encode(img_data_out)\n    return Response(content=output_data)\n```\n## Exposing the web server\nLet's take the Fast API app and expose it to Modal.\n```python\n@stub.function(\n    mounts=[Mount.from_local_dir(static_path, remote_path=\"/assets\")],\n)\n@asgi_app()\ndef fastapi_app():\n    web_app.mount(\"/\", StaticFiles(directory=\"/assets\", html=True))\n    return web_app\n```\n## Running this locally\nYou can run this as an ephemeral app, by running\n```shell\nmodal serve webcam.py\n```\n"}
{"text": "\n# FastAI model training with Weights & Biases and Gradio\nThis example trains a vision model to 98-99% accuracy on the CIFAR-10 dataset,\nand then makes this trained model shareable with others using the [Gradio.app](https://gradio.app/)\nweb interface framework.\nCombining GPU-accelerated Modal Functions, a network file system for caching, and Modal\nwebhooks for the model demo, we have a simple, productive, and cost-effective\npathway to building and deploying ML in the cloud!\n![Gradio.app image classifer interface](./gradio-image-classify.png)\n## Setting up the dependencies\nOur GPU training is done with PyTorch which bundles its CUDA dependencies, so\nwe can start with a slim Debian OS image and install a handful of `pip` packages into it.\n```python\nimport dataclasses\nimport os\nimport pathlib\nimport sys\nfrom typing import List, Optional, Tuple\nfrom fastapi import FastAPI\nfrom modal import (\n    Image,\n    Mount,\n    NetworkFileSystem,\n    Secret,\n    Stub,\n    asgi_app,\n    method,\n)\nweb_app = FastAPI()\nassets_path = pathlib.Path(__file__).parent / \"vision_model_training\" / \"assets\"\nstub = Stub(name=\"example-fastai-wandb-gradio-cifar10-demo\")\nimage = Image.debian_slim().pip_install(\n    \"fastai~=2.7.9\",\n    \"gradio~=3.6\",\n    \"httpx~=0.23.0\",\n    # When using pip PyTorch is not automatically installed by fastai.\n    \"torch~=1.12.1\",\n    \"torchvision~=0.13.1\",\n    \"wandb~=0.13.4\",\n)\n```\nA persisted network file system will store trained model artefacts across Modal app runs.\nThis is crucial as training runs are separate from the Gradio.app we run as a webhook.\n```python\nvolume = NetworkFileSystem.persisted(\"cifar10-training-vol\")\nFASTAI_HOME = \"/fastai_home\"\nMODEL_CACHE = pathlib.Path(FASTAI_HOME, \"models\")\nUSE_GPU = os.environ.get(\"MODAL_GPU\")\nMODEL_EXPORT_PATH = pathlib.Path(MODEL_CACHE, \"model-exports\", \"inference.pkl\")\nos.environ[\n    \"FASTAI_HOME\"\n] = FASTAI_HOME  # Ensure fastai saves data into persistent volume path.\n```\n## Config\nTraining config gets its own dataclass to avoid smattering special/magic values throughout code.\n```python\n@dataclasses.dataclass\nclass WandBConfig:\n    project: str = \"fast-cifar10\"\n    entity: Optional[str] = None\n@dataclasses.dataclass\nclass Config:\n    epochs: int = 10\n    img_dims: Tuple[int, int] = (32, 224)\n    gpu: str = USE_GPU\n    wandb: WandBConfig = WandBConfig()\n```\n## Get CIFAR-10 dataset\nThe `fastai` framework famously requires very little code to get things done,\nso our downloading function is very short and simple. The CIFAR-10 dataset is\nalso not large, about 150MB, so we don't bother persisting it in a network file system\nand just download and unpack it to ephemeral disk.\n```python\ndef download_dataset():\n    from fastai.data.external import URLs, untar_data\n    path = untar_data(URLs.CIFAR)\n    print(f\"Finished downloading and unpacking CIFAR-10 dataset to {path}.\")\n    return path\n```\n## Training a vision model with FastAI\nTo address the CIFAR-10 image classification problem, we use the high-level fastAI framework\nto train a Deep Residual Network (https://arxiv.org/pdf/1512.03385.pdf) with 18-layers, called `resnet18`.\nWe don't train the model from scratch. A transfer learning approach is sufficient to reach 98-99%\naccuracy on the classification task. The main tweak we make is to adjust the image size of the CIFAR-10\nexamples to be 224x224, which was the image size the `resnet18` model was originally trained on.\nJust to demonstrate the affect of the image upscaling on classification performance, we still train on\nthe original size 32x32 images.\n### Tracking with Weights & Biases\n![weights & biases UI](./wandb-ui.png)\nWeights & Biases (W&B) is a product that provides out-of-the-box model training observability. With a few\nlines of code and an account, we gain a dashboard will key metrics such as training loss, accuracy, and GPU\nutilization.\nIf you want to run this example without setting up Weights & Biases, just remove the\n`secret=Secret.from_name(\"wandb\")` line from the Function decorator below; this will disable Weights & Biases\nfunctionality.\n### Detaching our training run\nFine-tuning the base ResNet model takes about 30-40 minutes on a GPU. To avoid\nneeding to keep our terminal active, we can run training as a 'detached run'.\n`MODAL_GPU=any modal run --detach vision_model_training.py::stub.train`\n```python\n@stub.function(\n    image=image,\n    gpu=USE_GPU,\n    network_file_systems={str(MODEL_CACHE): volume},\n    secret=Secret.from_name(\"wandb\"),\n    timeout=2700,  # 45 minutes\n)\ndef train():\n    import wandb\n    from fastai.callback.wandb import WandbCallback\n    from fastai.data.transforms import parent_label\n    from fastai.metrics import accuracy\n    from fastai.vision.all import Resize, models, vision_learner\n    from fastai.vision.data import (\n        CategoryBlock,\n        DataBlock,\n        ImageBlock,\n        TensorCategory,\n        get_image_files,\n    )\n    config: Config = Config()\n    print(\"Downloading dataset\")\n    dataset_path = download_dataset()\n    wandb_enabled = bool(os.environ.get(\"WANDB_API_KEY\"))\n    if wandb_enabled:\n        wandb.init(\n            id=stub.app.app_id,\n            project=config.wandb.project,\n            entity=config.wandb.entity,\n        )\n        callbacks = WandbCallback()\n    else:\n        callbacks = None\n    for dim in config.img_dims:\n        print(f\"Training on {dim}x{dim} size images.\")\n        dblock = DataBlock(\n            blocks=(ImageBlock(), CategoryBlock()),\n            get_items=get_image_files,\n            get_y=parent_label,\n            item_tfms=Resize(dim),\n        )\n        dls = dblock.dataloaders(dataset_path, bs=64)\n        learn = vision_learner(\n            dls, models.resnet18, metrics=accuracy, cbs=callbacks\n        ).to_fp16()\n        learn.fine_tune(config.epochs, freeze_epochs=3)\n        learn.save(f\"cifar10_{dim}\")\n        # run on test set\n        test_files = get_image_files(dataset_path / \"test\")\n        label = TensorCategory(\n            [dls.vocab.o2i[parent_label(f)] for f in test_files]\n        )\n        test_set = learn.dls.test_dl(test_files)\n        pred = learn.get_preds(0, test_set)\n        acc = accuracy(pred[0], label).item()\n        print(f\"{dim}x{dim}, test accuracy={acc}\")\n    # \ud83d\udc1d Close wandb run\n    if wandb_enabled:\n        wandb.finish()\n    print(\"Exporting model for later inference.\")\n    MODEL_EXPORT_PATH.parent.mkdir(exist_ok=True)\n    learn.remove_cbs(\n        WandbCallback\n    )  # Added W&B callback is not compatible with inference.\n    learn.export(MODEL_EXPORT_PATH)\n```\n## Trained model plumbing\nTo load a trained model into the demo app, we write a small\namount of harness code that loads the saved model from persistent\ndisk once on container start.\nThe model's predict function accepts an image as bytes or a numpy array.\n```python\n@stub.cls(\n    image=image,\n    network_file_systems={str(MODEL_CACHE): volume},\n)\nclass ClassifierModel:\n    def __enter__(self):\n        from fastai.learner import load_learner\n        self.model = load_learner(MODEL_EXPORT_PATH)\n    @method()\n    def predict(self, image) -> str:\n        prediction = self.model.predict(image)\n        classification = prediction[0]\n        return classification\n@stub.function(\n    image=image,\n)\ndef classify_url(image_url: str) -> None:\n    \"\"\"Utility function for command-line classification runs.\"\"\"\n    import httpx\n    r = httpx.get(image_url)\n    if r.status_code != 200:\n        raise RuntimeError(f\"Could not download '{image_url}'\")\n    classifier = ClassifierModel()\n    label = classifier.predict.remote(image=r.content)\n    print(f\"Classification: {label}\")\n```\n## Wrap the trained model in Gradio's web UI\nGradio.app makes it super easy to expose a model's functionality\nin an intuitive web interface.\nThis model is an image classifier, so we set up an interface that\naccepts an image via drag-and-drop and uses the trained model to\noutput a classification label.\nRemember, this model was trained on tiny CIFAR-10 images so it's\ngoing to perform best against similarly simple and scaled-down images.\n```python\ndef create_demo_examples() -> List[str]:\n    # NB: Don't download these images to a network FS as it doesn't play well with Gradio.\n    import httpx\n    example_imgs = {\n        \"lion.jpg\": \"https://i0.wp.com/lioncenter.umn.edu/wp-content/uploads/2018/10/cropped-DSC4884_Lion_Hildur-1.jpg\",\n        \"mouse.jpg\": \"https://static-s.aa-cdn.net/img/ios/1077591533/18f74754ae55ee78e96e04d14e8bff35?v=1\",\n        \"plane.jpg\": \"https://x-plane.hu/L-410/img/about/2.png\",\n        \"modal.jpg\": \"https://pbs.twimg.com/profile_images/1567270019075031040/Hnrebn0M_400x400.jpg\",\n    }\n    available_examples = []\n    for dest, url in example_imgs.items():\n        filepath = pathlib.Path(dest)\n        r = httpx.get(url)\n        if r.status_code != 200:\n            print(f\"Could not download '{url}'\", file=sys.stderr)\n            continue\n        with open(filepath, \"wb\") as f:\n            f.write(r.content)\n        available_examples.append(str(filepath))\n    return available_examples\n@stub.function(\n    image=image,\n    network_file_systems={str(MODEL_CACHE): volume},\n    mounts=[Mount.from_local_dir(assets_path, remote_path=\"/assets\")],\n)\n@asgi_app()\ndef fastapi_app():\n    import gradio as gr\n    from gradio.routes import mount_gradio_app\n    classifier = ClassifierModel()\n    interface = gr.Interface(\n        fn=classifier.predict.remote,\n        inputs=gr.Image(shape=(224, 224)),\n        outputs=\"label\",\n        examples=create_demo_examples(),\n        css=\"/assets/index.css\",\n    )\n    return mount_gradio_app(\n        app=web_app,\n        blocks=interface,\n        path=\"/\",\n    )\n## Running this\n```\nTo run training as an ephemeral app:\n```shell\nmodal run vision_model_training.py::stub.train\n```\nTo test the model on an image, run:\n```shell\nmodal run vision_model_training.py::stub.classify_url --image-url <url>\n```\nTo run the Gradio server, run:\n```shell\nmodal serve vision_model_training.py\n```\nThis ML app is already deployed on Modal and you can try it out at https://modal-labs-example-fastai-wandb-gradio-cifar10-demo-fastapi-app.modal.run.\n"}
{"text": "# Screenshot with Chromium\nIn this example, we use Modal functions and the `playwright` package to take screenshots\nof websites from a list of URLs in parallel.\nYou can run this example on the command line with\n```\nmodal run 02_building_containers/screenshot.py --url 'https://www.youtube.com/watch?v=dQw4w9WgXcQ'\n```\nThis should take a few seconds then create a `/tmp/screenshots/screenshot.png` file, shown below.\n![screenshot](./screenshot.png)\n## Setup\nFirst we import the Modal client library.\n```python\nimport pathlib\nimport modal\nstub = modal.Stub(\"example-screenshot\")\n```\n## Define a custom image\nWe need an image with the `playwright` Python package as well as its `chromium` plugin pre-installed.\nThis requires intalling a few Debian packages, as well as setting up a new Debian repository.\nModal lets you run arbitrary commands, just like in Docker:\n```python\nimage = modal.Image.debian_slim().run_commands(\n    \"apt-get install -y software-properties-common\",\n    \"apt-add-repository non-free\",\n    \"apt-add-repository contrib\",\n    \"apt-get update\",\n    \"pip install playwright==1.30.0\",\n    \"playwright install-deps chromium\",\n    \"playwright install chromium\",\n)\n```\n## The screenshot function\nNext, the scraping function which runs headless Chromium, goes to a website, and takes a screenshot.\nThis is a Modal function which runs inside the remote container.\n```python\n@stub.function(image=image)\nasync def screenshot(url):\n    from playwright.async_api import async_playwright\n    async with async_playwright() as p:\n        browser = await p.chromium.launch()\n        page = await browser.new_page()\n        await page.goto(url, wait_until=\"networkidle\")\n        await page.screenshot(path=\"screenshot.png\")\n        await browser.close()\n        data = open(\"screenshot.png\", \"rb\").read()\n        print(\"Screenshot of size %d bytes\" % len(data))\n        return data\n```\n## Entrypoint code\nLet's kick it off by reading a bunch of URLs from a txt file and scrape some of those.\n```python\n@stub.local_entrypoint()\ndef main(url: str = \"https://modal.com\"):\n    filename = pathlib.Path(\"/tmp/screenshots/screenshot.png\")\n    data = screenshot.remote(url)\n    filename.parent.mkdir(exist_ok=True)\n    with open(filename, \"wb\") as f:\n        f.write(data)\n    print(f\"wrote {len(data)} bytes to {filename}\")\n```\nAnd we're done! Please also see our [introductory guide](/docs/guide/web-scraper) for another\nexample of a web scraper, with more in-depth logic.\n"}
{"text": "\n```python\nimport json\nimport os\nimport time\nfrom modal import (\n    Dict,\n    Image,\n    Mount,\n    Secret,\n    Stub,\n    asgi_app,\n    gpu,\n    method,\n)\nMODEL_DIR = \"/model\"\ndef download_model_to_folder():\n    from huggingface_hub import snapshot_download\n    snapshot_download(\n        \"meta-llama/Llama-2-13b-chat-hf\",\n        local_dir=MODEL_DIR,\n        local_dir_use_symlinks=False,\n        token=os.environ[\"HUGGINGFACE_TOKEN\"],\n    )\nvllm_image = (\n    Image.from_dockerhub(\"nvcr.io/nvidia/pytorch:22.12-py3\")\n    .pip_install(\n        \"torch==2.0.1\", index_url=\"https://download.pytorch.org/whl/cu118\"\n    )\n    # Pinned to 07/21/2023\n    .pip_install(\n        \"vllm @ git+https://github.com/vllm-project/vllm.git@d7a1c6d614756b3072df3e8b52c0998035fb453f\"\n    )\n    .run_function(\n        download_model_to_folder, secret=Secret.from_name(\"huggingface\")\n    )\n)\nstub = Stub(\"llama-demo\")\nstub.dict = Dict.new()\n```\nvLLM class\n```python\n@stub.cls(\n    gpu=gpu.A100(),\n    image=vllm_image,\n    allow_concurrent_inputs=60,\n    concurrency_limit=1,\n    container_idle_timeout=600,\n)\nclass Engine:\n    def __enter__(self):\n        from vllm.engine.arg_utils import AsyncEngineArgs\n        from vllm.engine.async_llm_engine import AsyncLLMEngine\n        # tokens generated since last report\n        self.last_report, self.generated_tokens = time.time(), 0\n        engine_args = AsyncEngineArgs(\n            model=MODEL_DIR,\n            # Only uses 90% of GPU memory by default\n            gpu_memory_utilization=0.95,\n        )\n        self.engine = AsyncLLMEngine.from_engine_args(engine_args)\n        self.template = \"\"\"<s>[INST] <<SYS>>\nYou are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n<</SYS>>\n{} [/INST] \"\"\"\n    def generated(self, n: int):\n        # Log that n tokens have been generated\n        t = time.time()\n        self.generated_tokens += n\n        # Save to dict every second\n        if t - self.last_report > 1.0:\n            stub.app.dict.update(\n                tps=self.generated_tokens / (t - self.last_report),\n                t=self.last_report,\n            )\n            self.last_report, self.generated_tokens = t, 0\n    @method()\n    async def completion(self, question: str):\n        if not question:\n            return\n        from vllm.sampling_params import SamplingParams\n        from vllm.utils import random_uuid\n        sampling_params = SamplingParams(\n            presence_penalty=0.8,\n            temperature=0.2,\n            top_p=0.95,\n            top_k=50,\n            max_tokens=1024,\n        )\n        request_id = random_uuid()\n        results_generator = self.engine.generate(\n            self.template.format(question), sampling_params, request_id\n        )\n        t0 = time.time()\n        index, tokens = 0, 0\n        async for request_output in results_generator:\n            if \"\\ufffd\" == request_output.outputs[0].text[-1]:\n                continue\n            yield request_output.outputs[0].text[index:]\n            index = len(request_output.outputs[0].text)\n            # Token accounting\n            new_tokens = len(request_output.outputs[0].token_ids)\n            self.generated(new_tokens - tokens)\n            tokens = new_tokens\n        throughput = tokens / (time.time() - t0)\n        print(f\"Request completed: {throughput:.4f} tokens/s\")\n        print(request_output.outputs[0].text)\n@stub.function(\n    mounts=[Mount.from_local_dir(frontend_path, remote_path=\"/assets\")],\n    keep_warm=3,\n    concurrency_limit=6,\n    allow_concurrent_inputs=24,\n    timeout=600,\n)\n@asgi_app()\ndef app():\n    import fastapi\n    import fastapi.staticfiles\n    from fastapi.responses import StreamingResponse\n    web_app = fastapi.FastAPI()\n    @web_app.get(\"/stats\")\n    async def stats():\n        stats = Engine().completion.get_current_stats()\n        try:\n            tps, t = stub.app.dict.get(\"tps\"), stub.app.dict.get(\"t\")\n        except KeyError:\n            tps, t = 0, 0\n        return {\n            \"backlog\": stats.backlog,\n            \"num_active_runners\": stats.num_active_runners,\n            \"num_total_runners\": stats.num_total_runners,\n            \"tps\": tps if t > time.time() - 4.0 else 0,\n        }\n    @web_app.get(\"/completion/{question}\")\n    async def get(question: str):\n        from urllib.parse import unquote\n        print(\"Web server received request for\", unquote(question))\n        # FastAPI will run this in a separate thread\n        def generate():\n            for chunk in Engine().completion.call(unquote(question)):\n                yield f\"data: {json.dumps(dict(text=chunk), ensure_ascii=False)}\\n\\n\"\n        return StreamingResponse(generate(), media_type=\"text/event-stream\")\n    web_app.mount(\n        \"/\", fastapi.staticfiles.StaticFiles(directory=\"/assets\", html=True)\n    )\n    return web_app\n```\n"}
{"text": "\n```python\nimport time\nimport modal\nstub = modal.Stub(\"example-parallel\")\n@stub.function()\ndef step1(word):\n    time.sleep(2)\n    print(\"step1 done\")\n    return word\n@stub.function()\ndef step2(number):\n    time.sleep(1)\n    print(\"step2 done\")\n    if number == 0:\n        raise ValueError(\"custom error\")\n    return number\n@stub.local_entrypoint()\ndef main():\n    # Start running a function and return a handle to its result.\n    word_call = step1.spawn(\"foo\")\n    number_call = step2.spawn(2)\n    # Print \"foofoo\" after 2 seconds.\n    print(word_call.get() * number_call.get())\n    # Alternatively, use `modal.functions.gather(...)` as a convenience wrapper,\n    # which returns an error if either call fails.\n    results = modal.functions.gather(step1.spawn(\"bar\"), step2.spawn(4))\n    assert results == [\"bar\", 4]\n    # Raise exception after 2 seconds.\n    try:\n        modal.functions.gather(step1.spawn(\"bar\"), step2.spawn(0))\n    except ValueError as exc:\n        assert str(exc) == \"custom error\"\n```\n"}
{"text": "# Create an image with CUDA\nThis example shows how you can use the Nvidia CUDA base image from DockerHub.\nWe need to install Python 3 and pip in `setup_dockerfile_commands` because the\nbase image doesn't have them by default. The commands to do this are specific\nto Ubuntu 22.04 in this example.\n```python\nfrom modal import Image, Stub\nstub = Stub()\nstub.image = Image.from_registry(\n    \"nvidia/cuda:12.2.0-devel-ubuntu22.04\",\n    setup_dockerfile_commands=[\n        \"RUN apt-get update\",\n        \"RUN apt-get install -y python3 python3-pip python-is-python3\",\n    ],\n)\n```\nNow, we can create a function with GPU capabilities. Run this file with\n`modal run install_cuda.py`.\n```python\n@stub.function(gpu=\"T4\")\ndef f():\n    import subprocess\n    subprocess.run([\"nvidia-smi\"])\n```\n"}
{"text": "# Write to Google Sheets\nIn this tutorial, we'll show how to use Modal to schedule a daily update of a dataset\nfrom an analytics database to Google Sheets.\n## Entering credentials\nWe begin by setting up some credentials that we'll need in order to access our database and output\nspreadsheet. To do that in a secure manner, we log in to our Modal account on the web and go to\nthe [Secrets](/secrets) section.\n### Database\nFirst we will enter our database credentials. The easiest way to do this is to click **New\nsecret** and select the **Postgres compatible** secret preset and fill in the requested\ninformation. Then we press **Next** and name our secret \"analytics-database\" and click **Create**.\n### Google Sheets/GCP\nWe'll now add another Secret for Google Sheets access through Google Cloud Platform. Click **New\nsecret** and select the Google Sheets preset.\nIn order to access the Google Sheets API, we'll need to create a *Service Account* in Google Cloud\nPlatform. You can skip this step if you already have a Service Account json file.\n1. Sign up to Google Cloud Platform or log in if you haven't\n   ([https://cloud.google.com/](https://cloud.google.com/)).\n2. Go to [https://console.cloud.google.com/](https://console.cloud.google.com/).\n3. In the navigation pane on the left, go to **IAM & Admin** > **Service Accounts**.\n4. Click the **+ CREATE SERVICE ACCOUNT** button.\n5. Give the service account a suitable name, like \"sheet-access-bot\". Click **Done**. You don't\n   have to grant it any specific access privileges at this time.\n6. Click your new service account in the list view that appears and navigate to the **Keys**\n   section.\n7. Click **Add key** and choose **Create new key**. Use the **JSON** key type and confirm by\n   clicking **Create**.\n8. A json key file should be downloaded to your computer at this point. Copy the contents of that\n   file and use it as the value for the **SERVICE_ACCOUNT_JSON** field in your new secret.\nWe'll name this other secret \"gsheets\".\nNow you can access the values of your secrets from modal functions that you annotate with the\ncorresponding EnvDict includes, e.g.:\n```python\nimport os\nimport modal\nstub = modal.Stub(\"example-db-to-sheet\")\n@stub.function(secret=modal.Secret.from_name(\"postgres-secret\"))\ndef my_func():\n    # automatically filled from the specified secret\n    print(\"Host is \" + os.environ[\"PGHOST\"])\n```\nIn order to connect to the database, we'll use the `psycopg2` Python package. To make it available\nto your Modal function you need to supply it with an `image` argument that tells Modal how to\nbuild the container image that contains that package. We'll base it off of the `Image.debian_slim` base\nimage that's built into modal, and make sure to install the required binary packages as well as\nthe psycopg2 package itself:\n```python\npg_image = (\n    modal.Image.debian_slim().apt_install(\"libpq-dev\").pip_install(\"psycopg2\")\n)\n```\nSince the default keynames for a **Postgres compatible** secret correspond to the environment\nvariables that `psycopg2` looks for, you can now easily connect to the database even without\nexplicit credentials in your code. We'll create a simple function that queries the city for each\nuser in our dummy `users` table:\n```python\n@stub.function(\n    image=pg_image,\n    secret=modal.Secret.from_name(\"postgres-secret\"),\n)\ndef get_db_rows():\n    import psycopg2\n    conn = psycopg2.connect()  # no explicit credentials needed\n    cur = conn.cursor()\n    cur.execute(\"SELECT city FROM users\")\n    return [row[0] for row in cur.fetchall()]\n```\nNote that we import psycopg2 inside our function instead of the global scope. This allows us to\nrun this Modal function even from an environment where psycopg2 is not installed. We can test run\nthis function using the `modal run` shell command: `modal run db_to_sheet.py::stub.get_db_rows`.\n## Applying Python logic\nFor each city in our source data we'll make an online lookup of the current weather using the\n[http://openweathermap.org](http://openweathermap.org) API. To do this, we'll add the API key to\nanother modal secret. We'll use a custom secret called \"weather\" with the key\n`OPENWEATHER_API_KEY` containing our API key for OpenWeatherMap.\n```python\nrequests_image = modal.Image.debian_slim().pip_install(\"requests\")\n@stub.function(\n    image=requests_image,\n    secret=modal.Secret.from_name(\"weather-secret\"),\n)\ndef city_weather(city):\n    import requests\n    url = \"https://api.openweathermap.org/data/2.5/weather\"\n    params = {\"q\": city, \"appid\": os.environ[\"OPENWEATHER_API_KEY\"]}\n    response = requests.get(url, params=params)\n    weather_label = response.json()[\"weather\"][0][\"main\"]\n    return weather_label\n```\nWe'll make use of Modal's built-in `function.map` method to create our report. `function.map`\nmakes it really easy to parallelise work by executing a function for a larger sequence of input\ndata. For this example we'll make a simple count of rows per weather type, using Python's\nstandard library `collections.Counter`.\n```python\nfrom collections import Counter\n@stub.function()\ndef create_report(cities):\n    # run city_weather for each city in parallel\n    user_weather = city_weather.map(cities)\n    users_by_weather = Counter(user_weather).items()\n    return users_by_weather\n```\nLet's try to run this! To make it simple to trigger the function with some\npredefined input data, we create a \"local entrypoint\" `main` that can be\neasily triggered from the command line:\n```python\n@stub.local_entrypoint()\ndef main():\n    cities = [\n        \"Stockholm,,Sweden\",\n        \"New York,NY,USA\",\n        \"Tokyo,,Japan\",\n    ]\n    print(create_report.remote(cities))\n```\nRunning the local entrypoint using `modal run db_to_sheet.py` should print something like:\n`dict_items([('Clouds', 3)])`.\nNote that since this file only has a single stub, and the stub has only one local entrypoint\nwe only have to specify the file to run - the function/entrypoint is inferred.\nIn this case the logic is quite simple, but in a real world context you could have applied a\nmachine learning model or any other tool you could build into a container to transform the data.\n## Sending output to a Google Sheet\nWe'll set up a new Google Sheet to send our report to. Using the \"Sharing\" dialog in Google\nSheets, we make sure to share the document to the service account's email address (the value of\nthe `client_email` field in the json file) and make the service account an editor of the document.\nThe URL of a Google Sheet is something like:\n`https://docs.google.com/spreadsheets/d/1wOktal......IJR77jD8Do`.\nWe copy the part of the URL that comes after `/d/` - that is the *key* of the document which\nwe'll refer to in our code. We'll make use of the `pygsheets` python package to authenticate with\nGoogle Sheets and then update the spreadsheet with information from the report we just created:\n```python\npygsheets_image = modal.Image.debian_slim().pip_install(\"pygsheets\")\n@stub.function(\n    image=pygsheets_image,\n    secret=modal.Secret.from_name(\"gsheets-secret\"),\n)\ndef update_sheet_report(rows):\n    import pygsheets\n    gc = pygsheets.authorize(service_account_env_var=\"SERVICE_ACCOUNT_JSON\")\n    document_key = \"1RqQrJ6Ikf611adKunm8tmL1mKzHLjNwLWm_T7mfXSYA\"\n    sh = gc.open_by_key(document_key)\n    worksheet = sh.sheet1\n    worksheet.clear(\"A2\")\n    worksheet.update_values(\"A2\", [list(row) for row in rows])\n```\nAt this point, we have everything we need in order to run the full program. We can put it all together in\nanother Modal function, and add a [schedule](/docs/guide/cron) argument so it runs every day automatically:\n```python\n@stub.function(schedule=modal.Cron(\"0 0 * * *\"))\ndef db_to_sheet():\n    rows = get_db_rows.remote()\n    report = create_report.remote(rows)\n    update_sheet_report.remote(report)\n    print(\"Updated sheet with new weather distribution\")\n    for weather, count in report:\n        print(f\"{weather}: {count}\")\n```\nThis entire stub can now be deployed using `modal deploy db_to_sheet.py`. The [apps page](/apps)\nshows our cron job's execution history and lets you navigate to each invocation's logs.\nTo trigger a manual run from your local code during development, you can also trigger this function using the cli:\n`modal run db_to_sheet.py::stub.db_to_sheet`\nNote that all of the @stub.function() annotated functions above run remotely in isolated containers that are specified per\nfunction, but they are called as seamlessly as using regular Python functions. This is a simple\nshowcase of how you can mix and match functions that use different environments and have them feed\ninto each other or even call each other as if they were all functions in the same local program.\n"}
{"text": "\n```python\nimport modal\nstub = modal.Stub(\"example-gpt2\")\nCACHE_PATH = \"/root/model_cache\"\n```\nRun as a build function to save the model files into the custom `modal.Image`.\n```python\ndef download_model():\n    from transformers import pipeline\n    generator = pipeline(\"text-generation\", model=\"gpt2\")\n    generator.save_pretrained(CACHE_PATH)\n@stub.function(\n    image=modal.Image.debian_slim()\n    .pip_install(\"torch\", \"transformers\")\n    .run_function(download_model),\n)\ndef generate_text(prompt: str):\n    from transformers import pipeline\n    # NOTE: This model load runs on every function invocation. It's more efficient\n    # to use lifecycle methods: modal.com/docs/guide/lifecycle-functions#container-lifecycle-beta.\n    generator = pipeline(\"text-generation\", model=CACHE_PATH)\n    return generator(prompt, do_sample=True, min_length=50, max_length=250)[0][\n        \"generated_text\"\n    ]\n@stub.local_entrypoint()\ndef main(prompt: str = \"\"):\n    generation = generate_text.remote(prompt=prompt or \"Show me the meaning of\")\n    print(generation)\n```\n"}
{"text": "\n```python\n\"\"\"Single-page application that lets you talk to a transformer chatbot.\nThis is a complex example demonstrating an end-to-end web application backed by\nserverless web handlers and GPUs. The user visits a single-page application,\nwritten using Solid.js. This interface makes API requests that are handled by a\nModal function running on the GPU.\nThe weights of the model are saved in the image, so they don't need to be\ndownloaded again while the app is running.\nChat history tensors are saved in a `modal.Dict` distributed dictionary.\n\"\"\"\nimport uuid\nfrom pathlib import Path\nfrom typing import Optional, Tuple\nimport fastapi\nfrom fastapi.responses import JSONResponse\nfrom fastapi.staticfiles import StaticFiles\nfrom modal import Dict, Image, Mount, Stub, asgi_app\nassets_path = Path(__file__).parent / \"chatbot_spa\"\nstub = Stub(\"example-chatbot-spa\")\nstub.chat_histories = Dict.new()\ndef load_tokenizer_and_model():\n    from transformers import AutoModelForCausalLM, AutoTokenizer\n    tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-large\")\n    model = AutoModelForCausalLM.from_pretrained(\n        \"microsoft/DialoGPT-large\",\n        device_map=\"auto\",\n    )\n    return tokenizer, model\nstub.gpu_image = (\n    Image.debian_slim()\n    .pip_install(\"torch\", find_links=\"https://download.pytorch.org/whl/cu116\")\n    .pip_install(\"transformers~=4.31\", \"accelerate\")\n    .run_function(load_tokenizer_and_model)\n)\nif stub.is_inside(stub.gpu_image):\n    import torch\n    tokenizer, model = load_tokenizer_and_model()\n@stub.function(\n    mounts=[Mount.from_local_dir(assets_path, remote_path=\"/assets\")]\n)\n@asgi_app()\ndef transformer():\n    app = fastapi.FastAPI()\n    @app.post(\"/chat\")\n    def chat(body: dict = fastapi.Body(...)):\n        message = body[\"message\"]\n        chat_id = body.get(\"id\")\n        id, response = generate_response.remote(message, chat_id)\n        return JSONResponse({\"id\": id, \"response\": response})\n    app.mount(\"/\", StaticFiles(directory=\"/assets\", html=True))\n    return app\n@stub.function(gpu=\"any\", image=stub.gpu_image)\ndef generate_response(\n    message: str, id: Optional[str] = None\n) -> Tuple[str, str]:\n    new_input_ids = tokenizer.encode(\n        message + tokenizer.eos_token, return_tensors=\"pt\"\n    ).to(\"cuda\")\n    if id is not None:\n        chat_history = stub.chat_histories[id]\n        bot_input_ids = torch.cat([chat_history, new_input_ids], dim=-1)\n    else:\n        id = str(uuid.uuid4())\n        bot_input_ids = new_input_ids\n    chat_history = model.generate(\n        bot_input_ids, max_length=1250, pad_token_id=tokenizer.eos_token_id\n    )\n    response = tokenizer.decode(\n        chat_history[:, bot_input_ids.shape[-1] :][0], skip_special_tokens=True\n    )\n    stub.chat_histories[id] = chat_history\n    return id, response\n@stub.local_entrypoint()\ndef test_response(message: str):\n    _, response = generate_response.remote(message)\n    print(response)\n```\n"}
{"text": "\n# Stable diffusion slackbot\nThis tutorial shows you how to build a Slackbot that uses\n[stable diffusion](https://stability.ai/blog/stable-diffusion-public-release)\nto produce realistic images from text prompts on demand.\n![stable diffusion slackbot](./stable_diff_screenshot.jpg)\n## Basic setup\n```python\nimport io\nimport os\nfrom typing import Optional\nfrom modal import Image, Secret, Stub, web_endpoint\n```\nAll Modal programs need a [`Stub`](/docs/reference/modal.Stub) \u2014 an object that acts as a recipe for\nthe application. Let's give it a friendly name.\n```python\nstub = Stub(\"example-stable-diff-bot\")\n```\n## Inference Function\n### HuggingFace token\nWe're going to use the pre-trained stable diffusion model in\nHuggingFace's `diffusers` library. To gain access, you need to sign in to your\nHuggingFace account ([sign up here](https://huggingface.co/join)) and request\naccess on the [model card page](https://huggingface.co/runwayml/stable-diffusion-v1-5).\nNext, [create a HuggingFace access token](https://huggingface.co/settings/tokens).\nTo access the token in a Modal function, we can create a secret on the\n[secrets page](https://modal.com/secrets). Let's use the environment variable\nnamed `HUGGINGFACE_TOKEN`. Functions that inject this secret will have access\nto the environment variable.\n![create a huggingface token](./huggingface_token.png)\n### Model caching\nThe `diffusers` library downloads the weights for a pre-trained model to a local\ndirectory, if those weights don't already exist. To decrease start-up time, we want\nthis download to happen just once, even across separate function invocations.\nTo accomplish this, we use simple function that will run at image build time and save the model into\nthe image's filesystem.\n```python\nCACHE_PATH = \"/root/model_cache\"\ndef fetch_model(local_files_only: bool = False):\n    from diffusers import StableDiffusionPipeline\n    from torch import float16\n    return StableDiffusionPipeline.from_pretrained(\n        \"runwayml/stable-diffusion-v1-5\",\n        use_auth_token=os.environ[\"HUGGINGFACE_TOKEN\"],\n        variant=\"fp16\",\n        torch_dtype=float16,\n        device_map=\"auto\",\n        cache_dir=CACHE_PATH,  # reads model saved in the modal.Image's filesystem.\n        local_files_only=local_files_only,\n    )\nimage = (\n    Image.debian_slim()\n    .run_commands(\n        \"pip install torch --extra-index-url https://download.pytorch.org/whl/cu117\"\n    )\n    .pip_install(\n        \"diffusers\",\n        \"huggingface-hub\",\n        \"safetensors\",\n        \"transformers\",\n        \"scipy\",\n        \"ftfy\",\n        \"accelerate\",\n    )\n    .run_function(fetch_model, secret=Secret.from_name(\"huggingface-secret\"))\n)\n```\n### The actual function\nNow that we have our token and `modal.Image` set up, we can put everything together.\nLet's define a function that takes a text prompt and an optional channel name\n(so we can post results to Slack if the value is set) and runs stable diffusion.\nThe `@stub.function()` decorator declares all the resources this function will\nuse: we configure it to use a GPU, run on an image that has all the packages and files we\nneed to run the model, and\nalso provide it the secret that contains the token we created above.\n```python\n@stub.function(\n    gpu=\"A10G\",\n    image=image,\n    secret=Secret.from_name(\"huggingface-secret\"),\n)\nasync def run_stable_diffusion(prompt: str, channel_name: Optional[str] = None):\n    pipe = fetch_model(local_files_only=True)\n    image = pipe(prompt, num_inference_steps=100).images[0]\n    # Convert PIL Image to PNG byte array.\n    with io.BytesIO() as buf:\n        image.save(buf, format=\"PNG\")\n        img_bytes = buf.getvalue()\n    if channel_name:\n        # `post_image_to_slack` is implemented further below.\n        post_image_to_slack.remote(prompt, channel_name, img_bytes)\n    return img_bytes\n```\n## Slack webhook\nNow that we wrote our function, we'd like to trigger it from Slack. We can do\nthis with [slash commands](https://api.slack.com/interactivity/slash-commands)\n\u2014 a feature that lets you register prefixes (such as `/run-my-bot`) to\ntrigger webhooks of your choice.\nTo serve our model as a web endpoint, we apply the\n[`@stub.web_endpoint`](/docs/guide/webhooks#web_endpoint) decorator in addition to\n`@stub.function()`. Modal webhooks are [FastAPI](https://fastapi.tiangolo.com/)\nendpoints by default (though we accept any ASGI web framework). This webhook\nretrieves the form body passed from Slack.\nInstead of blocking on the result of the stable diffusion model (which could\ntake some time), we want to notify the user immediately that their request\nis being processed. Modal Functions let you\n[`spawn`](/docs/reference/modal.Function#spawn) an input without waiting for\nthe results, which we use here to kick off model inference as a background task.\n```python\nfrom fastapi import Request\n@stub.function()\n@web_endpoint(method=\"POST\")\nasync def entrypoint(request: Request):\n    body = await request.form()\n    prompt = body[\"text\"]\n    run_stable_diffusion.spawn(prompt, body[\"channel_name\"])\n    return f\"Running stable diffusion for {prompt}.\"\n```\n## Post to Slack\nFinally, let's define a function to post images to a Slack channel.\nFirst, we need to create a Slack app and store the token for our app as a\nModal secret. To do so, visit the the Modal [Secrets](/secrets) page and click\n\"create a Slack secret\". Then, you will find instructions on how to create a\nSlack app, give it OAuth permissions, and get a token. Note that you need to\nadd the `file:write` OAuth scope to the created app.\n![create a slack secret](./slack_secret.png)\nBelow, we use the secret and `slack-sdk` to post to a Slack channel.\n```python\n@stub.function(\n    image=Image.debian_slim().pip_install(\"slack-sdk\"),\n    secret=Secret.from_name(\"stable-diff-slackbot-secret\"),\n)\ndef post_image_to_slack(title: str, channel_name: str, image_bytes: bytes):\n    import slack_sdk\n    client = slack_sdk.WebClient(token=os.environ[\"SLACK_BOT_TOKEN\"])\n    client.files_upload(channels=channel_name, title=title, content=image_bytes)\n```\n## Deploy the Slackbot\nThat's all the code we need! To deploy your application, run\n```shell\nmodal deploy stable_diffusion_slackbot.py\n```\nIf successful, this will print a URL for your new webhook. To point your Slack\napp at it:\n- Go back to the [Slack apps page](https://api.slack.com/apps/).\n- Find your app and navigate to \"Slash Commands\" under \"Features\" in the left\n  sidebar.\n- Click on \"Create New Command\" and paste the webhook URL from Modal into the\n  \"Request URL\" field.\n- Name the command whatever you like, and hit \"Save\".\n- Reinstall the app to your workspace.\nWe're done! \ud83c\udf89 Install the app to any channel you're in, and you can trigger it\nwith the command you chose above.\n## Run Manually\nWe can also trigger `run_stable_diffusion` manually for easier debugging.\n```python\n@stub.local_entrypoint()\ndef run(\n    prompt: str = \"oil painting of a shiba\",\n    output_dir: str = \"/tmp/stable-diffusion\",\n):\n    os.makedirs(output_dir, exist_ok=True)\n    img_bytes = run_stable_diffusion.remote(prompt)\n    output_path = os.path.join(output_dir, \"output.png\")\n    with open(output_path, \"wb\") as f:\n        f.write(img_bytes)\n    print(f\"Wrote data to {output_path}\")\n```\nThis code lets us call our script as follows:\n```shell\nmodal run stable_diffusion_slackbot.py --prompt \"a photo of an astronaut riding a horse on mars\"\n```\nThe resulting image can be found in `/tmp/stable-diffusion/output.png`.\n"}
{"text": "\n```python\nimport os\nimport modal\nstub = modal.Stub(\"example-count-faces\")\nopen_cv_image = (\n    modal.Image.debian_slim()\n    .apt_install(\"python3-opencv\")\n    .pip_install(\"opencv-python\", \"numpy\")\n)\n@stub.function(image=open_cv_image)\ndef count_faces(image_bytes):\n    import cv2\n    import numpy as np\n    # Example borrowed from https://towardsdatascience.com/face-detection-in-2-minutes-using-opencv-python-90f89d7c0f81\n    # Load the cascade\n    face_cascade = cv2.CascadeClassifier(\n        os.path.join(\n            cv2.data.haarcascades, \"haarcascade_frontalface_default.xml\"\n        )\n    )\n    # Read the input image\n    np_bytes = np.frombuffer(image_bytes, dtype=np.uint8)\n    img = cv2.imdecode(np_bytes, cv2.IMREAD_COLOR)\n    # Convert into grayscale\n    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n    # Detect faces\n    faces = face_cascade.detectMultiScale(gray, 1.1, 4)\n    return len(faces)\nif __name__ == \"__main__\":\n    # Code below could have been put in a different file, but keeping it in one place for cohesion\n    import sanic\n    app = sanic.Sanic(\"web_worker_example\")\n    @app.get(\"/\")\n    def index(request):\n        return sanic.html(\n            \"\"\"\n<html>\n<form action=\"/process\" method=\"post\" enctype=\"multipart/form-data\">\n    <input type=\"file\" name=\"file\" id=\"file\" />\n    <input type=\"submit\" />\n</form>\n</html>\n    \"\"\"\n        )\n    @app.post(\"/process\")\n    async def process(request: sanic.Request):\n        input_file = request.files[\"file\"][0]\n        async with app.run():  # type: ignore\n            num_faces = await count_faces.remote(input_file.body)\n        return sanic.json({\"faces\": num_faces})\n    stub.run(auto_reload=True, debug=True)\n```\n"}
{"text": "\n```python\n\"\"\"Example of setting up tailscale VPN (https://tailscale.com/) as a sidecar\nIn this example, it's used to access a private API in another machine on the tailnet\nThe `tailscale_sidecar` utility function contains most of the setup, and needs to\nbe called from the container defined in `tailscale_image`.\n\"\"\"\nimport contextlib\nimport os\nimport subprocess\nimport time\nimport modal\nTAILSCALE_DOWNLOAD = \"tailscale_1.36.0_amd64.tgz\"\ntailscale_image = (\n    modal.Image.debian_slim()\n    .apt_install(\"wget\")\n    .dockerfile_commands(\n        [\n            \"WORKDIR /tailscale\",\n            f\"RUN wget https://pkgs.tailscale.com/stable/{TAILSCALE_DOWNLOAD} && \\\ntar xzf {TAILSCALE_DOWNLOAD} --strip-components=1\",\n            \"RUN mkdir -p /tmp/tailscale\",\n        ]\n    )\n    .pip_install(\"requests[socks]\")\n)\nstub = modal.Stub(image=tailscale_image)\n@contextlib.contextmanager\ndef tailscale_sidecar(tailscale_authkey, show_output=False):\n    \"\"\"Context manager that sets up a tailscale userspace sidecar\n    Enables both SOCKS5 and HTTP proxies on localhost:1055 and sets\n    environment variables ALL_PROXY, HTTP_PROXY and http_proxy accordingly\n    \"\"\"\n    PROXY_SIDECAR_CMD = [\n        \"/tailscale/tailscaled\",\n        \"--tun=userspace-networking\",\n        \"--socks5-server=localhost:1055\",\n        \"--outbound-http-proxy-listen=localhost:1055\",\n    ]\n    AUTH_CMD = [\n        \"/tailscale/tailscale\",\n        \"up\",\n        f\"--authkey={tailscale_authkey}\",\n        \"--hostname=modal-app\",\n    ]\n    if not show_output:\n        output_args = dict(stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n    else:\n        output_args = {}\n    with subprocess.Popen(PROXY_SIDECAR_CMD, **output_args) as p:  # type: ignore\n        subprocess.check_call(AUTH_CMD)\n        # wait for tailscale to fully configure network, otherwise proxies can fail:\n        time.sleep(2)\n        os.environ[\"ALL_PROXY\"] = \"socks5://localhost:1055/\"\n        for key in [\"HTTP_PROXY\", \"http_proxy\"]:\n            os.environ[key] = \"http://localhost:1055/\"\n        yield\n        subprocess.check_call(\n            [\"/tailscale/tailscale\", \"logout\"]\n        )  # removes node from tailnet\n        p.kill()  # stop sidecar daemon\n@stub.function(secrets=[modal.Secret.from_name(\"tailscale-auth\")])\ndef tail():\n    import requests\n    TAILSCALE_AUTHKEY = os.environ[\"TAILSCALE_AUTHKEY\"]\n    with tailscale_sidecar(TAILSCALE_AUTHKEY, show_output=False):\n        resp = requests.get(\"http://raspberrypi:5000\")\n        print(resp.content)\n```\n"}
{"text": "\n```python\nimport modal\nstub = modal.Stub(\"example-pyjulia\")\nstub.image = (\n    modal.Image.debian_slim()\n    # Install Julia 1.7\n    .apt_install(\"wget\", \"ca-certificates\")\n    .run_commands(\n        \"wget -nv https://julialang-s3.julialang.org/bin/linux/x64/1.7/julia-1.7.2-linux-x86_64.tar.gz\",\n        \"tar -xf julia-1.7.2-linux-x86_64.tar.gz\",\n        \"cp -r julia-1.7.2 /opt/\",\n        \"ln -s /opt/julia-1.7.2/bin/julia /usr/local/bin/julia\",\n    )\n    # Install PyJulia bindings\n    .pip_install(\"julia\")\n    .run_commands('python -c \"import julia; julia.install()\"')\n)\n@stub.function()\ndef julia_subprocess():\n    \"\"\"Run the Julia interpreter as a subprocess.\"\"\"\n    import subprocess\n    print(\"-> Calling Julia as a subprocess...\")\n    subprocess.run('julia -e \"println(2 + 3)\"', shell=True)\n@stub.function()\ndef julia_matrix_determinant():\n    \"\"\"Compute the determinant of a random matrix with PyJulia.\"\"\"\n    from julia.Base import rand\n    from julia.LinearAlgebra import det\n    print(\"-> Calling Julia using PyJulia...\")\n    print(det(rand(5, 5)))\n    print(det(rand(10, 10)))\n@stub.local_entrypoint()\ndef run():\n    julia_subprocess.remote()\n    julia_matrix_determinant.remote()\n```\n"}
{"text": "# PyTorch with CUDA GPU support\nThis example shows how you can use CUDA GPUs in Modal, with a minimal PyTorch\nimage. You can specify GPU requirements in the `stub.function` decorator.\n```python\nimport time\nimport modal\nstub = modal.Stub(\n    \"example-import-torch\",\n    image=modal.Image.debian_slim().pip_install(\n        \"torch\", find_links=\"https://download.pytorch.org/whl/cu116\"\n    ),\n)\n@stub.function(gpu=\"any\")\ndef gpu_function():\n    import subprocess\n    import torch\n    subprocess.run([\"nvidia-smi\"])\n    print(\"Torch version:\", torch.__version__)\n    print(\"CUDA available:\", torch.cuda.is_available())\n    print(\"CUDA device count:\", torch.cuda.device_count())\nif __name__ == \"__main__\":\n    t0 = time.time()\n    with stub.run():\n        gpu_function.remote()\n    print(\"Full time spent:\", time.time() - t0)\n```\n"}
{"text": "\n# Play with the ControlNet demos\nThis example allows you to play with all 10 demonstration Gradio apps from the new and amazing ControlNet project.\nControlNet provides a minimal interface allowing users to use images to constrain StableDiffusion's generation process.\nWith ControlNet, users can easily condition the StableDiffusion image generation with different spatial contexts\nincluding a depth maps, segmentation maps, scribble drawings, and keypoints!\n<center>\n<video controls>\n<source src=\"https://user-images.githubusercontent.com/12058921/222927911-3ab52dd1-f2ee-4fb8-97e8-dafbf96ed5c5.mp4\" type=\"video/mp4\">\n</video>\n</center>\n## Imports and config preamble\n```python\nimport importlib\nimport os\nimport pathlib\nfrom dataclasses import dataclass, field\nfrom fastapi import FastAPI\nfrom modal import Image, Secret, Stub, asgi_app\n```\nBelow are the configuration objects for all **10** demos provided in the original [lllyasviel/ControlNet](https://github.com/lllyasviel/ControlNet) repo.\nThe demos each depend on their own custom pretrained StableDiffusion model, and these models are 5-6GB each.\nWe can only run one demo at a time, so this module avoids downloading the model and 'detector' dependencies for\nall 10 demos and instead uses the demo configuration object to download only what's necessary for the chosen demo.\nEven just limiting our dependencies setup to what's required for one demo, the resulting container image is *huge*.\n```python\n@dataclass(frozen=True)\nclass DemoApp:\n    \"\"\"Config object defining a ControlNet demo app's specific dependencies.\"\"\"\n    name: str\n    model_files: list[str]\n    detector_files: list[str] = field(default_factory=list)\ndemos = [\n    DemoApp(\n        name=\"canny2image\",\n        model_files=[\n            \"https://huggingface.co/lllyasviel/ControlNet/resolve/main/models/control_sd15_canny.pth\"\n        ],\n    ),\n    DemoApp(\n        name=\"depth2image\",\n        model_files=[\n            \"https://huggingface.co/lllyasviel/ControlNet/resolve/main/models/control_sd15_depth.pth\"\n        ],\n        detector_files=[\n            \"https://huggingface.co/lllyasviel/ControlNet/resolve/main/annotator/ckpts/dpt_hybrid-midas-501f0c75.pt\"\n        ],\n    ),\n    DemoApp(\n        name=\"fake_scribble2image\",\n        model_files=[\n            \"https://huggingface.co/lllyasviel/ControlNet/resolve/main/models/control_sd15_scribble.pth\"\n        ],\n        detector_files=[\n            \"https://huggingface.co/lllyasviel/ControlNet/resolve/main/annotator/ckpts/network-bsds500.pth\"\n        ],\n    ),\n    DemoApp(\n        name=\"hed2image\",\n        model_files=[\n            \"https://huggingface.co/lllyasviel/ControlNet/resolve/main/models/control_sd15_hed.pth\"\n        ],\n        detector_files=[\n            \"https://huggingface.co/lllyasviel/ControlNet/resolve/main/annotator/ckpts/network-bsds500.pth\"\n        ],\n    ),\n    DemoApp(\n        name=\"hough2image\",\n        model_files=[\n            \"https://huggingface.co/lllyasviel/ControlNet/resolve/main/models/control_sd15_mlsd.pth\"\n        ],\n        detector_files=[\n            \"https://huggingface.co/lllyasviel/ControlNet/resolve/main/annotator/ckpts/mlsd_large_512_fp32.pth\",\n            \"https://huggingface.co/lllyasviel/ControlNet/resolve/main/annotator/ckpts/mlsd_tiny_512_fp32.pth\",\n        ],\n    ),\n    DemoApp(\n        name=\"normal2image\",\n        model_files=[\n            \"https://huggingface.co/lllyasviel/ControlNet/resolve/main/models/control_sd15_normal.pth\"\n        ],\n    ),\n    DemoApp(\n        name=\"pose2image\",\n        model_files=[\n            \"https://huggingface.co/lllyasviel/ControlNet/resolve/main/models/control_sd15_openpose.pth\"\n        ],\n        detector_files=[\n            \"https://huggingface.co/lllyasviel/ControlNet/resolve/main/annotator/ckpts/body_pose_model.pth\",\n            \"https://huggingface.co/lllyasviel/ControlNet/resolve/main/annotator/ckpts/hand_pose_model.pth\",\n        ],\n    ),\n    DemoApp(\n        name=\"scribble2image\",\n        model_files=[\n            \"https://huggingface.co/lllyasviel/ControlNet/resolve/main/models/control_sd15_scribble.pth\"\n        ],\n    ),\n    DemoApp(\n        name=\"scribble2image_interactive\",\n        model_files=[\n            \"https://huggingface.co/lllyasviel/ControlNet/resolve/main/models/control_sd15_scribble.pth\"\n        ],\n    ),\n    DemoApp(\n        name=\"seg2image\",\n        model_files=[\n            \"https://huggingface.co/lllyasviel/ControlNet/resolve/main/models/control_sd15_seg.pth\"\n        ],\n        detector_files=[\n            \"https://huggingface.co/lllyasviel/ControlNet/resolve/main/annotator/ckpts/upernet_global_small.pth\"\n        ],\n    ),\n]\ndemos_map: dict[str, DemoApp] = {d.name: d for d in demos}\n```\n## Pick a demo, any demo\nSimply by changing the `DEMO_NAME` below, you can change which ControlNet demo app is setup\nand run by this Modal script.\n```python\nDEMO_NAME = \"scribble2image\"  # Change this value to change the active demo app.\nselected_demo = demos_map[DEMO_NAME]\n```\n## Setting up the dependencies\nControlNet requires *a lot* of dependencies which could be fiddly to setup manually, but Modal's programmatic\ncontainer image building Python APIs handle this complexity straightforwardly and automatically.\nTo run any of the 10 demo apps, we need the following:\n1. a base Python 3 Linux image (we use Debian Slim)\n2. a bunch of third party PyPi packages\n3. `git`, so that we can download the ControlNet source code (there's no `controlnet` PyPi package)\n4. some image process Linux system packages, including `ffmpeg`\n5. and demo specific pre-trained model and detector `.pth` files\nThat's a lot! Fortunately, the code below is already written for you that stitches together a working container image\nready to produce remarkable ControlNet images.\n**Note:** a ControlNet model pipeline is [now available in Huggingface's `diffusers` package](https://huggingface.co/blog/controlnet). But this does not contain the demo apps.\n```python\ndef download_file(url: str, output_path: pathlib.Path):\n    import httpx\n    from tqdm import tqdm\n    with open(output_path, \"wb\") as download_file:\n        with httpx.stream(\"GET\", url, follow_redirects=True) as response:\n            total = int(response.headers[\"Content-Length\"])\n            with tqdm(\n                total=total, unit_scale=True, unit_divisor=1024, unit=\"B\"\n            ) as progress:\n                num_bytes_downloaded = response.num_bytes_downloaded\n                for chunk in response.iter_bytes():\n                    download_file.write(chunk)\n                    progress.update(\n                        response.num_bytes_downloaded - num_bytes_downloaded\n                    )\n                    num_bytes_downloaded = response.num_bytes_downloaded\ndef download_demo_files() -> None:\n    \"\"\"\n    The ControlNet repo instructs: 'Make sure that SD models are put in \"ControlNet/models\".'\n    'ControlNet' is just the repo root, so we place in /root/models.\n    The ControlNet repo also instructs: 'Make sure that... detectors are put in \"ControlNet/annotator/ckpts\".'\n    'ControlNet' is just the repo root, so we place in /root/annotator/ckpts.\n    \"\"\"\n    demo = demos_map[os.environ[\"DEMO_NAME\"]]\n    models_dir = pathlib.Path(\"/root/models\")\n    for url in demo.model_files:\n        filepath = pathlib.Path(url).name\n        download_file(url=url, output_path=models_dir / filepath)\n        print(f\"download complete for {filepath}\")\n    detectors_dir = pathlib.Path(\"/root/annotator/ckpts\")\n    for url in demo.detector_files:\n        filepath = pathlib.Path(url).name\n        download_file(url=url, output_path=detectors_dir / filepath)\n        print(f\"download complete for {filepath}\")\n    print(\"\ud83c\udf89 finished baking demo file(s) into image.\")\nimage = (\n    Image.debian_slim(python_version=\"3.10\")\n    .pip_install(\n        \"gradio==3.16.2\",\n        \"albumentations==1.3.0\",\n        \"opencv-contrib-python\",\n        \"imageio==2.9.0\",\n        \"imageio-ffmpeg==0.4.2\",\n        \"pytorch-lightning==1.5.0\",\n        \"omegaconf==2.1.1\",\n        \"test-tube>=0.7.5\",\n        \"streamlit==1.12.1\",\n        \"einops==0.3.0\",\n        \"transformers==4.19.2\",\n        \"webdataset==0.2.5\",\n        \"kornia==0.6\",\n        \"open_clip_torch==2.0.2\",\n        \"invisible-watermark>=0.1.5\",\n        \"streamlit-drawable-canvas==0.8.0\",\n        \"torchmetrics==0.6.0\",\n        \"timm==0.6.12\",\n        \"addict==2.4.0\",\n        \"yapf==0.32.0\",\n        \"prettytable==3.6.0\",\n        \"safetensors==0.2.7\",\n        \"basicsr==1.4.2\",\n        \"tqdm~=4.64.1\",\n    )\n    # xformers library offers performance improvement.\n    .pip_install(\"xformers\", pre=True)\n    .apt_install(\"git\")\n    # Here we place the latest ControlNet repository code into /root.\n    # Because /root is almost empty, but not entirely empty, `git clone` won't work,\n    # so this `init` then `checkout` workaround is used.\n    .run_commands(\n        \"cd /root && git init .\",\n        \"cd /root && git remote add --fetch origin https://github.com/lllyasviel/ControlNet.git\",\n        \"cd /root && git checkout main\",\n    )\n    .apt_install(\"ffmpeg\", \"libsm6\", \"libxext6\")\n    .run_function(\n        download_demo_files, secret=Secret.from_dict({\"DEMO_NAME\": DEMO_NAME})\n    )\n)\nstub = Stub(name=\"example-controlnet\", image=image)\nweb_app = FastAPI()\n```\n## Serving the Gradio web UI\nEach ControlNet gradio demo module exposes a `block` Gradio interface running in queue-mode,\nwhich is initialized in module scope on import and served on `0.0.0.0`. We want the block interface object,\nbut the queueing and launched webserver aren't compatible with Modal's serverless web endpoint interface,\nso in the `import_gradio_app_blocks` function we patch out these behaviors.\n```python\ndef import_gradio_app_blocks(demo: DemoApp):\n    from gradio import blocks\n    # The ControlNet repo demo scripts are written to be run as\n    # standalone scripts, and have a lot of code that executes\n    # in global scope on import, including the launch of a Gradio web server.\n    # We want Modal to control the Gradio web app serving, so we\n    # monkeypatch the .launch() function to be a no-op.\n    blocks.Blocks.launch = lambda self, server_name: print(\n        \"launch() has been monkeypatched to do nothing.\"\n    )\n    # each demo app module is a file like gradio_{name}.py\n    module_name = f\"gradio_{demo.name}\"\n    mod = importlib.import_module(module_name)\n    blocks = mod.block\n    # disable queueing mode, which is incompatible with our Modal web app setup.\n    blocks.enable_queue = False\n    return blocks\n```\nBecause the ControlNet gradio app's are so time and compute intensive to cold-start\nthe web app function is limited to running just 1 warm container. This way, while playing\nwith the demos we can pay the cold-start cost once and have all web requests hit the warm\ncontainer. Spinning up extra containers to handle additional requests would not be efficient\ngiven the cold-start time.\n```python\n@stub.function(\n    gpu=\"A10G\",\n    concurrency_limit=1,\n    keep_warm=1,\n)\n@asgi_app()\ndef run():\n    from gradio.routes import mount_gradio_app\n    # mount for execution on Modal\n    return mount_gradio_app(\n        app=web_app,\n        blocks=import_gradio_app_blocks(demo=selected_demo),\n        path=\"/\",\n    )\n```\n## Have fun!\nServe your chosen demo app with `modal serve controlnet_gradio_demos.py`. If you don't have any images ready at hand,\ntry one that's in the `06_gpu_and_ml/controlnet/demo_images/` folder.\nStableDiffusion was already impressive enough, but ControlNet's ability to so accurately and intuitively constrain\nthe image generation process is sure to put a big, dumb grin on your face.\n"}
{"text": "\n# Serve a dynamic SVG badge\nIn this example, we use Modal's [webhook](/docs/guide/webhooks) capability to host a dynamic SVG badge that shows\nyou the current # of downloads for a Python package.\nFirst let's start off by creating a Modal stub, and defining an image with the Python packages we're going to be using:\n```python\nfrom modal import Image, Stub, web_endpoint\nstub = Stub(\n    \"example-web-badges\",\n    image=Image.debian_slim().pip_install(\"pybadges\", \"pypistats\"),\n)\n```\n## Defining the web endpoint\nIn addition to using `@stub.function()` to decorate our function, we use the\n`@modal.web_endpoint` decorator ([learn more](/docs/guide/webhooks#web_endpoint)), which instructs Modal\nto create a REST endpoint that serves this function. Note that the default method is `GET`, but this\ncan be overridden using the `method` argument.\n```python\n@stub.function()\n@web_endpoint()\nasync def package_downloads(package_name: str):\n    import json\n    import pypistats\n    from fastapi import Response\n    from pybadges import badge\n    stats = json.loads(pypistats.recent(package_name, format=\"json\"))\n    svg = badge(\n        left_text=f\"{package_name} downloads\",\n        right_text=str(stats[\"data\"][\"last_month\"]),\n        right_color=\"blue\",\n    )\n    return Response(content=svg, media_type=\"image/svg+xml\")\n```\nIn this function, we use `pypistats` to query the most recent stats for our package, and then\nuse that as the text for a SVG badge, rendered using `pybadges`.\nSince Modal web endpoints are FastAPI functions under the hood, we return this SVG wrapped in a FastAPI response with the correct media type.\nAlso note that FastAPI automatically interprets `package_name` as a [query param](https://fastapi.tiangolo.com/tutorial/query-params/).\n## Running and deploying\nWe can now run an ephemeral app on the command line using:\n```shell\nmodal serve badges.py\n```\nThis will create a short-lived web url that exists until you terminate the script.\nIt will also hot-reload the code if you make changes to it.\nIf you want to create a persistent URL, you have to deploy the script.\nTo deploy using the Modal CLI by running `modal deploy badges.py`,\nEither way, as soon as we run this command, Modal gives us the link to our brand new\nweb endpoint in the output:\n![web badge deployment](./badges_deploy.png)\nWe can now visit the link using a web browser, using a `package_name` of our choice in the URL query params.\nFor example:\n- `https://YOUR_SUBDOMAIN.modal.run/?package_name=synchronicity`\n- `https://YOUR_SUBDOMAIN.modal.run/?package_name=torch`\n"}
{"text": "\nThis is a simple demonstration of how to run a dbt-core project on Modal\nusing the dbt-sqlite adapter.\nThe underlying DBT data and models are from https://docs.getdbt.com/docs/get-started/getting-started-dbt-core\nTo run this example, first run the meltano example in `10_integrations/meltano/` to load the required data\ninto sqlite.\n**Run this example:**\n```\nmodal run dbt_sqlite.py::stub.run\n```\n**Launch an interactive sqlite3 shell on the output database:**\n```\nmodal run dbt_sqlite.py::stub.explore\n```\n```python\nimport os\nimport subprocess\nimport sys\nimport typing\nfrom pathlib import Path\nimport modal\nLOCAL_DBT_PROJECT = Path(__file__).parent / \"sample_proj_sqlite\"\nREMOTE_DBT_PROJECT = \"/sample_proj\"\nRAW_SCHEMAS = \"/raw\"\nOUTPUT_SCHEMAS = \"/db\"\n```\nCreate an environment dict that will be usable by dbt templates:\n```python\ndbt_env = modal.Secret.from_dict(\n    {\n        \"RAW_DB_PATH\": f\"{RAW_SCHEMAS}/jaffle_shop_raw.db\",\n        \"OUTPUT_SCHEMAS_PATH\": OUTPUT_SCHEMAS,\n        \"MAIN_DB_PATH\": f\"{OUTPUT_SCHEMAS}/main.db\",\n    }\n)\nimage = (\n    modal.Image.debian_slim()\n    .pip_install(\"dbt-core~=1.3.0\", \"dbt-sqlite~=1.3.0\")\n    .run_commands(\"apt-get install -y git\")\n)\n```\nraw data loaded by meltano, see the meltano example in 10_integrations/meltano\n```python\nraw_volume = modal.NetworkFileSystem.from_name(\"meltano_volume\")\n```\noutput schemas\n```python\ndb_volume = modal.NetworkFileSystem.persisted(\"dbt_dbs\")\nproject_mount = modal.Mount.from_local_dir(\n    LOCAL_DBT_PROJECT, remote_path=REMOTE_DBT_PROJECT\n)\nstub = modal.Stub(image=image, mounts=[project_mount], secrets=[dbt_env])\n@stub.function(\n    network_file_systems={RAW_SCHEMAS: raw_volume, OUTPUT_SCHEMAS: db_volume}\n)\ndef dbt_cli(subcommand: typing.List):\n    os.chdir(REMOTE_DBT_PROJECT)\n    cmd = [\"dbt\"] + subcommand\n    print(f\"Running {' '.join(cmd)} against {REMOTE_DBT_PROJECT}\")\n    subprocess.check_call(cmd)\n@stub.local_entrypoint()\ndef run():\n    dbt_cli.remote([\"run\"])\n@stub.local_entrypoint()\ndef debug():\n    dbt_cli.remote([\"debug\"])\n@stub.function(\n    interactive=sys.stdout.isatty(),\n    network_file_systems={RAW_SCHEMAS: raw_volume, OUTPUT_SCHEMAS: db_volume},\n    timeout=86400,\n    image=modal.Image.debian_slim().apt_install(\"sqlite3\"),\n)\ndef explore():\n    # explore the output database interactively using the sqlite3 shell\n    os.execlp(\"sqlite3\", \"sqlite3\", os.environ[\"MAIN_DB_PATH\"])\n```\n"}
{"text": "# Hello, world!\nThis is a trivial example of a Modal function, but it illustrates a few features:\n* You can print things to stdout and stderr.\n* You can return data.\n* You can map over a function.\n## Import Modal and define the app\nLet's start with the top level imports.\nYou need to import Modal and define the app.\nA stub is an object that defines everything that will be run.\n```python\nimport sys\nimport modal\nstub = modal.Stub(\"example-hello-world\")\n```\n## Defining a function\nHere we define a Modal function using the `modal.function` decorator.\nThe body of the function will automatically be run remotely.\nThis particular function is pretty silly: it just prints \"hello\"\nand \"world\" alternatingly to standard out and standard error.\n```python\n@stub.function()\ndef f(i):\n    if i % 2 == 0:\n        print(\"hello\", i)\n    else:\n        print(\"world\", i, file=sys.stderr)\n    return i * i\n```\n## Running it\nFinally, let's actually invoke it.\nWe put this invocation code inside a `@stub.local_entrypoint()`.\nThis is because this module will be imported in the cloud, and we don't want\nthis code to be executed a second time in the cloud.\nRun `modal run hello_world.py` and the `@stub.local_entrypoint()` decorator will handle\nstarting the Modal app and then executing the wrapped function body.\nInside the `main()` function body, we are calling the function `f` in three ways:\n1  As a simple local call, `f(1000)`\n2. As a simple *remote* call `f.remote(1000)`\n3. By mapping over the integers `0..19`\n```python\n@stub.local_entrypoint()\ndef main():\n    # Call the function locally.\n    print(f.local(1000))\n    # Call the function remotely.\n    print(f.remote(1000))\n    # Parallel map.\n    total = 0\n    for ret in f.map(range(20)):\n        total += ret\n    print(total)\n```\n## What happens?\nWhen you do `.remote` on function `f`, Modal will execute `f` **in the cloud,**\nnot locally on your computer. It will take the code, put it inside a\ncontainer, run it, and stream all the output back to your local\ncomputer.\nTry doing one of these things next.\n### Change the code and run again\nFor instance, change the `print` statement in the function `f`.\nYou can see that the latest code is always run.\nModal's goal is to make running code in the cloud feel like you're\nrunning code locally. You don't need to run any commands to rebuild,\npush containers, or go to a web UI to download logs.\n### Map over a larger dataset\nChange the map range from 20 to some large number. You can see that\nModal will create and run more containers in parallel.\nThe function `f` is obviously silly and doesn't do much, but you could\nimagine something more significant, like:\n* Training a machine learning model\n* Transcoding media\n* Backtesting a trading algorithm.\nModal lets you parallelize that operation trivially by running hundreds or\nthousands of containers in the cloud.\n"}
{"text": "\n# Using a queue to send/receive data\nThis is an example of how to use queues to send/receive data.\nWe don't do it here, but you could imagine doing this _between_ two functions.\n```python\nimport asyncio\nimport modal\nimport modal.queue\nstub = modal.Stub(\"example-queue-simple\", q=modal.Queue.new())\n@stub.function()\nasync def run_async(q: modal.Queue) -> None:\n    print(q)\n    print(q.put)\n    await q.put.aio(42)\n    r = await q.get.aio()\n    assert r == 42\n    await q.put_many.aio([42, 43, 44, 45, 46])\n    await q.put_many.aio([47, 48, 49, 50, 51])\n    r = await q.get_many.aio(3)\n    assert r == [42, 43, 44]\n    r = await q.get_many.aio(99)\n    assert r == [45, 46, 47, 48, 49, 50, 51]\n@stub.function()\nasync def many_consumers(q: modal.Queue) -> None:\n    print(\"Creating getters\")\n    tasks = [asyncio.create_task(q.get.aio()) for i in range(20)]\n    print(\"Putting values\")\n    await q.put_many.aio(list(range(10)))\n    await asyncio.sleep(1)\n    # About 10 tasks should now be done\n    n_done_tasks = sum(1 for t in tasks if t.done())\n    assert n_done_tasks == 10\n    # Finish remaining ones\n    await q.put_many.aio(list(range(10)))\n    await asyncio.sleep(1)\n    assert all(t.done() for t in tasks)\nasync def main():\n    with stub.run():\n        await run_async.remote.aio(stub.q)\n        await many_consumers.remote.aio(stub.q)\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n"}
{"text": "\n# Using the ChatGPT streaming API\nThis example shows how to stream from the ChatGPT API as the model is generating a completion, instead of\nwaiting for the entire completion to finish. This provides a much better user experience, and is what you\nget when playing with ChatGPT on [chat.openai.com](https://chat.openai.com/).\nYou can try this out from the command line using the `modal` CLI, or serve the application and use the\nincluded web endpoint.\n## Imports and Modal application configuration\nOpenAI's Python client library is the only package dependency we need. We also need an API key.\nThe former is specified in the Modal application's `image` definition, and the latter is attached to the app's\nstub as a [`modal.Secret`](/docs/guide/secrets).\n```python\nfrom modal import Image, Secret, Stub, web_endpoint\nimage = Image.debian_slim().pip_install(\"openai\")\nstub = Stub(\n    name=\"example-chatgpt-stream\",\n    image=image,\n    secrets=[Secret.from_name(\"openai-secret\")],\n)\n```\nThis is all the code needed to stream answers back from ChatGPT.\nNot much code to worry about!\nBecause this Python function is decorated with `@stub.function`, it becomes\ncallable as a Modal remote function. But `stream_chat` can still be used as a\nregular Python function, which becomes important below.\n```python\n@stub.function()\ndef stream_chat(prompt: str):\n    import openai\n    for chunk in openai.ChatCompletion.create(\n        model=\"gpt-3.5-turbo\",\n        messages=[{\"role\": \"user\", \"content\": prompt}],\n        stream=True,\n    ):\n        content = chunk[\"choices\"][0].get(\"delta\", {}).get(\"content\")\n        if content is not None:\n            yield content\n```\n## Streaming web endpoint\nThese four lines are all you need to take that function above and serve it\nover HTTP. It is a single function definition, annotated with decorators to make\nit a Modal function [with a web serving capability](/docs/guide/webhooks).\nNotice that the `stream_chat` function is passed into the retuned streaming response.\nThis works because the function is a generator and is thus compatible with streaming.\nWe use the standard Python calling convention `stream_chat(...)` and not the\nModal-specific calling convention `stream_chat.remote(...)`. The latter would still work,\nbut it would create a remote function invocation which would unnecessarily involve `stream_chat`\nrunning in a separate container, sending its results back to the caller over the network.\n```python\n@stub.function()\n@web_endpoint()\ndef web(prompt: str):\n    from fastapi.responses import StreamingResponse\n    return StreamingResponse(stream_chat(prompt), media_type=\"text/html\")\n```\n## Try out the web endpoint\nRun this example with `modal serve chatgpt_streaming.py` and you'll see an ephemeral web endpoint\nhas started serving. Hit this endpoint with a prompt and watch the ChatGPT response streaming back in\nyour browser or terminal window.\nWe've also already deployed this example and so you can try out our deployed web endpoint:\n```bash\ncurl --get \\\n  --data-urlencode \"prompt=Generate a list of 20 great names for sentient cheesecakes that teach SQL\" \\\n  https://modal-labs--example-chatgpt-stream-web.modal.run\n```\n## CLI interface\nDoing `modal run chatgpt_streaming.py --prompt=\"Generate a list of the world's most famous people\"` also works, and uses the `local_entrypoint` defined below.\n```python\ndefault_prompt = (\n    \"Generate a list of 20 great names for sentient cheesecakes that teach SQL\"\n)\n@stub.local_entrypoint()\ndef main(prompt: str = default_prompt):\n    for part in stream_chat.remote_gen(prompt=prompt):\n        print(part, end=\"\")\n```\n"}
{"text": "\n# Batch inference using a model from Huggingface\n<center>\n  <img src=\"./batch_inference_huggingface.png\" alt=\"Huggingface company logo\" />\n</center>\nThis example shows how to use a sentiment analysis model from Huggingface to classify\n25,000 movie reviews in a couple of minutes.\nSome Modal features it uses:\n* Container lifecycle hook: this lets us load the model only once in each container\n* CPU requests: the prediction function is very CPU-hungry, so we reserve 8 cores\n* Mapping: we map over 25,000 sentences and Modal manages the pool of containers for us\n## Basic setup\nLet's get started writing code.\nFor the Modal container image, we need a few Python packages,\nincluding `transformers`, which is the main Huggingface package.\n```python\nimport io\nimport modal\nstub = modal.Stub(\n    \"example-batch-inference-using-huggingface\",\n    image=modal.Image.debian_slim().pip_install(\n        \"datasets\",\n        \"matplotlib\",\n        \"scikit-learn\",\n        \"torch\",\n        \"transformers\",\n    ),\n)\n```\n## Defining the prediction function\nInstead of a using `@stub.function()` in the global scope,\nwe put the method on a class, and define an `__enter__` method on that class.\nModal reuses containers for successive calls to the same function, so\nwe want to take advantage of this and avoid setting up the same model\nfor every function call.\nSince the transformer model is very CPU-hungry, we allocate 8 CPUs\nto the model.\nEvery container that runs will have 8 CPUs set aside for it.\n```python\n@stub.cls(cpu=8, retries=3)\nclass SentimentAnalysis:\n    def __enter__(self):\n        from transformers import pipeline\n        self.sentiment_pipeline = pipeline(\n            model=\"distilbert-base-uncased-finetuned-sst-2-english\"\n        )\n    @modal.method()\n    def predict(self, phrase: str):\n        pred = self.sentiment_pipeline(\n            phrase, truncation=True, max_length=512, top_k=2\n        )\n        # pred will look like: [{'label': 'NEGATIVE', 'score': 0.99}, {'label': 'POSITIVE', 'score': 0.01}]\n        probs = {p[\"label\"]: p[\"score\"] for p in pred}\n        return probs[\"POSITIVE\"]\n```\n## Getting data\nWe need some data to run the batch inference on.\nWe use this [dataset of IMDB reviews](https://ai.stanford.edu/~amaas/data/sentiment/) for this purpose.\nHuggingface actually offers this data [as a preprocessed dataaset](https://huggingface.co/datasets/imdb),\nwhich we can download using the `datasets` package:\n```python\n@stub.function()\ndef get_data():\n    from datasets import load_dataset\n    imdb = load_dataset(\"imdb\")\n    data = [(row[\"text\"], row[\"label\"]) for row in imdb[\"test\"]]\n    return data\n```\n## Plotting the ROC curve\nIn order to evaluate the classifier, let's plot an\n[ROC curve](https://en.wikipedia.org/wiki/Receiver_operating_characteristic).\nThis is a common way to evaluate classifiers on binary data.\n```python\n@stub.function()\ndef roc_plot(labels, predictions):\n    from matplotlib import pyplot\n    from sklearn.metrics import RocCurveDisplay\n    pyplot.style.use(\"ggplot\")\n    RocCurveDisplay.from_predictions(labels, predictions)\n    with io.BytesIO() as buf:\n        pyplot.savefig(buf, format=\"png\")\n        return buf.getvalue()\n```\nA bit of a spoiler warning, but if you run this script, the ROC curve will look like this:\n![roc](./batch_inference_roc.png)\nThe AUC of this classifier is 0.96, which means it's very good!\n## Putting it together\nThe main flow of the code downloads the data, then runs the batch inference,\nthen plots the results.\nEach prediction takes roughly 0.1-1s, so if we ran everything sequentially it would take 2,500-25,000 seconds.\nThat's a lot! Luckily because of Modal's `.map` method, we can process everything in a couple of minutes at most.\nModal will automatically spin up more and more workers until all inputs are processed.\n```python\n@stub.local_entrypoint()\ndef main():\n    print(\"Downloading data...\")\n    data = get_data.remote()\n    print(\"Got\", len(data), \"reviews\")\n    reviews = [review for review, label in data]\n    labels = [label for review, label in data]\n    # Let's check that the model works by classifying the first 5 entries\n    predictor = SentimentAnalysis()\n    for review, label in data[:5]:\n        prediction = predictor.predict.remote(review)\n        print(\n            f\"Sample prediction with positivity score {prediction}:\\n{review}\\n\\n\"\n        )\n    # Now, let's run batch inference over it\n    print(\"Running batch prediction...\")\n    predictions = list(predictor.predict.map(reviews))\n    # Generate a ROC plot\n    print(\"Creating ROC plot...\")\n    png_data = roc_plot.remote(labels, predictions)\n    fn = \"/tmp/roc.png\"\n    with open(fn, \"wb\") as f:\n        f.write(png_data)\n    print(f\"Wrote ROC curve to {fn}\")\n```\n## Running this\nWhen you run this, it will download the dataset and load the model, then output some\nsample predictions:\n```\nSample prediction with positivity score 0.0003837468393612653:\nI love sci-fi and am willing to put up with a lot. Sci-fi movies/TV are usually underfunded, under-appreciated and misunderstood. I tried to like this, I really did, but it is to good TV sci-fi as Babylon 5 is to Star Trek (the original). Silly prosthetics, cheap cardboard sets, stilted dialogues, CG that doesn't match the background, and painfully one-dimensional characters cannot be overcome with a 'sci-fi' setting. (I'm sure there are those of you out there who think Babylon 5 is good sci-fi TV. It's not. It's clich\u00e9d and uninspiring.) While US viewers might like emotion and character development, sci-fi is a genre that does not take itself seriously (cf. Star Trek). It may treat important issues, yet not as a serious philosophy. It's really difficult to care about the characters here as they are not simply foolish, just missing a spark of life. Their actions and reactions are wooden and predictable, often painful to watch. The makers of Earth KNOW it's rubbish as they have to always say \"Gene Roddenberry's Earth...\" otherwise people would not continue watching. Roddenberry's ashes must be turning in their orbit as this dull, cheap, poorly edited (watching it without advert breaks really brings this home) trudging Trabant of a show lumbers into space. Spoiler. So, kill off a main character. And then bring him back as another actor. Jeeez! Dallas all over again.\nSample prediction with positivity score 0.38294079899787903:\nWorth the entertainment value of a rental, especially if you like action movies. This one features the usual car chases, fights with the great Van Damme kick style, shooting battles with the 40 shell load shotgun, and even terrorist style bombs. All of this is entertaining and competently handled but there is nothing that really blows you away if you've seen your share before.<br /><br />The plot is made interesting by the inclusion of a rabbit, which is clever but hardly profound. Many of the characters are heavily stereotyped -- the angry veterans, the terrified illegal aliens, the crooked cops, the indifferent feds, the bitchy tough lady station head, the crooked politician, the fat federale who looks like he was typecast as the Mexican in a Hollywood movie from the 1940s. All passably acted but again nothing special.<br /><br />I thought the main villains were pretty well done and fairly well acted. By the end of the movie you certainly knew who the good guys were and weren't. There was an emotional lift as the really bad ones got their just deserts. Very simplistic, but then you weren't expecting Hamlet, right? The only thing I found really annoying was the constant cuts to VDs daughter during the last fight scene.<br /><br />Not bad. Not good. Passable 4.\nSample prediction with positivity score 0.0002899310493376106:\nits a totally average film with a few semi-alright action sequences that make the plot seem a little better and remind the viewer of the classic van dam films. parts of the plot don't make sense and seem to be added in to use up time. the end plot is that of a very basic type that doesn't leave the viewer guessing and any twists are obvious from the beginning. the end scene with the flask backs don't make sense as they are added in and seem to have little relevance to the history of van dam's character. not really worth watching again, bit disappointed in the end production, even though it is apparent it was shot on a low budget certain shots and sections in the film are of poor directed quality\nSample prediction with positivity score 0.004243704490363598:\nSTAR RATING: ***** Saturday Night **** Friday Night *** Friday Morning ** Sunday Night * Monday Morning <br /><br />Former New Orleans homicide cop Jack Robideaux (Jean Claude Van Damme) is re-assigned to Columbus, a small but violent town in Mexico to help the police there with their efforts to stop a major heroin smuggling operation into their town. The culprits turn out to be ex-military, lead by former commander Benjamin Meyers (Stephen Lord, otherwise known as Jase from East Enders) who is using a special method he learned in Afghanistan to fight off his opponents. But Jack has a more personal reason for taking him down, that draws the two men into an explosive final showdown where only one will walk away alive.<br /><br />After Until Death, Van Damme appeared to be on a high, showing he could make the best straight to video films in the action market. While that was a far more drama oriented film, with The Shepherd he has returned to the high-kicking, no brainer action that first made him famous and has sadly produced his worst film since Derailed. It's nowhere near as bad as that film, but what I said still stands.<br /><br />A dull, predictable film, with very little in the way of any exciting action. What little there is mainly consists of some limp fight scenes, trying to look cool and trendy with some cheap slo-mo/sped up effects added to them that sadly instead make them look more desperate. Being a Mexican set film, director Isaac Florentine has tried to give the film a Robert Rodriguez/Desperado sort of feel, but this only adds to the desperation.<br /><br />VD gives a particularly uninspired performance and given he's never been a Robert De Niro sort of actor, that can't be good. As the villain, Lord shouldn't expect to leave the beeb anytime soon. He gets little dialogue at the beginning as he struggles to muster an American accent but gets mysteriously better towards the end. All the supporting cast are equally bland, and do nothing to raise the films spirits at all.<br /><br />This is one shepherd that's strayed right from the flock. *\nSample prediction with positivity score 0.996307373046875:\nFirst off let me say, If you haven't enjoyed a Van Damme movie since bloodsport, you probably will not like this movie. Most of these movies may not have the best plots or best actors but I enjoy these kinds of movies for what they are. This movie is much better than any of the movies the other action guys (Segal and Dolph) have thought about putting out the past few years. Van Damme is good in the movie, the movie is only worth watching to Van Damme fans. It is not as good as Wake of Death (which i highly recommend to anyone of likes Van Damme) or In hell but, in my opinion it's worth watching. It has the same type of feel to it as Nowhere to Run. Good fun stuff!\n```\nAfter that, it kicks off the actual batch inference.\nIt should look something like the screenshot below (we are very proud of the progress bar):\n![progress](./batch_inference_progress.png)\nThe whole thing should take a few minutes to run.\n## Further optimization notes\nEvery container downloads the model when it starts, which is a bit inefficient.\nIn order to improve this, what you could do is store the model in the image that\nbacks each container.\nSee [`Image.run_function`](/docs/guide/custom-container#running-a-function-as-a-build-step-beta).\n"}
{"text": "\n# Tensorflow tutorial\nThis is essentially a version of the\n[image classification example in the Tensorflow documention](https://www.tensorflow.org/tutorials/images/classification)\nrunning inside Modal on a GPU.\nIf you run this script, it will also create an Tensorboard URL you can go to:\n![tensorboard](./tensorboard.png)\n## Setting up the dependencies\nInstalling Tensorflow in Modal is quite straightforward.\nIf you want it to run on a GPU, you need the container image to have CUDA libraries available\nto the Tensorflow package. You can use Conda to install these libraries before `pip` installing `tensorflow`, or you\ncan use Tensorflow's official GPU base image which comes with the CUDA libraries and `tensorflow` already installed.\n```python\nimport time\nfrom modal import Image, NetworkFileSystem, Stub, wsgi_app\ndockerhub_image = Image.from_registry(\n    \"tensorflow/tensorflow:latest-gpu\",\n).pip_install(\"protobuf==3.20.*\")\nconda_image = (\n    Image.conda()\n    .conda_install(\n        \"cudatoolkit=11.2\",\n        \"cudnn=8.1.0\",\n        \"cuda-nvcc\",\n        channels=[\"conda-forge\", \"nvidia\"],\n    )\n    .pip_install(\"tensorflow~=2.9.1\")\n)\nstub = Stub(\n    \"example-tensorflow-tutorial\",\n    image=conda_image or dockerhub_image,  # pick one and remove the other.\n)\n```\n## Logging data for Tensorboard\nWe want to run the web server for Tensorboard at the same time as we are training the Tensorflow model.\nThe easiest way to do this is to set up a shared filesystem between the training and the web server.\n```python\nstub.volume = NetworkFileSystem.new()\nlogdir = \"/tensorboard\"\n```\n## Training function\nThis is basically the same code as the official example.\nA few things are worth pointing out:\n* We set up the network file system in the arguments to `stub.function`\n* We also annotate this function with `gpu=\"any\"`\n* We put all the Tensorflow imports inside the function body.\n  This makes it a bit easier to run this example even if you don't have Tensorflow installed on you local computer.\n```python\n@stub.function(\n    network_file_systems={logdir: stub.volume}, gpu=\"any\", timeout=600\n)\ndef train():\n    import pathlib\n    import tensorflow as tf\n    from tensorflow.keras import layers\n    from tensorflow.keras.models import Sequential\n    dataset_url = \"https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz\"\n    data_dir = tf.keras.utils.get_file(\n        \"flower_photos\", origin=dataset_url, untar=True\n    )\n    data_dir = pathlib.Path(data_dir)\n    batch_size = 32\n    img_height = 180\n    img_width = 180\n    train_ds = tf.keras.utils.image_dataset_from_directory(\n        data_dir,\n        validation_split=0.2,\n        subset=\"training\",\n        seed=123,\n        image_size=(img_height, img_width),\n        batch_size=batch_size,\n    )\n    val_ds = tf.keras.utils.image_dataset_from_directory(\n        data_dir,\n        validation_split=0.2,\n        subset=\"validation\",\n        seed=123,\n        image_size=(img_height, img_width),\n        batch_size=batch_size,\n    )\n    class_names = train_ds.class_names\n    train_ds = (\n        train_ds.cache().shuffle(1000).prefetch(buffer_size=tf.data.AUTOTUNE)  # type: ignore\n    )\n    val_ds = val_ds.cache().prefetch(buffer_size=tf.data.AUTOTUNE)  # type: ignore\n    num_classes = len(class_names)\n    model = Sequential(\n        [\n            layers.Rescaling(1.0 / 255, input_shape=(img_height, img_width, 3)),\n            layers.Conv2D(16, 3, padding=\"same\", activation=\"relu\"),\n            layers.MaxPooling2D(),\n            layers.Conv2D(32, 3, padding=\"same\", activation=\"relu\"),\n            layers.MaxPooling2D(),\n            layers.Conv2D(64, 3, padding=\"same\", activation=\"relu\"),\n            layers.MaxPooling2D(),\n            layers.Flatten(),\n            layers.Dense(128, activation=\"relu\"),\n            layers.Dense(num_classes),\n        ]\n    )\n    model.compile(\n        optimizer=\"adam\",\n        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n        metrics=[\"accuracy\"],\n    )\n    model.summary()\n    tensorboard_callback = tf.keras.callbacks.TensorBoard(\n        log_dir=logdir,\n        histogram_freq=1,\n    )\n    model.fit(\n        train_ds,\n        validation_data=val_ds,\n        epochs=20,\n        callbacks=[tensorboard_callback],\n    )\n```\n## Running Tensorboard\nTensorboard is a WSGI-compatible web server, so it's easy to expose it in Modal.\nThe app isn't exposed directly through the Tensorboard library, but it gets\n[created in the source code](https://github.com/tensorflow/tensorboard/blob/master/tensorboard/program.py#L467)\nin a way where we can do the same thing quite easily too.\nNote that the Tensorboard server runs in a different container.\nThis container shares the same log directory containing the logs from the training.\nThe server does not need GPU support.\nNote that this server will be exposed to the public internet!\n```python\n@stub.function(network_file_systems={logdir: stub.volume})\n@wsgi_app()\ndef tensorboard_app():\n    import tensorboard\n    board = tensorboard.program.TensorBoard()\n    board.configure(logdir=logdir)\n    (data_provider, deprecated_multiplexer) = board._make_data_provider()\n    wsgi_app = tensorboard.backend.application.TensorBoardWSGIApp(\n        board.flags,\n        board.plugin_loaders,\n        data_provider,\n        board.assets_zip_provider,\n        deprecated_multiplexer,\n    )\n    return wsgi_app\n```\n## Local entrypoint code\nLet's kick everything off.\nEverything runs in an ephemeral \"app\" that gets destroyed once it's done.\nIn order to keep the Tensorboard web server running, we sleep in an infinite loop\nuntil the user hits ctrl-c.\nThe script will take a few minutes to run, although each epoch is quite fast since it runs on a GPU.\nThe first time you run it, it might have to build the image, which can take an additional few minutes.\n```python\n@stub.local_entrypoint()\ndef main(just_run: bool = False):\n    train.remote()\n    if not just_run:\n        print(\"Training is done, but app is still running until you hit ctrl-c\")\n        try:\n            while True:\n                time.sleep(1)\n        except KeyboardInterrupt:\n            print(\"Terminating app\")\n```\n"}
{"text": "Adapted from\nhttps://github.com/lm-sys/FastChat/blob/168ccc29d3f7edc50823016105c024fe2282732a/fastchat/serve/openai_api_server.py\n```python\nimport argparse\nimport asyncio\nimport json\nimport os\nimport time\nfrom http import HTTPStatus\nfrom typing import AsyncGenerator, Dict, List, Optional\nimport fastapi\nfrom fastapi import BackgroundTasks, Request\nfrom fastapi.exceptions import RequestValidationError\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom fastapi.responses import JSONResponse, StreamingResponse\n```\nModal stub setup\n```python\nfrom modal import Image, Secret, Stub, asgi_app, method\nfrom packaging import version\n```\nAvoid using global variables in the download function\n```python\ndef download_model_to_folder():\n    from huggingface_hub import snapshot_download\n    snapshot_download(\n        \"meta-llama/Llama-2-13b-chat-hf\",\n        token=os.environ[\"HUGGINGFACE_TOKEN\"],\n    )\nimage = (\n    Image.from_dockerhub(\"nvcr.io/nvidia/pytorch:22.12-py3\")\n    .pip_install(\n        \"torch==2.0.1\", index_url=\"https://download.pytorch.org/whl/cu118\"\n    )\n    # Pinned to 07/21/2023\n    .pip_install(\n        \"vllm @ git+https://github.com/vllm-project/vllm.git@d7a1c6d614756b3072df3e8b52c0998035fb453f\"\n    )\n    .run_function(\n        download_model_to_folder, secret=Secret.from_name(\"huggingface\")\n    )\n)\nstub = Stub(\"vllm-openai\", image=image)\nif stub.is_inside():\n    # Only do the imports inside the container where vLLM is already installeda\n    import uvicorn\n    from vllm.engine.arg_utils import AsyncEngineArgs\n    from vllm.engine.async_llm_engine import AsyncLLMEngine\n    from vllm.entrypoints.openai.protocol import (\n        ChatCompletionRequest,\n        ChatCompletionResponse,\n        ChatCompletionResponseChoice,\n        ChatCompletionResponseStreamChoice,\n        ChatCompletionStreamResponse,\n        ChatMessage,\n        CompletionRequest,\n        CompletionResponse,\n        CompletionResponseChoice,\n        CompletionResponseStreamChoice,\n        CompletionStreamResponse,\n        DeltaMessage,\n        ErrorResponse,\n        LogProbs,\n        ModelCard,\n        ModelList,\n        ModelPermission,\n        UsageInfo,\n    )\n    from vllm.logger import init_logger\n    from vllm.outputs import RequestOutput\n    from vllm.sampling_params import SamplingParams\n    from vllm.transformers_utils.tokenizer import get_tokenizer\n    from vllm.utils import random_uuid\n    try:\n        import fastchat\n        from fastchat.conversation import Conversation, SeparatorStyle\n        from fastchat.model.model_adapter import get_conversation_template\n        _fastchat_available = True\n    except ImportError:\n        _fastchat_available = False\nTIMEOUT_KEEP_ALIVE = 5  # seconds\nlogger = init_logger(__name__)\nserved_model = None\napp = fastapi.FastAPI()\ndef create_error_response(\n    status_code: HTTPStatus, message: str\n) -> JSONResponse:\n    return JSONResponse(\n        ErrorResponse(message=message, type=\"invalid_request_error\").dict(),\n        status_code=status_code.value,\n    )\n@app.exception_handler(RequestValidationError)\nasync def validation_exception_handler(\n    request, exc\n):  # pylint: disable=unused-argument\n    return create_error_response(HTTPStatus.BAD_REQUEST, str(exc))\nasync def check_model(request) -> Optional[JSONResponse]:\n    if request.model == served_model:\n        return\n    ret = create_error_response(\n        HTTPStatus.NOT_FOUND,\n        f\"The model `{request.model}` does not exist.\",\n    )\n    return ret\nasync def get_gen_prompt(request) -> str:\n    if not _fastchat_available:\n        raise ModuleNotFoundError(\n            \"fastchat is not installed. Please install fastchat to use \"\n            \"the chat completion and conversation APIs: `$ pip install fschat`\"\n        )\n    if version.parse(fastchat.__version__) < version.parse(\"0.2.23\"):\n        raise ImportError(\n            f\"fastchat version is low. Current version: {fastchat.__version__} \"\n            \"Please upgrade fastchat to use: `$ pip install -U fschat`\"\n        )\n    conv = get_conversation_template(request.model)\n    conv = Conversation(\n        name=conv.name,\n        system_template=conv.system_template,\n        system_message=conv.system_message,\n        roles=conv.roles,\n        messages=list(conv.messages),  # prevent in-place modification\n        offset=conv.offset,\n        sep_style=SeparatorStyle(conv.sep_style),\n        sep=conv.sep,\n        sep2=conv.sep2,\n        stop_str=conv.stop_str,\n        stop_token_ids=conv.stop_token_ids,\n    )\n    if isinstance(request.messages, str):\n        prompt = request.messages\n    else:\n        for message in request.messages:\n            msg_role = message[\"role\"]\n            if msg_role == \"system\":\n                conv.system_message = message[\"content\"]\n            elif msg_role == \"user\":\n                conv.append_message(conv.roles[0], message[\"content\"])\n            elif msg_role == \"assistant\":\n                conv.append_message(conv.roles[1], message[\"content\"])\n            else:\n                raise ValueError(f\"Unknown role: {msg_role}\")\n        # Add a blank message for the assistant.\n        conv.append_message(conv.roles[1], None)\n        prompt = conv.get_prompt()\n    return prompt\nasync def check_length(request, prompt):\n    input_ids = tokenizer(prompt).input_ids\n    token_num = len(input_ids)\n    if token_num + request.max_tokens > max_model_len:\n        return create_error_response(\n            HTTPStatus.BAD_REQUEST,\n            f\"This model's maximum context length is {max_model_len} tokens. \"\n            f\"However, you requested {request.max_tokens + token_num} tokens \"\n            f\"({token_num} in the messages, \"\n            f\"{request.max_tokens} in the completion). \"\n            f\"Please reduce the length of the messages or completion.\",\n        )\n    else:\n        return None\n@app.get(\"/v1/models\")\nasync def show_available_models():\n    \"\"\"Show available models. Right now we only have one model.\"\"\"\n    model_cards = [\n        ModelCard(\n            id=served_model, root=served_model, permission=[ModelPermission()]\n        )\n    ]\n    return ModelList(data=model_cards)\ndef create_logprobs(\n    token_ids: List[int],\n    id_logprobs: List[Dict[int, float]],\n    initial_text_offset: int = 0,\n) -> LogProbs:\n    \"\"\"Create OpenAI-style logprobs.\"\"\"\n    logprobs = LogProbs()\n    last_token_len = 0\n    for token_id, id_logprob in zip(token_ids, id_logprobs):\n        token = tokenizer.convert_ids_to_tokens(token_id)\n        logprobs.tokens.append(token)\n        logprobs.token_logprobs.append(id_logprob[token_id])\n        if len(logprobs.text_offset) == 0:\n            logprobs.text_offset.append(initial_text_offset)\n        else:\n            logprobs.text_offset.append(\n                logprobs.text_offset[-1] + last_token_len\n            )\n        last_token_len = len(token)\n        logprobs.top_logprobs.append(\n            {\n                tokenizer.convert_ids_to_tokens(i): p\n                for i, p in id_logprob.items()\n            }\n        )\n    return logprobs\n@app.post(\"/v1/chat/completions\")\nasync def create_chat_completion(raw_request: Request):\n    \"\"\"Completion API similar to OpenAI's API.\n    See  https://platform.openai.com/docs/api-reference/chat/create\n    for the API specification. This API mimics the OpenAI ChatCompletion API.\n    NOTE: Currently we do not support the following features:\n        - function_call (Users should implement this by themselves)\n        - logit_bias (to be supported by vLLM engine)\n    \"\"\"\n    request = ChatCompletionRequest(**await raw_request.json())\n    logger.info(f\"Received chat completion request: {request}\")\n    error_check_ret = await check_model(request)\n    if error_check_ret is not None:\n        return error_check_ret\n    if request.logit_bias is not None:\n        # TODO: support logit_bias in vLLM engine.\n        return create_error_response(\n            HTTPStatus.BAD_REQUEST, \"logit_bias is not currently supported\"\n        )\n    prompt = await get_gen_prompt(request)\n    error_check_ret = await check_length(request, prompt)\n    if error_check_ret is not None:\n        return error_check_ret\n    model_name = request.model\n    request_id = f\"cmpl-{random_uuid()}\"\n    created_time = int(time.time())\n    try:\n        sampling_params = SamplingParams(\n            n=request.n,\n            presence_penalty=request.presence_penalty,\n            frequency_penalty=request.frequency_penalty,\n            temperature=request.temperature,\n            top_p=request.top_p,\n            stop=request.stop,\n            max_tokens=request.max_tokens,\n            best_of=request.best_of,\n            top_k=request.top_k,\n            ignore_eos=request.ignore_eos,\n            use_beam_search=request.use_beam_search,\n        )\n    except ValueError as e:\n        return create_error_response(HTTPStatus.BAD_REQUEST, str(e))\n    result_generator = Model().generate.call(\n        prompt, sampling_params, request_id\n    )\n    # async def abort_request() -> None:\n    # await engine.abort(request_id)\n    def create_stream_response_json(\n        index: int,\n        text: str,\n        finish_reason: Optional[str] = None,\n    ) -> str:\n        choice_data = ChatCompletionResponseStreamChoice(\n            index=index,\n            delta=DeltaMessage(content=text),\n            finish_reason=finish_reason,\n        )\n        response = ChatCompletionStreamResponse(\n            id=request_id,\n            created=created_time,\n            model=model_name,\n            choices=[choice_data],\n        )\n        response_json = response.json(ensure_ascii=False)\n        return response_json\n    async def completion_stream_generator() -> AsyncGenerator[str, None]:\n        # First chunk with role\n        for i in range(request.n):\n            choice_data = ChatCompletionResponseStreamChoice(\n                index=i,\n                delta=DeltaMessage(role=\"assistant\"),\n                finish_reason=None,\n            )\n            chunk = ChatCompletionStreamResponse(\n                id=request_id, choices=[choice_data], model=model_name\n            )\n            data = chunk.json(exclude_unset=True, ensure_ascii=False)\n            yield f\"data: {data}\\n\\n\"\n        previous_texts = [\"\"] * request.n\n        previous_num_tokens = [0] * request.n\n        async for res in result_generator:\n            res: RequestOutput\n            for output in res.outputs:\n                i = output.index\n                delta_text = output.text[len(previous_texts[i]) :]\n                previous_texts[i] = output.text\n                previous_num_tokens[i] = len(output.token_ids)\n                response_json = create_stream_response_json(\n                    index=i,\n                    text=delta_text,\n                )\n                yield f\"data: {response_json}\\n\\n\"\n                if output.finish_reason is not None:\n                    response_json = create_stream_response_json(\n                        index=i,\n                        text=\"\",\n                        finish_reason=output.finish_reason,\n                    )\n                    yield f\"data: {response_json}\\n\\n\"\n        yield \"data: [DONE]\\n\\n\"\n    # Streaming response\n    if request.stream:\n        background_tasks = BackgroundTasks()\n        # Abort the request if the client disconnects.\n        # background_tasks.add_task(abort_request)\n        return StreamingResponse(\n            completion_stream_generator(),\n            media_type=\"text/event-stream\",\n            background=background_tasks,\n        )\n    # Non-streaming response\n    final_res: RequestOutput = None\n    async for res in result_generator:\n        if await raw_request.is_disconnected():\n            # Abort the request if the client disconnects.\n            # await abort_request()\n            return create_error_response(\n                HTTPStatus.BAD_REQUEST, \"Client disconnected\"\n            )\n        final_res = res\n    assert final_res is not None\n    choices = []\n    for output in final_res.outputs:\n        choice_data = ChatCompletionResponseChoice(\n            index=output.index,\n            message=ChatMessage(role=\"assistant\", content=output.text),\n            finish_reason=output.finish_reason,\n        )\n        choices.append(choice_data)\n    num_prompt_tokens = len(final_res.prompt_token_ids)\n    num_generated_tokens = sum(\n        len(output.token_ids) for output in final_res.outputs\n    )\n    usage = UsageInfo(\n        prompt_tokens=num_prompt_tokens,\n        completion_tokens=num_generated_tokens,\n        total_tokens=num_prompt_tokens + num_generated_tokens,\n    )\n    response = ChatCompletionResponse(\n        id=request_id,\n        created=created_time,\n        model=model_name,\n        choices=choices,\n        usage=usage,\n    )\n    if request.stream:\n        # When user requests streaming but we don't stream, we still need to\n        # return a streaming response with a single event.\n        response_json = response.json(ensure_ascii=False)\n        async def fake_stream_generator() -> AsyncGenerator[str, None]:\n            yield f\"data: {response_json}\\n\\n\"\n            yield \"data: [DONE]\\n\\n\"\n        return StreamingResponse(\n            fake_stream_generator(), media_type=\"text/event-stream\"\n        )\n    return response\n@app.post(\"/v1/completions\")\nasync def create_completion(raw_request: Request):\n    \"\"\"Completion API similar to OpenAI's API.\n    See https://platform.openai.com/docs/api-reference/completions/create\n    for the API specification. This API mimics the OpenAI Completion API.\n    NOTE: Currently we do not support the following features:\n        - echo (since the vLLM engine does not currently support\n          getting the logprobs of prompt tokens)\n        - suffix (the language models we currently support do not support\n          suffix)\n        - logit_bias (to be supported by vLLM engine)\n    \"\"\"\n    request = CompletionRequest(**await raw_request.json())\n    logger.info(f\"Received completion request: {request}\")\n    error_check_ret = await check_model(request)\n    if error_check_ret is not None:\n        return error_check_ret\n    if request.echo:\n        # We do not support echo since the vLLM engine does not\n        # currently support getting the logprobs of prompt tokens.\n        return create_error_response(\n            HTTPStatus.BAD_REQUEST, \"echo is not currently supported\"\n        )\n    if request.suffix is not None:\n        # The language models we currently support do not support suffix.\n        return create_error_response(\n            HTTPStatus.BAD_REQUEST, \"suffix is not currently supported\"\n        )\n    if request.logit_bias is not None:\n        # TODO: support logit_bias in vLLM engine.\n        return create_error_response(\n            HTTPStatus.BAD_REQUEST, \"logit_bias is not currently supported\"\n        )\n    model_name = request.model\n    request_id = f\"cmpl-{random_uuid()}\"\n    if isinstance(request.prompt, list):\n        if len(request.prompt) == 0:\n            return create_error_response(\n                HTTPStatus.BAD_REQUEST, \"please provide at least one prompt\"\n            )\n        if len(request.prompt) > 1:\n            return create_error_response(\n                HTTPStatus.BAD_REQUEST,\n                \"multiple prompts in a batch is not currently supported\",\n            )\n        prompt = request.prompt[0]\n    else:\n        prompt = request.prompt\n    created_time = int(time.time())\n    try:\n        sampling_params = SamplingParams(\n            n=request.n,\n            best_of=request.best_of,\n            presence_penalty=request.presence_penalty,\n            frequency_penalty=request.frequency_penalty,\n            temperature=request.temperature,\n            top_p=request.top_p,\n            top_k=request.top_k,\n            stop=request.stop,\n            ignore_eos=request.ignore_eos,\n            max_tokens=request.max_tokens,\n            logprobs=request.logprobs,\n            use_beam_search=request.use_beam_search,\n        )\n    except ValueError as e:\n        return create_error_response(HTTPStatus.BAD_REQUEST, str(e))\n    result_generator = engine.generate(prompt, sampling_params, request_id)\n    # Similar to the OpenAI API, when n != best_of, we do not stream the\n    # results. In addition, we do not stream the results when use beam search.\n    stream = (\n        request.stream\n        and (request.best_of is None or request.n == request.best_of)\n        and not request.use_beam_search\n    )\n    # async def abort_request() -> None:\n    # await engine.abort(request_id)\n    def create_stream_response_json(\n        index: int,\n        text: str,\n        logprobs: Optional[LogProbs] = None,\n        finish_reason: Optional[str] = None,\n    ) -> str:\n        choice_data = CompletionResponseStreamChoice(\n            index=index,\n            text=text,\n            logprobs=logprobs,\n            finish_reason=finish_reason,\n        )\n        response = CompletionStreamResponse(\n            id=request_id,\n            created=created_time,\n            model=model_name,\n            choices=[choice_data],\n        )\n        response_json = response.json(ensure_ascii=False)\n        return response_json\n    async def completion_stream_generator() -> AsyncGenerator[str, None]:\n        previous_texts = [\"\"] * request.n\n        previous_num_tokens = [0] * request.n\n        async for res in result_generator:\n            res: RequestOutput\n            for output in res.outputs:\n                i = output.index\n                delta_text = output.text[len(previous_texts[i]) :]\n                if request.logprobs is not None:\n                    logprobs = create_logprobs(\n                        output.token_ids[previous_num_tokens[i] :],\n                        output.logprobs[previous_num_tokens[i] :],\n                        len(previous_texts[i]),\n                    )\n                else:\n                    logprobs = None\n                previous_texts[i] = output.text\n                previous_num_tokens[i] = len(output.token_ids)\n                response_json = create_stream_response_json(\n                    index=i,\n                    text=delta_text,\n                    logprobs=logprobs,\n                )\n                yield f\"data: {response_json}\\n\\n\"\n                if output.finish_reason is not None:\n                    logprobs = (\n                        LogProbs() if request.logprobs is not None else None\n                    )\n                    response_json = create_stream_response_json(\n                        index=i,\n                        text=\"\",\n                        logprobs=logprobs,\n                        finish_reason=output.finish_reason,\n                    )\n                    yield f\"data: {response_json}\\n\\n\"\n        yield \"data: [DONE]\\n\\n\"\n    # Streaming response\n    if stream:\n        background_tasks = BackgroundTasks()\n        # Abort the request if the client disconnects.\n        background_tasks.add_task(abort_request)\n        return StreamingResponse(\n            completion_stream_generator(),\n            media_type=\"text/event-stream\",\n            background=background_tasks,\n        )\n    # Non-streaming response\n    final_res: RequestOutput = None\n    async for res in result_generator:\n        if await raw_request.is_disconnected():\n            # Abort the request if the client disconnects.\n            await abort_request()\n            return create_error_response(\n                HTTPStatus.BAD_REQUEST, \"Client disconnected\"\n            )\n        final_res = res\n    assert final_res is not None\n    choices = []\n    for output in final_res.outputs:\n        if request.logprobs is not None:\n            logprobs = create_logprobs(output.token_ids, output.logprobs)\n        else:\n            logprobs = None\n        choice_data = CompletionResponseChoice(\n            index=output.index,\n            text=output.text,\n            logprobs=logprobs,\n            finish_reason=output.finish_reason,\n        )\n        choices.append(choice_data)\n    num_prompt_tokens = len(final_res.prompt_token_ids)\n    num_generated_tokens = sum(\n        len(output.token_ids) for output in final_res.outputs\n    )\n    usage = UsageInfo(\n        prompt_tokens=num_prompt_tokens,\n        completion_tokens=num_generated_tokens,\n        total_tokens=num_prompt_tokens + num_generated_tokens,\n    )\n    response = CompletionResponse(\n        id=request_id,\n        created=created_time,\n        model=model_name,\n        choices=choices,\n        usage=usage,\n    )\n    if request.stream:\n        # When user requests streaming but we don't stream, we still need to\n        # return a streaming response with a single event.\n        response_json = response.json(ensure_ascii=False)\n        async def fake_stream_generator() -> AsyncGenerator[str, None]:\n            yield f\"data: {response_json}\\n\\n\"\n            yield \"data: [DONE]\\n\\n\"\n        return StreamingResponse(\n            fake_stream_generator(), media_type=\"text/event-stream\"\n        )\n    return response\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(\n        description=\"vLLM OpenAI-Compatible RESTful API server.\"\n    )\n    parser.add_argument(\n        \"--host\", type=str, default=\"localhost\", help=\"host name\"\n    )\n    parser.add_argument(\"--port\", type=int, default=8000, help=\"port number\")\n    parser.add_argument(\n        \"--allow-credentials\", action=\"store_true\", help=\"allow credentials\"\n    )\n    parser.add_argument(\n        \"--allowed-origins\",\n        type=json.loads,\n        default=[\"*\"],\n        help=\"allowed origins\",\n    )\n    parser.add_argument(\n        \"--allowed-methods\",\n        type=json.loads,\n        default=[\"*\"],\n        help=\"allowed methods\",\n    )\n    parser.add_argument(\n        \"--allowed-headers\",\n        type=json.loads,\n        default=[\"*\"],\n        help=\"allowed headers\",\n    )\n    parser.add_argument(\n        \"--served-model-name\",\n        type=str,\n        default=None,\n        help=\"The model name used in the API. If not \"\n        \"specified, the model name will be the same as \"\n        \"the huggingface name.\",\n    )\n    parser = AsyncEngineArgs.add_cli_args(parser)\n    args = parser.parse_args()\n    app.add_middleware(\n        CORSMiddleware,\n        allow_origins=args.allowed_origins,\n        allow_credentials=args.allow_credentials,\n        allow_methods=args.allowed_methods,\n        allow_headers=args.allowed_headers,\n    )\n    logger.info(f\"args: {args}\")\n    if args.served_model_name is not None:\n        served_model = args.served_model_name\n    else:\n        served_model = args.model\n    engine_args = AsyncEngineArgs.from_cli_args(args)\n    engine = AsyncLLMEngine.from_engine_args(engine_args)\n    engine_model_config = asyncio.run(engine.get_model_config())\n    max_model_len = engine_model_config.get_max_model_len()\n    uvicorn.run(\n        app,\n        host=args.host,\n        port=args.port,\n        log_level=\"info\",\n        timeout_keep_alive=TIMEOUT_KEEP_ALIVE,\n    )\n```\nFunction implementations for Modal\nThese functions are called\nTokenizer function\n```python\n@stub.cls(gpu=\"A100\", keep_warm=1)\nclass Tokenizer:\n    def __enter__(self):\n        # A separate tokenizer to map token IDs to strings.\n        self.tokenizer = get_tokenizer(\n            engine_args.tokenizer,\n            tokenizer_mode=engine_args.tokenizer_mode,\n            trust_remote_code=engine_args.trust_remote_code,\n        )\n    @method()\n    async def generate(prompt: str, sampling_params, request_id: str):\n        Model().generate.call(prompt, sampling_params, request_id)\n```\nModel inference function\n```python\n@stub.cls(gpu=\"A100\", keep_warm=1)\nclass Model:\n    def __enter__(self):\n        from vllm.engine.arg_utils import AsyncEngineArgs\n        from vllm.engine.async_llm_engine import AsyncLLMEngine\n        # tokens generated since last report\n        self.last_report, self.generated_tokens = time.time(), 0\n        engine_args = AsyncEngineArgs(\n            model=\"/model\", gpu_memory_utilization=0.95\n        )\n        self.engine = AsyncLLMEngine.from_engine_args(engine_args)\n    @method()\n    async def generate(prompt: str, sampling_params, request_id: str):\n        Model().generate.call(prompt, sampling_params, request_id)\n        from vllm.sampling_params import SamplingParams\n        from vllm.utils import random_uuid\n        sampling_params = SamplingParams(\n            presence_penalty=0.8,\n            temperature=0.2,\n            top_p=0.95,\n            top_k=50,\n            max_tokens=1024,\n        )\n        request_id = random_uuid()\n        results_generator = self.engine.generate(\n            self.template.format(question), sampling_params, request_id\n        )\n        t0 = time.time()\n        index, tokens = 0, 0\n        async for request_output in results_generator:\n            if \"\\ufffd\" == request_output.outputs[0].text[-1]:\n                continue\n            yield request_output.outputs[0].text[index:]\n            index = len(request_output.outputs[0].text)\n            # Token accounting\n            new_tokens = len(request_output.outputs[0].token_ids)\n            self.generated(new_tokens - tokens)\n            tokens = new_tokens\n        throughput = tokens / (time.time() - t0)\n        print(f\"Request completed: {throughput:.4f} tokens/s\")\n        print(request_output.outputs[0].text)\n@stub.function()\n@asgi_app()\ndef fastapi_app():\n    return app\n```\n"}
{"text": "\n```python\nimport io\nimport os\nfrom typing import Optional\nfrom fastapi import Request\nfrom modal import Image, Secret, Stub, web_endpoint\nCACHE_PATH = \"/root/model_cache\"\ndef load_model(device=None):\n    import torch\n    from min_dalle import MinDalle\n    # Instantiate the model, which has the side-effect of persisting\n    # the model to disk if it does not already exist.\n    return MinDalle(\n        models_root=CACHE_PATH,\n        dtype=torch.float32,\n        device=device,\n        is_mega=True,\n        is_reusable=True,\n    )\nstub = Stub(\n    \"example-dalle-bot\",\n    image=Image.debian_slim().pip_install(\"min-dalle\").run_function(load_model),\n)\n@stub.function(\n    image=Image.debian_slim().pip_install(\"slack-sdk\"),\n    secret=Secret.from_name(\"dalle-bot-slack-secret\"),\n)\ndef post_to_slack(prompt: str, channel_name: str, image_bytes: bytes):\n    import slack_sdk\n    client = slack_sdk.WebClient(token=os.environ[\"SLACK_BOT_TOKEN\"])\n    client.files_upload(\n        channels=channel_name, title=prompt, content=image_bytes\n    )\n@stub.function(gpu=\"A10G\")\nasync def run_minidalle(prompt: str, channel_name: Optional[str]):\n    model = load_model(device=\"cuda\")\n    image = model.generate_image(\n        text=prompt,\n        seed=-1,\n        grid_size=3,\n        is_seamless=False,\n        temperature=1,\n        top_k=256,\n        supercondition_factor=16,\n        is_verbose=False,\n    )\n    with io.BytesIO() as buf:\n        image.save(buf, format=\"PNG\")\n        if channel_name:\n            post_to_slack.remote(prompt, channel_name, buf.getvalue())\n        return buf.getvalue()\n```\npython-multipart is needed for fastapi form parsing.\n```python\n@stub.function(\n    image=Image.debian_slim().pip_install(\"python-multipart\"),\n)\n@web_endpoint(\n    method=\"POST\",\n)\nasync def entrypoint(request: Request):\n    body = await request.form()\n    prompt = body[\"text\"]\n    # Deferred call to function.\n    run_minidalle.spawn(prompt, body[\"channel_name\"])\n    return f\"Running text2im for {prompt}.\"\n```\nEntrypoint code so this can be easily run from the command line\n```python\nOUTPUT_DIR = \"/tmp/render\"\n@stub.local_entrypoint()\ndef main(prompt: str = \"martha stewart at burning man\"):\n    os.makedirs(OUTPUT_DIR, exist_ok=True)\n    output_path = os.path.join(OUTPUT_DIR, \"output.png\")\n    img_bytes = run_minidalle.remote(prompt, None)\n    with open(output_path, \"wb\") as f:\n        f.write(img_bytes)\n    print(f\"Done! Your DALL-E output image is at '{output_path}'\")\n```\n"}
{"text": "\n```python\nfrom typing import Optional\nfrom fastapi import FastAPI, Header\nfrom modal import Image, Stub, asgi_app, web_endpoint\nfrom pydantic import BaseModel\nweb_app = FastAPI()\nstub = Stub(\"example-fastapi-app\")\nimage = Image.debian_slim()\nclass Item(BaseModel):\n    name: str\n@web_app.get(\"/\")\nasync def handle_root(user_agent: Optional[str] = Header(None)):\n    print(f\"GET /     - received user_agent={user_agent}\")\n    return \"Hello World\"\n@web_app.post(\"/foo\")\nasync def handle_foo(item: Item, user_agent: Optional[str] = Header(None)):\n    print(\n        f\"POST /foo - received user_agent={user_agent}, item.name={item.name}\"\n    )\n    return item\n@stub.function(image=image)\n@asgi_app()\ndef fastapi_app():\n    return web_app\n@stub.function()\n@web_endpoint(method=\"POST\")\ndef f(item: Item):\n    return \"Hello \" + item.name\nif __name__ == \"__main__\":\n    stub.deploy(\"webapp\")\n```\n"}
{"text": "\nQuick snippet to connect to a Jupyter notebook server running inside a Modal container,\nespecially useful for exploring the contents of Modal network file systems.\nThis uses https://github.com/ekzhang/bore to expose the server to the public internet.\n```python\nimport os\nimport subprocess\nimport time\nimport modal\nstub = modal.Stub(\n    image=modal.Image.debian_slim()\n    .pip_install(\"jupyter\", \"bing-image-downloader~=1.1.2\")\n    .apt_install(\"curl\")\n    .run_commands(\"curl https://sh.rustup.rs -sSf | bash -s -- -y\")\n    .run_commands(\". $HOME/.cargo/env && cargo install bore-cli\")\n)\n```\nThis volume is not persisted, so the data will be deleted when this demo app is stopped.\n```python\nvolume = modal.NetworkFileSystem.new()\nCACHE_DIR = \"/root/cache\"\nJUPYTER_TOKEN = \"1234\"  # Change me to something non-guessable!\n@stub.function(\n    network_file_systems={CACHE_DIR: volume},\n)\ndef seed_volume():\n    # Bing it!\n    from bing_image_downloader import downloader\n    # This will save into the Modal volume and allow you view the images\n    # from within Jupyter at a path like `/cache/modal labs/Image_1.png`.\n    downloader.download(\n        query=\"modal labs\",\n        limit=10,\n        output_dir=CACHE_DIR,\n        force_replace=False,\n        timeout=60,\n        verbose=True,\n    )\n```\nThis is all that's needed to create a long-lived Jupyter server process in Modal\nthat you can access in your Browser through a secure network tunnel.\nThis can be useful when you want to interactively engage with network file system contents\nwithout having to download it to your host computer.\n```python\n@stub.function(\n    concurrency_limit=1, network_file_systems={CACHE_DIR: volume}, timeout=1_500\n)\ndef run_jupyter(timeout: int):\n    jupyter_process = subprocess.Popen(\n        [\n            \"jupyter\",\n            \"notebook\",\n            \"--no-browser\",\n            \"--allow-root\",\n            \"--port=8888\",\n            \"--NotebookApp.allow_origin='*'\",\n            \"--NotebookApp.allow_remote_access=1\",\n        ],\n        env={**os.environ, \"JUPYTER_TOKEN\": JUPYTER_TOKEN},\n    )\n    bore_process = subprocess.Popen(\n        [\"/root/.cargo/bin/bore\", \"local\", \"8888\", \"--to\", \"bore.pub\"],\n    )\n    try:\n        end_time = time.time() + timeout\n        while time.time() < end_time:\n            time.sleep(5)\n        print(f\"Reached end of {timeout} second timeout period. Exiting...\")\n    except KeyboardInterrupt:\n        print(\"Exiting...\")\n    finally:\n        bore_process.kill()\n        jupyter_process.kill()\n@stub.local_entrypoint()\ndef main(timeout: int = 10_000):\n    # Write some images to a volume, for demonstration purposes.\n    seed_volume.remote()\n    # Run the Jupyter Notebook server\n    run_jupyter.remote(timeout=timeout)\n```\nDoing `modal run jupyter_inside_modal.py` will run a Modal app which starts\nthe Juypter server at an address like http://bore.pub:$PORT/. Visit this address\nin your browser, and enter the security token you set for `JUPYTER_TOKEN`.\n"}
{"text": "\n```python\nimport asyncio\nimport time\nfrom fastapi import FastAPI\nfrom fastapi.responses import StreamingResponse\nfrom modal import Stub, asgi_app, web_endpoint\nstub = Stub(\"example-fastapi-streaming\")\nweb_app = FastAPI()\n```\nThis asynchronous generator function simulates\nprogressively returning data to the client. The `asyncio.sleep`\nis not necessary, but makes it easier to see the iterative behavior\nof the response.\n```python\nasync def fake_video_streamer():\n    for i in range(10):\n        yield f\"frame {i}: hello world!\".encode()\n        await asyncio.sleep(1.0)\n```\nASGI app with streaming handler.\nThis `fastapi_app` also uses the fake video streamer async generator,\npassing it directly into `StreamingResponse`.\n```python\n@web_app.get(\"/\")\nasync def main():\n    return StreamingResponse(\n        fake_video_streamer(), media_type=\"text/event-stream\"\n    )\n@stub.function()\n@asgi_app()\ndef fastapi_app():\n    return web_app\n```\nThis `hook` web endpoint Modal function calls *another* Modal function,\nand it just works!\n```python\n@stub.function()\ndef sync_fake_video_streamer():\n    for i in range(10):\n        yield f\"frame {i}: some data\\n\".encode()\n        time.sleep(1)\n@stub.function()\n@web_endpoint()\ndef hook():\n    return StreamingResponse(\n        sync_fake_video_streamer.remote(), media_type=\"text/event-stream\"\n    )\n```\nThis `mapped` web endpoint Modal function does a parallel `.map` on a simple\nModal function. Using `.starmap` also would work in the same fashion.\n```python\n@stub.function()\ndef map_me(i):\n    time.sleep(i)  # stagger the results for demo purposes\n    return f\"hello from {i}\\n\"\n@stub.function()\n@web_endpoint()\ndef mapped():\n    return StreamingResponse(\n        map_me.map(range(10)), media_type=\"text/event-stream\"\n    )\n```\nA collection of basic examples of a webhook streaming response.\n```\nmodal serve streaming.py\n```\nTo try out the webhook, ensure that your client is not buffering the server response\nuntil it gets newline (\\n) characters. By default browsers and `curl` are buffering,\nthough modern browsers should respect the \"text/event-stream\" content type header being set.\n```shell\ncurl --no-buffer https://modal-labs--example-fastapi-streaming-fastapi-app.modal.run\ncurl --no-buffer https://modal-labs--example-fastapi-streaming-hook.modal.run\ncurl --no-buffer https://modal-labs--example-fastapi-streaming-mapped.modal.run\n````\n"}
{"text": "# Run OpenLLaMa on an A100 GPU\nIn this example, we run [OpenLLaMa](https://github.com/openlm-research/open_llama),\nan open-source large language model, using HuggingFace's [transformers](https://huggingface.co/docs/transformers/index)\nlibrary.\n## Setup\nFirst we import the components we need from `modal`.\n```python\nfrom modal import Image, Stub, gpu, method\n```\n## Define a container image\nTo take advantage of Modal's blazing fast cold-start times, we'll need to download our model weights\ninside our container image.\nTo do this, we have to define a function that loads both the model and tokenizer using\n[from_pretrained](https://huggingface.co/docs/transformers/main_classes/model#transformers.PreTrainedModel.from_pretrained).\nSince HuggingFace stores this model into a local cache, when Modal snapshots the image after running this function,\nthe model weights will be saved and available for use when the container starts up next time.\n```python\nBASE_MODEL = \"openlm-research/open_llama_7b\"\ndef download_models():\n    from transformers import LlamaForCausalLM, LlamaTokenizer\n    LlamaForCausalLM.from_pretrained(BASE_MODEL)\n    LlamaTokenizer.from_pretrained(BASE_MODEL)\n```\nNow, we define our image. We'll use the `debian-slim` base image, and install the dependencies we need\nusing [`pip_install`](/docs/reference/modal.Image#pip_install). At the end, we'll use\n[`run_function`](/docs/guide/custom-container#running-a-function-as-a-build-step-beta) to run the\nfunction defined above as part of the image build.\n```python\nimage = (\n    # Python 3.11+ not yet supported for torch.compile\n    Image.debian_slim(python_version=\"3.10\")\n    .pip_install(\n        \"accelerate~=0.18.0\",\n        \"transformers~=4.28.1\",\n        \"torch~=2.0.0\",\n        \"sentencepiece~=0.1.97\",\n    )\n    .run_function(download_models)\n)\n```\nLet's instantiate and name our [Stub](/docs/guide/apps).\n```python\nstub = Stub(name=\"example-open-llama\", image=image)\n```\n## The model class\nNext, we write the model code. We want Modal to load the model into memory just once every time a container starts up,\nso we use [class syntax](/docs/guide/lifecycle-functions) and the `__enter__` method.\nWithin the [@stub.cls](/docs/reference/modal.Stub#cls) decorator, we use the [gpu parameter](/docs/guide/gpu)\nto specify that we want to run our function on an [A100 GPU with 20 GB of VRAM](/pricing).\nThe rest is just using the [generate](https://huggingface.co/docs/transformers/en/main_classes/text_generation#transformers.GenerationMixin.generate) function\nfrom the `transformers` library. Refer to the documentation for more parameters and tuning.\n```python\n@stub.cls(gpu=gpu.A100(memory=20))\nclass OpenLlamaModel:\n    def __enter__(self):\n        import torch\n        from transformers import LlamaForCausalLM, LlamaTokenizer\n        self.tokenizer = LlamaTokenizer.from_pretrained(BASE_MODEL)\n        model = LlamaForCausalLM.from_pretrained(\n            BASE_MODEL,\n            torch_dtype=torch.float16,\n            device_map=\"auto\",\n        )\n        self.tokenizer.bos_token_id = 1\n        model.eval()\n        self.model = torch.compile(model)\n        self.device = \"cuda\"\n    @method()\n    def generate(\n        self,\n        input,\n        max_new_tokens=128,\n        **kwargs,\n    ):\n        import torch\n        from transformers import GenerationConfig\n        inputs = self.tokenizer(input, return_tensors=\"pt\")\n        input_ids = inputs[\"input_ids\"].to(self.device)\n        generation_config = GenerationConfig(**kwargs)\n        with torch.no_grad():\n            generation_output = self.model.generate(\n                input_ids=input_ids,\n                generation_config=generation_config,\n                return_dict_in_generate=True,\n                output_scores=True,\n                max_new_tokens=max_new_tokens,\n            )\n        s = generation_output.sequences[0]\n        output = self.tokenizer.decode(s)\n        print(f\"\\033[96m{input}\\033[0m\")\n        print(output.split(input)[1].strip())\n```\n## Run the model\nFinally, we define a [`local_entrypoint`](/docs/guide/apps#entrypoints-for-ephemeral-apps) to call our remote function\nsequentially for a list of inputs. You can run this locally with `modal run openllama.py`.\n```python\n@stub.local_entrypoint()\ndef main():\n    inputs = [\n        \"Building a website can be done in 10 simple steps:\",\n    ]\n    model = OpenLlamaModel()\n    for input in inputs:\n        model.generate.remote(\n            input,\n            top_p=0.75,\n            top_k=40,\n            num_beams=1,\n            temperature=0.1,\n            do_sample=True,\n        )\n```\n## Next steps\nThe above is a simple example of how to run a basic model. Note that OpenLLaMa has not been fine-tuned on an instruction-following dataset,\nso the results aren't amazing out of the box. Refer to [DoppelBot, our Slack fine-tuning demo](https://github.com/modal-labs/doppel-bot) for how\nyou could use OpenLLaMa to perform a more useful downstream task.\nIf you're looking for useful responses out-of-the-box like ChatGPT, you could try Vicuna-13B, which is larger and has been instruction-tuned.\nHowever, note that this model is not permissively licensed due to the dataset it was trained on. Refer to our [LLM voice chat](/docs/guide/llm-voice-chat)\npost for how to build a complete voice chat app using Vicuna, or go straight to the [file](https://github.com/modal-labs/quillman/blob/main/src/llm_vicuna.py)\nif you want to run it by itself.\n"}
{"text": "\n# Use a generator to fetch search results\nThis is a simple example which\n1. Installs a custom Python package.\n2. Uses a _generator_ to return results back to the launcher process.\n```python\nimport modal\n```\nWe build a custom image by adding the `google` package to the base image.\n```python\nstub = modal.Stub(\n    \"example-google-search-generator\",\n    image=modal.Image.debian_slim().pip_install(\"google\"),\n)\n```\nNext, let's define a _generator_ function that uses our custom image.\n```python\n@stub.function()\ndef scrape(query):\n    from googlesearch import search\n    for url in search(query.encode(), stop=100):\n        yield url\n```\nFinally, let's launch it from the command line with `modal run`:\n```python\n@stub.local_entrypoint()\ndef main(query: str = \"modal\"):\n    for url in scrape.remote_gen(query):\n        print(url)\n```\n"}
{"text": "\n```python\nimport time\nfrom datetime import datetime, timezone\nimport modal\nstub = modal.Stub(\"example-say-hello-cron\")\n@stub.function(schedule=modal.Period(seconds=10))\ndef say_hello():\n    start_time = datetime.now(timezone.utc)\n    for i in range(10):\n        print(f\"Message #{i} from invocation at {start_time}\")\n        time.sleep(1.5)\n```\n"}
{"text": "\n# Pet Art Dreambooth with Hugging Face and Gradio\nThis example finetunes the [Stable Diffusion v1.5 model](https://huggingface.co/runwayml/stable-diffusion-v1-5)\non images of a pet (by default, a puppy named Qwerty)\nusing a technique called textual inversion from [the \"Dreambooth\" paper](https://dreambooth.github.io/).\nEffectively, it teaches a general image generation model a new \"proper noun\",\nallowing for the personalized generation of art and photos.\nIt then makes the model shareable with others using the [Gradio.app](https://gradio.app/)\nweb interface framework.\nIt demonstrates a simple, productive, and cost-effective pathway\nto building on large pretrained models\nby using Modal's building blocks, like\n[GPU-accelerated](https://modal.com/docs/guide/gpu#using-a100-gpus-alpha) Modal Functions, [volumes](/docs/guide/volumes) for caching, and [Modal webhooks](https://modal.com/docs/guide/webhooks#webhook).\nAnd with some light customization, you can use it to generate images of your pet!\n![Gradio.app image generation interface](./gradio-image-generate.png)\n## Setting up the dependencies\nWe can start from a base image and specify all of our dependencies.\n```python\nimport os\nfrom dataclasses import dataclass\nfrom pathlib import Path\nfrom fastapi import FastAPI\nfrom modal import (\n    Image,\n    Mount,\n    Secret,\n    Stub,\n    Volume,\n    asgi_app,\n    method,\n)\nweb_app = FastAPI()\nassets_path = Path(__file__).parent / \"assets\"\nstub = Stub(name=\"example-dreambooth-app\")\n```\nCommit in `diffusers` to checkout `train_dreambooth.py` from.\n```python\nGIT_SHA = \"ed616bd8a8740927770eebe017aedb6204c6105f\"\nimage = (\n    Image.debian_slim(python_version=\"3.10\")\n    .pip_install(\n        \"accelerate==0.19\",\n        \"datasets~=2.13\",\n        \"ftfy\",\n        \"gradio~=3.10\",\n        \"smart_open\",\n        \"transformers\",\n        \"torch\",\n        \"torchvision\",\n        \"triton\",\n    )\n    .pip_install(\"xformers\", pre=True)\n    .apt_install(\"git\")\n    # Perform a shallow fetch of just the target `diffusers` commit, checking out\n    # the commit in the container's current working directory, /root. Then install\n    # the `diffusers` package.\n    .run_commands(\n        \"cd /root && git init .\",\n        \"cd /root && git remote add origin https://github.com/huggingface/diffusers\",\n        f\"cd /root && git fetch --depth=1 origin {GIT_SHA} && git checkout {GIT_SHA}\",\n        \"cd /root && pip install -e .\",\n    )\n)\n```\nA persisted `modal.Volume` will store model artefacts across Modal app runs.\nThis is crucial as finetuning runs are separate from the Gradio app we run as a webhook.\n```python\nvolume = Volume.persisted(\"dreambooth-finetuning-volume\")\nMODEL_DIR = Path(\"/model\")\nstub.volume = volume\n```\n## Config\nAll configs get their own dataclasses to avoid scattering special/magic values throughout code.\nYou can read more about how the values in `TrainConfig` are chosen and adjusted [in this blog post on Hugging Face](https://huggingface.co/blog/dreambooth).\nTo run training on images of your own pet, upload the images to separate URLs and edit the contents of the file at `TrainConfig.instance_example_urls_file` to point to them.\n```python\n@dataclass\nclass SharedConfig:\n    \"\"\"Configuration information shared across project components.\"\"\"\n    # The instance name is the \"proper noun\" we're teaching the model\n    instance_name: str = \"Qwerty\"\n    # That proper noun is usually a member of some class (person, bird),\n    # and sharing that information with the model helps it generalize better.\n    class_name: str = \"Golden Retriever\"\n@dataclass\nclass TrainConfig(SharedConfig):\n    \"\"\"Configuration for the finetuning step.\"\"\"\n    # training prompt looks like `{PREFIX} {INSTANCE_NAME} the {CLASS_NAME} {POSTFIX}`\n    prefix: str = \"a photo of\"\n    postfix: str = \"\"\n    # locator for plaintext file with urls for images of target instance\n    instance_example_urls_file: str = str(\n        Path(__file__).parent / \"instance_example_urls.txt\"\n    )\n    # identifier for pretrained model on Hugging Face\n    model_name: str = \"runwayml/stable-diffusion-v1-5\"\n    # Hyperparameters/constants from the huggingface training example\n    resolution: int = 512\n    train_batch_size: int = 1\n    gradient_accumulation_steps: int = 1\n    learning_rate: float = 2e-6\n    lr_scheduler: str = \"constant\"\n    lr_warmup_steps: int = 0\n    max_train_steps: int = 600\n    checkpointing_steps: int = 1000\n@dataclass\nclass AppConfig(SharedConfig):\n    \"\"\"Configuration information for inference.\"\"\"\n    num_inference_steps: int = 50\n    guidance_scale: float = 7.5\n```\n## Get finetuning dataset\nPart of the magic of Dreambooth is that we only need 4-10 images for finetuning.\nSo we can fetch just a few images, stored on consumer platforms like Imgur or Google Drive\n-- no need for expensive data collection or data engineering.\n```python\nIMG_PATH = Path(\"/img\")\ndef load_images(image_urls):\n    import PIL.Image\n    from smart_open import open\n    os.makedirs(IMG_PATH, exist_ok=True)\n    for ii, url in enumerate(image_urls):\n        with open(url, \"rb\") as f:\n            image = PIL.Image.open(f)\n            image.save(IMG_PATH / f\"{ii}.png\")\n    print(\"Images loaded.\")\n    return IMG_PATH\n```\n## Finetuning a text-to-image model\nThis model is trained to do a sort of \"reverse [ekphrasis](https://en.wikipedia.org/wiki/Ekphrasis)\":\nit attempts to recreate a visual work of art or image from only its description.\nWe can use a trained model to synthesize wholly new images\nby combining the concepts it has learned from the training data.\nWe use a pretrained model, version 1.5 of the Stable Diffusion model. In this example, we \"finetune\" SD v1.5, making only small adjustments to the weights,\nin order to just teach it a new word: the name of our pet.\nThe result is a model that can generate novel images of our pet:\nas an astronaut in space, as painted by Van Gogh or Bastiat, etc.\n### Finetuning with Hugging Face \ud83e\udde8 Diffusers and Accelerate\nThe model weights, libraries, and training script are all provided by [\ud83e\udd17 Hugging Face](https://huggingface.co).\nTo access the model weights, you'll need a [Hugging Face account](https://huggingface.co/join)\nand from that account you'll need to accept the model license [here](https://huggingface.co/runwayml/stable-diffusion-v1-5).\nLastly, you'll need to create a token from that account and share it with Modal\nunder the name `\"huggingface\"`. Follow the instructions [here](https://modal.com/secrets).\nThen, you can kick off a training job with the command\n`modal run dreambooth_app.py::stub.train`.\nIt should take about ten minutes.\nTip: if the results you're seeing don't match the prompt too well, and instead produce an image of your subject again, the model has likely overfit. In this case, repeat training with a lower # of max_train_steps. On the other hand, if the results don't look like your subject, you might need to increase # of max_train_steps.\n```python\n@stub.function(\n    image=image,\n    gpu=\"A100\",  # finetuning is VRAM hungry, so this should be an A100\n    volumes={\n        str(\n            MODEL_DIR\n        ): volume,  # fine-tuned model will be stored at `MODEL_DIR`\n    },\n    timeout=1800,  # 30 minutes\n    secrets=[Secret.from_name(\"huggingface\")],\n)\ndef train(instance_example_urls):\n    import subprocess\n    import huggingface_hub\n    from accelerate.utils import write_basic_config\n    from transformers import CLIPTokenizer\n    # set up TrainConfig\n    config = TrainConfig()\n    # set up runner-local image and shared model weight directories\n    img_path = load_images(instance_example_urls)\n    os.makedirs(MODEL_DIR, exist_ok=True)\n    # set up hugging face accelerate library for fast training\n    write_basic_config(mixed_precision=\"fp16\")\n    # authenticate to hugging face so we can download the model weights\n    hf_key = os.environ[\"HUGGINGFACE_TOKEN\"]\n    huggingface_hub.login(hf_key)\n    # check whether we can access to model repo\n    try:\n        CLIPTokenizer.from_pretrained(config.model_name, subfolder=\"tokenizer\")\n    except OSError as e:  # handle error raised when license is not accepted\n        license_error_msg = f\"Unable to load tokenizer. Access to this model requires acceptance of the license on Hugging Face here: https://huggingface.co/{config.model_name}.\"\n        raise Exception(license_error_msg) from e\n    # define the training prompt\n    instance_phrase = f\"{config.instance_name} {config.class_name}\"\n    prompt = f\"{config.prefix} {instance_phrase} {config.postfix}\".strip()\n    # run training -- see huggingface accelerate docs for details\n    try:\n        subprocess.run(\n            [\n                \"accelerate\",\n                \"launch\",\n                \"examples/dreambooth/train_dreambooth.py\",\n                \"--train_text_encoder\",  # needs at least 16GB of GPU RAM.\n                f\"--pretrained_model_name_or_path={config.model_name}\",\n                f\"--instance_data_dir={img_path}\",\n                f\"--output_dir={MODEL_DIR}\",\n                f\"--instance_prompt='{prompt}'\",\n                f\"--resolution={config.resolution}\",\n                f\"--train_batch_size={config.train_batch_size}\",\n                f\"--gradient_accumulation_steps={config.gradient_accumulation_steps}\",\n                f\"--learning_rate={config.learning_rate}\",\n                f\"--lr_scheduler={config.lr_scheduler}\",\n                f\"--lr_warmup_steps={config.lr_warmup_steps}\",\n                f\"--max_train_steps={config.max_train_steps}\",\n                f\"--checkpointing_steps={config.checkpointing_steps}\",\n            ],\n            check=True,\n            capture_output=True,\n        )\n    except subprocess.CalledProcessError as exc:\n        print(exc.stdout.decode())\n        print(exc.stderr.decode())\n        raise\n    # The trained model artefacts have been output to the volume mounted at `MODEL_DIR`.\n    # To persist these artefacts for use in future inference function calls, we 'commit' the changes\n    # to the volume.\n    stub.app.volume.commit()\n```\n## The inference function.\nTo generate images from prompts using our fine-tuned model, we define a function called `inference`.\nIn order to initialize the model just once on container startup, we use Modal's [container\nlifecycle](https://modal.com/docs/guide/lifecycle-functions) feature, which requires the function to be part\nof a class.  The `modal.Volume` is mounted at `MODEL_DIR`, so that the fine-tuned model created  by `train` is then available to `inference`.\n```python\n@stub.cls(\n    image=image,\n    gpu=\"A100\",\n    volumes={str(MODEL_DIR): volume},\n)\nclass Model:\n    def __enter__(self):\n        import torch\n        from diffusers import DDIMScheduler, StableDiffusionPipeline\n        # Reload the modal.Volume to ensure the latest state is accessible.\n        stub.app.volume.reload()\n        # set up a hugging face inference pipeline using our model\n        ddim = DDIMScheduler.from_pretrained(MODEL_DIR, subfolder=\"scheduler\")\n        pipe = StableDiffusionPipeline.from_pretrained(\n            MODEL_DIR,\n            scheduler=ddim,\n            torch_dtype=torch.float16,\n            safety_checker=None,\n        ).to(\"cuda\")\n        pipe.enable_xformers_memory_efficient_attention()\n        self.pipe = pipe\n    @method()\n    def inference(self, text, config):\n        image = self.pipe(\n            text,\n            num_inference_steps=config.num_inference_steps,\n            guidance_scale=config.guidance_scale,\n        ).images[0]\n        return image\n```\n## Wrap the trained model in Gradio's web UI\nGradio.app makes it super easy to expose a model's functionality\nin an easy-to-use, responsive web interface.\nThis model is a text-to-image generator,\nso we set up an interface that includes a user-entry text box\nand a frame for displaying images.\nWe also provide some example text inputs to help\nguide users and to kick-start their creative juices.\nYou can deploy the app on Modal forever with the command\n`modal deploy dreambooth_app.py`.\n```python\n@stub.function(\n    image=image,\n    concurrency_limit=3,\n    mounts=[Mount.from_local_dir(assets_path, remote_path=\"/assets\")],\n)\n@asgi_app()\ndef fastapi_app():\n    import gradio as gr\n    from gradio.routes import mount_gradio_app\n    # Call to the GPU inference function on Modal.\n    def go(text):\n        return Model().inference.remote(text, config)\n    # set up AppConfig\n    config = AppConfig()\n    instance_phrase = f\"{config.instance_name} the {config.class_name}\"\n    example_prompts = [\n        f\"{instance_phrase}\",\n        f\"a painting of {instance_phrase.title()} With A Pearl Earring, by Vermeer\",\n        f\"oil painting of {instance_phrase} flying through space as an astronaut\",\n        f\"a painting of {instance_phrase} in cyberpunk city. character design by cory loftis. volumetric light, detailed, rendered in octane\",\n        f\"drawing of {instance_phrase} high quality, cartoon, path traced, by studio ghibli and don bluth\",\n    ]\n    modal_docs_url = \"https://modal.com/docs/guide\"\n    modal_example_url = f\"{modal_docs_url}/ex/dreambooth_app\"\n    description = f\"\"\"Describe what they are doing or how a particular artist or style would depict them. Be fantastical! Try the examples below for inspiration.\n### Learn how to make your own [here]({modal_example_url}).\n    \"\"\"\n    # add a gradio UI around inference\n    interface = gr.Interface(\n        fn=go,\n        inputs=\"text\",\n        outputs=gr.Image(shape=(512, 512)),\n        title=f\"Generate images of {instance_phrase}.\",\n        description=description,\n        examples=example_prompts,\n        css=\"/assets/index.css\",\n        allow_flagging=\"never\",\n    )\n    # mount for execution on Modal\n    return mount_gradio_app(\n        app=web_app,\n        blocks=interface,\n        path=\"/\",\n    )\n```\n## Running this on the command line\nYou can use the `modal` command-line interface to interact with this code,\nin particular training the model and running the interactive Gradio service\n- `modal run dreambooth_app.py` will train the model\n- `modal serve dreambooth_app.py` will [serve](https://modal.com/docs/guide/webhooks#developing-with-modal-serve) the Gradio interface at a temporarily location.\n- `modal shell dreambooth_app.py` is a convenient helper to open a bash [shell](https://modal.com/docs/guide/developing-debugging#stubinteractive_shell) in our image (for debugging)\nRemember, once you've trained your own fine-tuned model, you can deploy it using `modal deploy dreambooth_app.py`.\nThis app is already deployed on Modal and you can try it out at https://modal-labs-example-dreambooth-app-fastapi-app.modal.run\n```python\n@stub.local_entrypoint()\ndef run():\n    with open(TrainConfig().instance_example_urls_file) as f:\n        instance_example_urls = [line.strip() for line in f.readlines()]\n    train.remote(instance_example_urls)\n```\n"}
{"text": "\n```python\nimport modal\nstub = modal.Stub(\n    \"example-shell\", image=modal.Image.debian_slim().apt_install(\"vim\")\n)\nif __name__ == \"__main__\":\n    stub.interactive_shell(\"/bin/bash\")\n```\n"}
{"text": "\n# Stable Diffusion XL 1.0\nThis example is similar to the [Stable Diffusion CLI](/docs/guide/ex/stable_diffusion_cli)\nexample, but it generates images from the larger XL 1.0 model. Specifically, it runs the\nfirst set of steps with the base model, followed by the refiner model.\n[Try out the live demo here!](https://modal-labs--stable-diffusion-xl-app.modal.run/) The first\ngeneration may include a cold-start, which takes around 20 seconds. The inference speed depends on the GPU\nand step count (for reference, an A100 runs 40 steps in 8 seconds).\n## Basic setup\n```python\nfrom pathlib import Path\nfrom modal import Image, Mount, Stub, asgi_app, gpu, method\n```\n## Define a container image\nTo take advantage of Modal's blazing fast cold-start times, we'll need to download our model weights\ninside our container image with a download function. We ignore binaries, ONNX weights and 32-bit weights.\nTip: avoid using global variables in this function to ensure the download step detects model changes and\ntriggers a rebuild.\n```python\ndef download_models():\n    from huggingface_hub import snapshot_download\n    ignore = [\"*.bin\", \"*.onnx_data\", \"*/diffusion_pytorch_model.safetensors\"]\n    snapshot_download(\n        \"stabilityai/stable-diffusion-xl-base-1.0\", ignore_patterns=ignore\n    )\n    snapshot_download(\n        \"stabilityai/stable-diffusion-xl-refiner-1.0\", ignore_patterns=ignore\n    )\nimage = (\n    Image.debian_slim()\n    .apt_install(\n        \"libglib2.0-0\", \"libsm6\", \"libxrender1\", \"libxext6\", \"ffmpeg\", \"libgl1\"\n    )\n    .pip_install(\n        \"diffusers~=0.19\",\n        \"invisible_watermark~=0.1\",\n        \"transformers~=4.31\",\n        \"accelerate~=0.21\",\n        \"safetensors~=0.3\",\n    )\n    .run_function(download_models)\n)\nstub = Stub(\"stable-diffusion-xl\", image=image)\n```\n## Load model and run inference\nThe container lifecycle [`__enter__` function](https://modal.com/docs/guide/lifecycle-functions#container-lifecycle-beta)\nloads the model at startup. Then, we evaluate it in the `run_inference` function.\nTo avoid excessive cold-starts, we set the idle timeout to 240 seconds, meaning once a GPU has loaded the model it will stay\nonline for 4 minutes before spinning down. This can be adjusted for cost/experience trade-offs.\n```python\n@stub.cls(gpu=gpu.A10G(), container_idle_timeout=240)\nclass Model:\n    def __enter__(self):\n        import torch\n        from diffusers import DiffusionPipeline\n        load_options = dict(\n            torch_dtype=torch.float16,\n            use_safetensors=True,\n            variant=\"fp16\",\n            device_map=\"auto\",\n        )\n        # Load base model\n        self.base = DiffusionPipeline.from_pretrained(\n            \"stabilityai/stable-diffusion-xl-base-1.0\", **load_options\n        )\n        # Load refiner model\n        self.refiner = DiffusionPipeline.from_pretrained(\n            \"stabilityai/stable-diffusion-xl-refiner-1.0\",\n            text_encoder_2=self.base.text_encoder_2,\n            vae=self.base.vae,\n            **load_options,\n        )\n        # These suggested compile commands actually increase inference time, but may be mis-used.\n        # self.base.unet = torch.compile(self.base.unet, mode=\"reduce-overhead\", fullgraph=True)\n        # self.refiner.unet = torch.compile(self.refiner.unet, mode=\"reduce-overhead\", fullgraph=True)\n    @method()\n    def inference(self, prompt, n_steps=24, high_noise_frac=0.8):\n        negative_prompt = \"disfigured, ugly, deformed\"\n        image = self.base(\n            prompt=prompt,\n            negative_prompt=negative_prompt,\n            num_inference_steps=n_steps,\n            denoising_end=high_noise_frac,\n            output_type=\"latent\",\n        ).images\n        image = self.refiner(\n            prompt=prompt,\n            negative_prompt=negative_prompt,\n            num_inference_steps=n_steps,\n            denoising_start=high_noise_frac,\n            image=image,\n        ).images[0]\n        import io\n        byte_stream = io.BytesIO()\n        image.save(byte_stream, format=\"PNG\")\n        image_bytes = byte_stream.getvalue()\n        return image_bytes\n```\nAnd this is our entrypoint; where the CLI is invoked. Explore CLI options\nwith: `modal run stable_diffusion_xl.py --prompt 'An astronaut riding a green horse'`\n```python\n@stub.local_entrypoint()\ndef main(prompt: str):\n    image_bytes = Model().inference.remote(prompt)\n    dir = Path(\"/tmp/stable-diffusion-xl\")\n    if not dir.exists():\n        dir.mkdir(exist_ok=True, parents=True)\n    output_path = dir / \"output.png\"\n    print(f\"Saving it to {output_path}\")\n    with open(output_path, \"wb\") as f:\n        f.write(image_bytes)\n```\n## A user interface\nHere we ship a simple web application that exposes a front-end (written in Alpine.js) for\nour backend deployment.\nThe Model class will serve multiple users from a its own shared pool of warm GPU containers automatically.\nWe can deploy this with `modal deploy stable_diffusion_xl.py`.\n```python\nfrontend_path = Path(__file__).parent / \"frontend\"\n@stub.function(\n    mounts=[Mount.from_local_dir(frontend_path, remote_path=\"/assets\")],\n    allow_concurrent_inputs=20,\n)\n@asgi_app()\ndef app():\n    import fastapi.staticfiles\n    from fastapi import FastAPI\n    web_app = FastAPI()\n    @web_app.get(\"/infer/{prompt}\")\n    async def infer(prompt: str):\n        from fastapi.responses import Response\n        image_bytes = Model().inference.remote(prompt)\n        return Response(image_bytes, media_type=\"image/png\")\n    web_app.mount(\n        \"/\", fastapi.staticfiles.StaticFiles(directory=\"/assets\", html=True)\n    )\n    return web_app\n```\n"}
{"text": "\n```python\nimport sys\nfrom modal import Image, Stub, method\n```\nDefine a function for downloading the models, that will run once on image build.\nThis allows the weights to be present inside the image for faster startup.\n```python\nbase_model = \"decapoda-research/llama-7b-hf\"\nlora_weights = \"tloen/alpaca-lora-7b\"\ndef download_models():\n    from peft import PeftModel\n    from transformers import LlamaForCausalLM, LlamaTokenizer\n    model = LlamaForCausalLM.from_pretrained(\n        base_model,\n    )\n    PeftModel.from_pretrained(model, lora_weights)\n    LlamaTokenizer.from_pretrained(base_model)\n```\nAlpaca-LoRA is distributed as a public Github repository and the repository is not\ninstallable by `pip`, so instead we install the repository by cloning it into our Modal\nimage.\n```python\nrepo_url = \"https://github.com/tloen/alpaca-lora\"\ncommit_hash = \"fcbc45e4c0db8948743bd1227b46a796c1effcd0\"\nimage = (\n    Image.debian_slim()\n    .apt_install(\"git\")\n    # Here we place the latest repository code into /root.\n    # Because /root is almost empty, but not entirely empty, `git clone` won't work,\n    # so this `init` then `checkout` workaround is used.\n    .run_commands(\n        \"cd /root && git init .\",\n        f\"cd /root && git remote add --fetch origin {repo_url}\",\n        f\"cd /root && git checkout {commit_hash}\",\n    )\n    # The alpaca-lora repository's dependencies list is in the repository,\n    # but it's currently missing a dependency and not specifying dependency versions,\n    # which leads to issues: https://github.com/tloen/alpaca-lora/issues/200.\n    # So we install a strictly versioned dependency list. This list excludes one or two\n    # dependencies listed by `tloen/alpaca-lora` but that are irrelevant within Modal,\n    # e.g. `black` code formatting library.\n    .pip_install(\n        \"accelerate==0.18.0\",\n        \"appdirs==1.4.4\",\n        \"bitsandbytes==0.37.0\",\n        \"bitsandbytes-cuda117==0.26.0.post2\",\n        \"datasets==2.10.1\",\n        \"fire==0.5.0\",\n        \"gradio==3.23.0\",\n        \"peft @ git+https://github.com/huggingface/peft.git@d8c3b6bca49e4aa6e0498b416ed9adc50cc1a5fd\",\n        \"transformers @ git+https://github.com/huggingface/transformers.git@a92e0ad2e20ef4ce28410b5e05c5d63a5a304e65\",\n        \"torch==2.0.0\",\n        \"torchvision==0.15.1\",\n        \"sentencepiece==0.1.97\",\n    )\n    .run_function(download_models)\n)\nstub = Stub(name=\"example-alpaca-lora\", image=image)\n```\nThe Alpaca-LoRA model is integrated into model as a Python class with an __enter__\nmethod to take advantage of Modal's container lifecycle functionality.\nhttps://modal.com/docs/guide/lifecycle-functions#container-lifecycle-beta\nOn each container startup the model is loaded once and then subsequent model\ntext generations are run 'warm' with the model already initialized in memory.\n```python\n@stub.cls(gpu=\"A10G\")\nclass AlpacaLoRAModel:\n    def __enter__(self):\n        \"\"\"\n        Container-lifeycle method for model setup. Code is taken from\n        https://github.com/tloen/alpaca-lora/blob/main/generate.py and minor\n        modifications are made to support usage in a Python class.\n        \"\"\"\n        import torch\n        from peft import PeftModel\n        from transformers import LlamaForCausalLM, LlamaTokenizer\n        load_8bit = False\n        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n        self.tokenizer = LlamaTokenizer.from_pretrained(base_model)\n        if device == \"cuda\":\n            model = LlamaForCausalLM.from_pretrained(\n                base_model,\n                load_in_8bit=load_8bit,\n                torch_dtype=torch.float16,\n                device_map=\"auto\",\n            )\n            model = PeftModel.from_pretrained(\n                model,\n                lora_weights,\n                torch_dtype=torch.float16,\n            )\n        elif device == \"mps\":\n            model = LlamaForCausalLM.from_pretrained(\n                base_model,\n                device_map={\"\": device},\n                torch_dtype=torch.float16,\n            )\n            model = PeftModel.from_pretrained(\n                model,\n                lora_weights,\n                device_map={\"\": device},\n                torch_dtype=torch.float16,\n            )\n        else:\n            model = LlamaForCausalLM.from_pretrained(\n                base_model, device_map={\"\": device}, low_cpu_mem_usage=True\n            )\n            model = PeftModel.from_pretrained(\n                model,\n                lora_weights,\n                device_map={\"\": device},\n            )\n        # unwind broken decapoda-research config\n        model.config.pad_token_id = self.tokenizer.pad_token_id = 0  # unk\n        model.config.bos_token_id = 1\n        model.config.eos_token_id = 2\n        if not load_8bit:\n            model.half()  # seems to fix bugs for some users.\n        model.eval()\n        if torch.__version__ >= \"2\" and sys.platform != \"win32\":\n            model = torch.compile(model)\n        self.model = model\n        self.device = device\n    def evaluate(\n        self,\n        instruction,\n        input=None,\n        temperature=0.1,\n        top_p=0.75,\n        top_k=40,\n        num_beams=1,\n        max_new_tokens=128,\n        **kwargs,\n    ):\n        import torch\n        from generate import generate_prompt\n        from transformers import GenerationConfig\n        prompt = generate_prompt(instruction, input)\n        inputs = self.tokenizer(prompt, return_tensors=\"pt\")\n        input_ids = inputs[\"input_ids\"].to(self.device)\n        generation_config = GenerationConfig(\n            temperature=temperature,\n            top_p=top_p,\n            top_k=top_k,\n            num_beams=num_beams,\n            do_sample=temperature > 0,\n            **kwargs,\n        )\n        with torch.no_grad():\n            generation_output = self.model.generate(\n                input_ids=input_ids,\n                generation_config=generation_config,\n                return_dict_in_generate=True,\n                output_scores=True,\n                max_new_tokens=max_new_tokens,\n            )\n        s = generation_output.sequences[0]\n        output = self.tokenizer.decode(s)\n        return output.split(\"### Response:\")[1].strip()\n    @method()\n    def generate(self, instructions: list[str]):\n        for instrctn in instructions:\n            print(f\"\\033[96mInstruction: {instrctn}\\033[0m\")\n            print(\"Response:\", self.evaluate(instrctn))\n            print()\n```\nThis Modal app local entrypoint just runs the example instructions shown in the\nrepository's README: https://github.com/tloen/alpaca-lora#example-outputs.\nRun this app with `modal run alpaca_lora.py` and you can cross-reference against the\nrepository to see how well the outputs match.\n```python\n@stub.local_entrypoint()\ndef main():\n    instructions = [\n        \"Tell me about alpacas.\",\n        \"Tell me about the president of Mexico in 2019.\",\n        \"Tell me about the king of France in 2019.\",\n        \"List all Canadian provinces in alphabetical order.\",\n        \"Write a Python program that prints the first 10 Fibonacci numbers.\",\n        \"Write a program that prints the numbers from 1 to 100. But for multiples of three print 'Fizz' instead of the number and for the multiples of five print 'Buzz'. For numbers which are multiples of both three and five print 'FizzBuzz'.\",  # noqa: E501\n        \"Tell me five words that rhyme with 'shock'.\",\n        \"Translate the sentence 'I have no mouth but I must scream' into Spanish.\",\n        \"Count up from 1 to 500.\",\n    ]\n    model = AlpacaLoRAModel()\n    model.generate.remote(instructions)\n```\n"}
{"text": "\n# Fetching stock prices in parallel\nThis is a simple example that uses the Yahoo! Finance API to fetch a bunch of ETFs\nWe do this in parallel, which demonstrates the ability to map over a set of items\nIn this case, we fetch 100 stocks in parallel\nYou can run this script on the terminal with\n```bash\nmodal run 03_scaling_out/fetch_stock_prices.py\n```\nIf everything goes well, it should plot something like this:\n![stock prices](./stock_prices.png)\n## Setup\nFor this image, we need\n- `httpx` and `beautifulsoup4` to fetch a list of ETFs from a HTML page\n- `yfinance` to fetch stock prices from the Yahoo Finance API\n- `matplotlib` to plot the result\n```python\nimport io\nimport os\nimport modal\nstub = modal.Stub(\n    \"example-fetch-stock-prices\",\n    image=modal.Image.debian_slim().pip_install(\n        \"httpx~=0.24.0\",\n        \"yfinance~=0.2.18\",\n        \"beautifulsoup4~=4.12.2\",\n        \"matplotlib~=3.7.1\",\n    ),\n)\n```\n## Fetch a list of tickers\nThe `yfinance` package does not have a way to download a list of stocks.\nTo get a list of stocks, we parse the HTML from Yahoo Finance using Beautiful Soup\nand ask for the top 100 ETFs.\n```python\n@stub.function()\ndef get_stocks():\n    import bs4\n    import httpx\n    headers = {\n        \"user-agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.122 Safari/537.36\",\n        \"referer\": \"https://finance.yahoo.com/\",\n    }\n    url = \"https://finance.yahoo.com/etfs?count=100&offset=0\"\n    res = httpx.get(url, headers=headers)\n    res.raise_for_status()\n    soup = bs4.BeautifulSoup(res.text, \"html.parser\")\n    for td in soup.find_all(\"td\", {\"aria-label\": \"Symbol\"}):\n        for link in td.find_all(\"a\", {\"data-test\": \"quoteLink\"}):\n            symbol = str(link.next)\n            print(f\"Found symbol {symbol}\")\n            yield symbol\n```\n## Fetch stock prices\nNow, let's fetch the stock data. This is the function that we will parallelize.\nIt's fairly simple and just uses the `yfinance` package.\n```python\n@stub.function()\ndef get_prices(symbol):\n    import yfinance\n    print(f\"Fetching symbol {symbol}...\")\n    ticker = yfinance.Ticker(symbol)\n    data = ticker.history(period=\"1Y\")[\"Close\"]\n    print(f\"Done fetching symbol {symbol}!\")\n    return symbol, data.to_dict()\n```\n## Plot the result\nHere is our plotting code. We run this in Modal, although you could also run it locally.\nNote that the plotting code calls the other two functions.\nSince we plot the data in the cloud, we can't display it, so we generate a PNG\nand return the binary content from the function.\n```python\n@stub.function()\ndef plot_stocks():\n    from matplotlib import pyplot, ticker\n    # Setup\n    pyplot.style.use(\"ggplot\")\n    fig, ax = pyplot.subplots(figsize=(8, 5))\n    # Get data\n    tickers = list(get_stocks.remote_gen())\n    if not tickers:\n        raise RuntimeError(\"Retrieved zero stock tickers!\")\n    data = list(get_prices.map(tickers))\n    first_date = min((min(prices.keys()) for symbol, prices in data if prices))\n    last_date = max((max(prices.keys()) for symbol, prices in data if prices))\n    # Plot every symbol\n    for symbol, prices in data:\n        if len(prices) == 0:\n            continue\n        dates = list(sorted(prices.keys()))\n        prices = list(prices[date] for date in dates)\n        changes = [\n            100.0 * (price / prices[0] - 1) for price in prices\n        ]  # Normalize to initial price\n        if changes[-1] > 20:\n            # Highlight this line\n            p = ax.plot(dates, changes, alpha=0.7)\n            ax.annotate(\n                symbol,\n                (last_date, changes[-1]),\n                ha=\"left\",\n                va=\"center\",\n                color=p[0].get_color(),\n                alpha=0.7,\n            )\n        else:\n            ax.plot(dates, changes, color=\"gray\", alpha=0.2)\n    # Configure axes and title\n    ax.yaxis.set_major_formatter(ticker.PercentFormatter())\n    ax.set_title(f\"Best ETFs {first_date.date()} - {last_date.date()}\")\n    ax.set_ylabel(f\"% change, {first_date.date()} = 0%\")\n    # Dump the chart to .png and return the bytes\n    with io.BytesIO() as buf:\n        pyplot.savefig(buf, format=\"png\", dpi=300)\n        return buf.getvalue()\n```\n## Entrypoint\nThe entrypoint locally runs the app, gets the chart back as a PNG file, and\nsaves it to disk.\n```python\nOUTPUT_DIR = \"/tmp/\"\n@stub.local_entrypoint()\ndef main():\n    os.makedirs(OUTPUT_DIR, exist_ok=True)\n    data = plot_stocks.remote()\n    filename = os.path.join(OUTPUT_DIR, \"stock_prices.png\")\n    print(f\"saving data to {filename}\")\n    with open(filename, \"wb\") as f:\n        f.write(data)\n```\n"}
{"text": "# Run Falcon-40B with AutoGPTQ\nIn this example, we run a quantized 4-bit version of Falcon-40B, the first open-source large language\nmodel of its size, using HuggingFace's [transformers](https://huggingface.co/docs/transformers/index)\nlibrary and [AutoGPTQ](https://github.com/PanQiWei/AutoGPTQ).\nDue to the current limitations of the library, the inference speed is a little under 1 token/second and the\ncold start time on Modal is around 25s.\nFor faster inference at the expense of a slower cold start, check out\n[Running Falcon-40B with `bitsandbytes` quantization](/docs/guide/ex/falcon_bitsandbytes). You can also\nrun a smaller, 7-billion-parameter model with the [OpenLLaMa example](/docs/guide/ex/openllama).\n## Setup\nFirst we import the components we need from `modal`.\n```python\nfrom modal import Image, Stub, gpu, method, web_endpoint\n```\n## Define a container image\nTo take advantage of Modal's blazing fast cold-start times, we download model weights\ninto a folder inside our container image. These weights come from a quantized model\nfound on Huggingface.\n```python\nIMAGE_MODEL_DIR = \"/model\"\ndef download_model():\n    from huggingface_hub import snapshot_download\n    model_name = \"TheBloke/falcon-40b-instruct-GPTQ\"\n    snapshot_download(model_name, local_dir=IMAGE_MODEL_DIR)\n```\nNow, we define our image. We'll use the `debian-slim` base image, and install the dependencies we need\nusing [`pip_install`](/docs/reference/modal.Image#pip_install). At the end, we'll use\n[`run_function`](/docs/guide/custom-container#running-a-function-as-a-build-step-beta) to run the\nfunction defined above as part of the image build.\n```python\nimage = (\n    Image.debian_slim(python_version=\"3.10\")\n    .apt_install(\"git\")\n    .pip_install(\n        \"auto-gptq @ git+https://github.com/PanQiWei/AutoGPTQ.git@b5db750c00e5f3f195382068433a3408ec3e8f3c\",\n        \"einops==0.6.1\",\n        \"hf-transfer~=0.1\",\n        \"huggingface_hub==0.14.1\",\n        \"transformers @ git+https://github.com/huggingface/transformers.git@f49a3453caa6fe606bb31c571423f72264152fce\",\n    )\n    # Use huggingface's hi-perf hf-transfer library to download this large model.\n    .env({\"HF_HUB_ENABLE_HF_TRANSFER\": \"1\"})\n    .run_function(download_model)\n)\n```\nLet's instantiate and name our [Stub](/docs/guide/apps).\n```python\nstub = Stub(name=\"example-falcon-gptq\", image=image)\n```\n## The model class\nNext, we write the model code. We want Modal to load the model into memory just once every time a container starts up,\nso we use [class syntax](/docs/guide/lifecycle-functions) and the `__enter__` method.\nWithin the [@stub.cls](/docs/reference/modal.Stub#cls) decorator, we use the [gpu parameter](/docs/guide/gpu)\nto specify that we want to run our function on an [A100 GPU](/pricing). We also allow each call 10 mintues to complete,\nand request the runner to stay live for 5 minutes after its last request.\nThe rest is just using the `transformers` library to run the model. Refer to the\n[documentation](https://huggingface.co/docs/transformers/v4.29.1/en/main_classes/text_generation#transformers.GenerationMixin.generate)\nfor more parameters and tuning.\nNote that we need to create a separate thread to call the `generate` function because we need to\nyield the text back from the streamer in the main thread. This is an idiosyncrasy with streaming in `transformers`.\n```python\n@stub.cls(gpu=gpu.A100(), timeout=60 * 10, container_idle_timeout=60 * 5)\nclass Falcon40BGPTQ:\n    def __enter__(self):\n        from auto_gptq import AutoGPTQForCausalLM\n        from transformers import AutoTokenizer\n        self.tokenizer = AutoTokenizer.from_pretrained(\n            IMAGE_MODEL_DIR, use_fast=True\n        )\n        print(\"Loaded tokenizer.\")\n        self.model = AutoGPTQForCausalLM.from_quantized(\n            IMAGE_MODEL_DIR,\n            trust_remote_code=True,\n            use_safetensors=True,\n            device_map=\"auto\",\n            use_triton=False,\n            strict=False,\n        )\n        print(\"Loaded model.\")\n    @method()\n    def generate(self, prompt: str):\n        from threading import Thread\n        from transformers import TextIteratorStreamer\n        inputs = self.tokenizer(prompt, return_tensors=\"pt\")\n        streamer = TextIteratorStreamer(\n            self.tokenizer, skip_special_tokens=True\n        )\n        generation_kwargs = dict(\n            inputs=inputs.input_ids.cuda(),\n            attention_mask=inputs.attention_mask,\n            temperature=0.1,\n            max_new_tokens=512,\n            streamer=streamer,\n        )\n        # Run generation on separate thread to enable response streaming.\n        thread = Thread(target=self.model.generate, kwargs=generation_kwargs)\n        thread.start()\n        for new_text in streamer:\n            yield new_text\n        thread.join()\n```\n## Run the model\nWe define a [`local_entrypoint`](/docs/guide/apps#entrypoints-for-ephemeral-apps) to call our remote function\nsequentially for a list of inputs. You can run this locally with `modal run -q falcon_gptq.py`. The `-q` flag\nenables streaming to work in the terminal output.\n```python\nprompt_template = (\n    \"A chat between a curious human user and an artificial intelligence assistant. The assistant give a helpful, detailed, and accurate answer to the user's question.\"\n    \"\\n\\nUser:\\n{}\\n\\nAssistant:\\n\"\n)\n@stub.local_entrypoint()\ndef cli():\n    question = \"What are the main differences between Python and JavaScript programming languages?\"\n    model = Falcon40BGPTQ()\n    for text in model.generate.remote_gen(prompt_template.format(question)):\n        print(text, end=\"\", flush=True)\n```\n## Serve the model\nFinally, we can serve the model from a web endpoint with `modal deploy falcon_gptq.py`. If\nyou visit the resulting URL with a question parameter in your URL, you can view the model's\nstream back a response.\nYou can try our deployment [here](https://modal-labs--example-falcon-gptq-get.modal.run/?question=Why%20are%20manhole%20covers%20round?).\n```python\n@stub.function(timeout=60 * 10)\n@web_endpoint()\ndef get(question: str):\n    from itertools import chain\n    from fastapi.responses import StreamingResponse\n    model = Falcon40BGPTQ()\n    return StreamingResponse(\n        chain(\n            (\"Loading model. This usually takes around 20s ...\\n\\n\"),\n            model.generate.remote_gen(prompt_template.format(question)),\n        ),\n        media_type=\"text/event-stream\",\n    )\n```\n"}
{"text": "\n```python\nimport modal\nstub = modal.Stub(\"example-get-started\")\n@stub.function()\ndef square(x):\n    print(\"This code is running on a remote worker!\")\n    return x**2\n@stub.local_entrypoint()\ndef main():\n    print(\"the square is\", square.remote(42))\n```\n"}
{"text": "\n```python\n\"\"\"\nComputes sentence embeddings using Modal.\nThis uses huggingface's `transformer` library to calculate the vector\nrepresentations for a collection of sentences.\nExample modified from: https://huggingface.co/sentence-transformers/paraphrase-xlm-r-multilingual-v1\nInstall dependencies before running example:\n$ pip3 install torch==1.10.2 tqdm numpy requests tensorboard\n\"\"\"\nimport tarfile\nfrom pathlib import Path\nfrom typing import List\nimport modal\n```\ndependencies\n```python\ndependencies = [\"torch==1.10.2\", \"transformers==4.16.2\", \"tensorboard\"]\nstub = modal.Stub(\n    \"example-sentence-embeddings\",\n    image=modal.Image.debian_slim().pip_install(*dependencies),\n)\nif stub.is_inside():\n    import numpy as np\n    import requests\n    import torch\n    from torch.utils.tensorboard import SummaryWriter\n    from transformers import AutoModel, AutoTokenizer\n    TOKENIZER = AutoTokenizer.from_pretrained(\n        \"sentence-transformers/paraphrase-xlm-r-multilingual-v1\"\n    )\n    MODEL = AutoModel.from_pretrained(\n        \"sentence-transformers/paraphrase-xlm-r-multilingual-v1\"\n    )\ndef mean_pooling(model_output, attention_mask):\n    token_embeddings = model_output[\n        0\n    ]  # First element of model_output contains all token embeddings\n    input_mask_expanded = (\n        attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n    )\n    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(\n        input_mask_expanded.sum(1), min=1e-9\n    )\n@stub.function()\ndef vectorize(x: str):\n    \"\"\"Vectorizes a string (sentence) into a PyTorch Tensor.\"\"\"\n    # encode input and calculate vector\n    encoded_input = TOKENIZER(\n        x, padding=True, truncation=True, return_tensors=\"pt\"\n    )\n    model_output = MODEL(**encoded_input)\n    y = mean_pooling(model_output, encoded_input[\"attention_mask\"])\n    sentence = f\"{x[:100]} ...\"\n    print(f\" \u2192 calculated vector for: {sentence}\")\n    return (sentence, y.detach().numpy())\ndef write_tensorboard_logs(embedding, metadata: List[str]):\n    \"\"\"Write tensorboard logs.\"\"\"\n    print(\" \u2192 writing tensorboard logs\")\n    writer = SummaryWriter(log_dir=\"./tensorboard\")\n    writer.add_embedding(np.concatenate(embedding), metadata)\n    writer.close()\nclass MovieReviewsDataset:\n    \"\"\"Standford's Large Movie Review Dataset.\"\"\"\n    url: str = \"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n    def __init__(\n        self, output_path: Path = Path(\"./movie_reviews_dataset.tar.gz\")\n    ):\n        self.output_path = output_path\n        self.dataset: List[str] = []\n    def download(self) -> None:\n        \"\"\"Downloads dataset if not available locally.\"\"\"\n        if self.output_path.exists():\n            print(\" \u2192 using cached dataset\")\n            return\n        print(f\" \u2192 downloading dataset from {self.url}\")\n        with requests.get(self.url, stream=True) as r:\n            r.raise_for_status()\n            with open(self.output_path, \"wb\") as f:\n                for chunk in r.iter_content(chunk_size=8192):\n                    # If you have chunk encoded response uncomment if\n                    # and set chunk_size parameter to None.\n                    # if chunk:\n                    f.write(chunk)\n        print(\" \u2192 dataset downloaded\")\n    def extract(self) -> None:\n        \"\"\"Extracts dataset package into local path.\"\"\"\n        if Path(\"./aclImdb\").exists():\n            print(\" \u2192 using cached extracted dataset\")\n            return\n        print(f\" \u2192 extracting file {self.output_path}\")\n        with tarfile.open(self.output_path) as tf:\n            tf.extractall()\n    def index(self) -> list[Path]:\n        \"\"\"Creates in-memory index for dataset\"\"\"\n        # read only train files\n        dataset_path = Path(\"./aclImdb/train\")\n        # combine both negative and positive files\n        negative = [f for f in (dataset_path / \"neg\").glob(\"*.txt\")]\n        positive = [f for f in (dataset_path / \"pos\").glob(\"*.txt\")]\n        return negative + positive\n    def load(self, sample_size: int = 100) -> None:\n        \"\"\"Loads dataset into memory\"\"\"\n        print(f\" \u2192 loading dataset with sample size = {sample_size}\")\n        for file in np.random.choice(self.index(), size=sample_size):\n            self.dataset.append(file.open().read())  # limit to 500 characters\n@stub.local_entrypoint()\ndef main():\n    # download & prepare dataset\n    dataset = MovieReviewsDataset()\n    dataset.download()\n    dataset.extract()\n    # load dataset into memory; use a small samplesize (N=25k)\n    dataset.load(sample_size=100)\n    # vectorize the entire dataset\n    embedding = []\n    metadata = []\n    with stub.run():\n        for sentence, vector in vectorize.map(dataset.dataset):\n            embedding.append(vector)\n            metadata.append(sentence)\n        write_tensorboard_logs(embedding, metadata)\n    # open tensorboard\n    print(\" \u2192 done!\")\n    print(\n        \" \u2192 to see results in TensorBoard, run: tensorboard --logdir tensorboard/\"\n    )\n    print(\" \u2192 (open http://localhost:6006#projector and wait for it to load)\")\n```\n"}
{"text": "\n# Stable Diffusion (AITemplate Edition)\nExample by [@maxscheel](https://github.com/maxscheel)\nThis example shows the Stable Diffusion 2.1 compiled with [AITemplate](https://github.com/facebookincubator/AITemplate) to run faster on Modal.\nThere is also a [Stable Diffusion CLI example](/docs/guide/ex/stable_diffusion_cli).\n#### Upsides\n - Image generation improves over the CLI example to about 550ms per image generated (A10G, 10 steps, 512x512, png).\n#### Downsides\n - Width and height as well as batch size must be configured prior to compilation which takes about 15 minutes.\n - In this example the compilation is done at docker image creation.\n - Cold start time are also increased to up-to ~30s from ~10s.\n## Setup\n```python\nimport io\nimport os\nimport sys\nimport modal\nfrom fastapi import FastAPI, Response\nfrom pydantic import BaseModel\n```\nSet cache path, size of output image, and stable diffusion version.\n```python\nHF_CACHE_DIR: str = \"/root/.cache/huggingface\"\nAIT_BUILD_CACHE_DIR: str = \"/root/.cache/aitemplate\"\nGPU_TYPE: str = \"A10G\"\nMODEL_ID: str = \"stabilityai/stable-diffusion-2-1\"\nWIDTH: int = 512\nHEIGHT: int = 512\nBATCH_SIZE: int = 1\nMODEL_PATH: str = \"./tmp/diffusers/\"\ndef set_paths():\n    ait_sd_example_path = \"/app/AITemplate/examples/05_stable_diffusion\"\n    os.chdir(ait_sd_example_path)\n    sys.path.append(ait_sd_example_path)\n```\nDownload and compile model during image creation. This will store both the\noriginal HuggingFace model and the AITemplate compiled artifacts in the image,\nmaking startup times slightly faster.\n```python\ndef download_and_compile():\n    import diffusers\n    import torch\n    set_paths()\n    os.environ[\"AIT_BUILD_CACHE_DIR\"] = AIT_BUILD_CACHE_DIR\n    # Download model and scheduler\n    diffusers.StableDiffusionPipeline.from_pretrained(\n        MODEL_ID,\n        revision=\"fp16\",\n        torch_dtype=torch.float16,\n        use_auth_token=os.environ[\"HUGGINGFACE_TOKEN\"],\n    ).save_pretrained(MODEL_PATH, safe_serialization=True)\n    diffusers.EulerDiscreteScheduler.from_pretrained(\n        MODEL_PATH,\n        subfolder=\"scheduler\",\n    ).save_pretrained(MODEL_PATH, safe_serialization=True)\n    # Compilation\n    from src.compile_lib.compile_clip import compile_clip\n    from src.compile_lib.compile_unet import compile_unet\n    from src.compile_lib.compile_vae import compile_vae\n    pipe = diffusers.StableDiffusionPipeline.from_pretrained(\n        MODEL_PATH, revision=\"fp16\", torch_dtype=torch.float16\n    )\n    compile_clip(\n        pipe.text_encoder,\n        batch_size=BATCH_SIZE,\n        seqlen=77,\n        use_fp16_acc=True,\n        convert_conv_to_gemm=True,\n        depth=pipe.text_encoder.config.num_hidden_layers,\n        num_heads=pipe.text_encoder.config.num_attention_heads,\n        dim=pipe.text_encoder.config.hidden_size,\n        act_layer=pipe.text_encoder.config.hidden_act,\n    )\n    compile_unet(\n        pipe.unet,\n        batch_size=BATCH_SIZE * 2,\n        width=WIDTH // 8,\n        height=HEIGHT // 8,\n        use_fp16_acc=True,\n        convert_conv_to_gemm=True,\n        hidden_dim=pipe.unet.config.cross_attention_dim,\n        attention_head_dim=pipe.unet.config.attention_head_dim,\n        use_linear_projection=pipe.unet.config.get(\n            \"use_linear_projection\", False\n        ),\n    )\n    compile_vae(\n        pipe.vae,\n        batch_size=BATCH_SIZE,\n        width=WIDTH // 8,\n        height=HEIGHT // 8,\n        use_fp16_acc=True,\n        convert_conv_to_gemm=True,\n    )\ndef _get_pipe():\n    set_paths()\n    os.environ[\"AIT_BUILD_CACHE_DIR\"] = AIT_BUILD_CACHE_DIR\n    import torch\n    from diffusers import EulerDiscreteScheduler\n    from src.pipeline_stable_diffusion_ait import StableDiffusionAITPipeline\n    torch.backends.cudnn.benchmark = True\n    torch.backends.cuda.matmul.allow_tf32 = True\n    scheduler = EulerDiscreteScheduler.from_pretrained(\n        MODEL_PATH,\n        subfolder=\"scheduler\",\n        device_map=\"auto\",\n    )\n    pipe = StableDiffusionAITPipeline.from_pretrained(\n        MODEL_PATH,\n        scheduler=scheduler,\n        low_cpu_mem_usage=True,\n        torch_dtype=torch.float16,\n        device_map=\"auto\",\n    )\n    pipe.enable_xformers_memory_efficient_attention()\n    return pipe\ndef _inference(\n    pipe,\n    prompt: str,\n    num_inference_steps: int,\n    guidance_scale: float,\n    negative_prompt: str,\n    format: str = \"webp\",\n):\n    from torch import autocast, inference_mode\n    with inference_mode():\n        with autocast(\"cuda\"):\n            single_image = pipe(\n                prompt=[prompt] * BATCH_SIZE,\n                HEIGHT=HEIGHT,\n                WIDTH=WIDTH,\n                num_inference_steps=num_inference_steps,\n                guidance_scale=guidance_scale,\n                negative_prompt=[negative_prompt] * BATCH_SIZE,\n            ).images[0]\n            with io.BytesIO() as buf:\n                single_image.save(buf, format=format)\n                return Response(\n                    content=buf.getvalue(), media_type=f\"image/{format}\"\n                )\n```\n## Build image\nInstall AITemplate from source, download, and compile the configured model. We\nwill use an official NVIDIA image from [Docker Hub](https://hub.docker.com/r/nvidia/cuda)\nwhich include all required drivers.\n```python\nimage = (\n    modal.Image.from_registry(\n        \"nvidia/cuda:12.2.0-devel-ubuntu22.04\",\n        setup_dockerfile_commands=[\n            \"RUN apt-get update && apt-get install -y git python3-pip\",\n            \"RUN ln -s /usr/bin/python3 /usr/bin/python\",\n            \"WORKDIR /app\",\n            \"RUN git clone --recursive https://github.com/facebookincubator/AITemplate.git\",\n            \"WORKDIR /app/AITemplate/python\",\n            # Set hash for reproducibility\n            \"RUN git checkout 6305588af76eeec987762c5b5ee373a61f8a7fb3\",\n            # Build and install aitemplate library\n            \"RUN python setup.py bdist_wheel && pip install dist/aitemplate-*.whl && rm -rf dist\",\n            \"WORKDIR /app/AITemplate/examples/05_stable_diffusion\",\n            # Patch deprecated access of unet.in_channels (silence warning) in AIT pipeline example implementation\n            \"RUN sed -i src/pipeline_stable_diffusion_ait.py -e 's/unet.in_channels/unet.config.in_channels/g'\",\n        ],\n    )\n    .pip_install(\n        \"accelerate\",\n        \"diffusers[torch]>=0.15.1\",\n        \"ftfy\",\n        \"transformers~=4.25.1\",\n        \"safetensors\",\n        \"torch>=2.0\",\n        \"triton\",\n        \"xformers==0.0.20\",\n    )\n    .run_function(\n        download_and_compile,\n        secret=modal.Secret.from_name(\"huggingface-secret\"),\n        timeout=60 * 30,\n        gpu=GPU_TYPE,\n    )\n)\nfunction_params = {\n    \"image\": image,\n    \"concurrency_limit\": 1,\n    \"container_idle_timeout\": 60,\n    \"timeout\": 60,\n    \"gpu\": GPU_TYPE,\n}\nstub = modal.Stub(\"example-stable-diffusion-aitemplate\")\n```\n## Inference as asgi app\nWe load the pipe and serve the example via the `/inference` endpoint. You can interact\nthe endpoint via a `POST` request with a JSON payload containing parameters defined\nin `InferenceRequest`.\n```python\nclass InferenceRequest(BaseModel):\n    prompt: str = \"photo of a wolf in the snow, blue eyes, highly detailed, 8k, 200mm canon lens, shallow depth of field\"\n    num_inference_steps: int = 10\n    guidance_scale: float = 7.5\n    negative_prompt: str = \"deformed, extra legs, no tail\"\n    format: str = \"webp\"  # png or webp; webp is slightly faster\n@stub.function(**function_params)\n@modal.asgi_app(\n    label=f'{GPU_TYPE.lower()}-{WIDTH}-{HEIGHT}-{BATCH_SIZE}-{MODEL_ID.replace(\"/\",\"--\")}'\n)\ndef inference_asgi():\n    pipe = _get_pipe()\n    app = FastAPI()\n    @app.post(\"/inference\")\n    def inference(request: InferenceRequest):\n        return _inference(\n            pipe,\n            request.prompt,\n            request.num_inference_steps,\n            request.guidance_scale,\n            request.negative_prompt,\n            request.format,\n        )\n    return app\n```\nServe your app using `modal serve` as follows:\n```bash\nmodal serve stable_diffusion_aitemplate.py\n```\nGrab the Modal app URL then query the  API with curl:\n```bash\ncurl --location --request POST '$ENDPOINT_URL/inference' \\\n     --header 'Content-Type: application/json' \\\n     --data-raw '{\n        \"prompt\": \"photo of a wolf in the snow, blue eyes, highly detailed, 8k, 200mm canon lens, shallow depth of field\",\n        \"num_inference_steps\": 10,\n        \"guidance_scale\": 10.0,\n        \"negative_prompt\": \"deformed, extra legs, no tail\",\n        \"format\": \"webp\"\n     }'\n```\n"}
{"text": "\n# Document OCR job queue\nThis tutorial shows you how to use Modal as an infinitely scalable job queue\nthat can service async tasks from a web app. For the purpose of this tutorial,\nwe've also built a [React + FastAPI web app on Modal](/docs/guide/ex/doc_ocr_webapp)\nthat works together with it, but note that you don't need a web app running on Modal\nto use this pattern. You can submit async tasks to Modal from any Python\napplication (for example, a regular Django app running on Kubernetes).\nOur job queue will handle a single task: running OCR transcription for images.\nWe'll make use of a pre-trained Document Understanding model using the\n[donut](https://github.com/clovaai/donut) package to accomplish this. Try\nit out for yourself [here](https://modal-labs-example-doc-ocr-webapp-wrapper.modal.run/).\n![receipt parser frontend](./receipt_parser_frontend_2.jpg)\n## Define a Stub\nLet's first import `modal` and define a [`Stub`](/docs/reference/modal.Stub). Later, we'll use the name provided\nfor our `Stub` to find it from our web app, and submit tasks to it.\n```python\nimport urllib.request\nimport modal\nstub = modal.Stub(\"example-doc-ocr-jobs\")\n```\n## Model cache\n`donut` downloads the weights for pre-trained models to a local directory, if those weights don't already exist.\nTo decrease start-up time, we want this download to happen just once, even across separate function invocations.\nTo accomplish this, we use the [`Image.run_function`](/docs/reference/modal.Image#run_function) method, which allows\nus to run some code at image build time to save the model weights into the image.\n```python\nCACHE_PATH = \"/root/model_cache\"\nMODEL_NAME = \"naver-clova-ix/donut-base-finetuned-cord-v2\"\ndef download_model_weights() -> None:\n    from huggingface_hub import snapshot_download\n    snapshot_download(repo_id=MODEL_NAME, cache_dir=CACHE_PATH)\nimage = (\n    modal.Image.debian_slim()\n    .pip_install(\n        \"donut-python==1.0.7\",\n        \"huggingface-hub==0.16.4\",\n        \"transformers==4.21.3\",\n        \"timm==0.5.4\",\n    )\n    .run_function(download_model_weights)\n)\n```\n## Handler function\nNow let's define our handler function. Using the [@stub.function()](https://modal.com/docs/reference/modal.Stub#function)\ndecorator, we set up a Modal [Function](/docs/reference/modal.Function) that uses GPUs,\nruns on a [custom container image](/docs/guide/custom-container),\nand automatically [retries](/docs/guide/retries#function-retries) failures up to 3 times.\n```python\n@stub.function(\n    gpu=\"any\",\n    image=image,\n    retries=3,\n)\ndef parse_receipt(image: bytes):\n    import io\n    import torch\n    from donut import DonutModel\n    from PIL import Image\n    # Use donut fine-tuned on an OCR dataset.\n    task_prompt = \"<s_cord-v2>\"\n    pretrained_model = DonutModel.from_pretrained(\n        MODEL_NAME,\n        cache_dir=CACHE_PATH,\n    )\n    # Initialize model.\n    pretrained_model.half()\n    device = torch.device(\"cuda\")\n    pretrained_model.to(device)\n    # Run inference.\n    input_img = Image.open(io.BytesIO(image))\n    output = pretrained_model.inference(image=input_img, prompt=task_prompt)[\n        \"predictions\"\n    ][0]\n    print(\"Result: \", output)\n    return output\n```\n## Deploy\nNow that we have a function, we can publish it by deploying the app:\n```shell\nmodal deploy doc_ocr_jobs.py\n```\nOnce it's published, we can [look up](/docs/guide/trigger-deployed-functions) this function from another\nPython process and submit tasks to it:\n```python\nfn = modal.Function.lookup(\"example-doc-ocr-jobs\", \"parse_receipt\")\nfn.spawn(my_image)\n```\nModal will auto-scale to handle all the tasks queued, and\nthen scale back down to 0 when there's no work left. To see how you could use this from a Python web\napp, take a look at the [receipt parser frontend](/docs/guide/ex/doc_ocr_webapp)\ntutorial.\n## Run manually\nWe can also trigger `parse_receipt` manually for easier debugging:\n`modal run doc_ocr_jobs::stub.main`\nTo try it out, you can find some\nexample receipts [here](https://drive.google.com/drive/folders/1S2D1gXd4YIft4a5wDtW99jfl38e85ouW).\n```python\n@stub.local_entrypoint()\ndef main():\n    from pathlib import Path\n    receipt_filename = Path(__file__).parent / \"receipt.png\"\n    if receipt_filename.exists():\n        with open(receipt_filename, \"rb\") as f:\n            image = f.read()\n    else:\n        image = urllib.request.urlopen(\n            \"https://nwlc.org/wp-content/uploads/2022/01/Brandys-walmart-receipt-8.webp\"\n        ).read()\n    print(parse_receipt.remote(image))\n```\n"}
{"text": "# Stable Diffusion with ONNX Runtime\nThis example is similar to the [Stable Diffusion CLI](/docs/guide/ex/stable_diffusion_cli)\nexample, but it runs inference unsing [ONNX Runtime](https://onnxruntime.ai/) instead of PyTorch.\n## Basic setup\n```python\nimport io\nimport time\nfrom pathlib import Path\nfrom modal import Image, Stub, method\n```\nCreate a Stub representing a Modal app.\n```python\nstub = Stub(\"stable-diffusion-onnx\")\n```\n## Model dependencies\nWe will install `optimum` and the ONNX runtime GPU package.\n```python\nmodel_id = \"runwayml/stable-diffusion-v1-5\"\ncache_path = \"/vol/cache\"\ndef download_models():\n    from optimum.onnxruntime import ORTStableDiffusionPipeline\n    pipe = ORTStableDiffusionPipeline.from_pretrained(\n        model_id, revision=\"fp16\", export=True\n    )\n    pipe.save_pretrained(cache_path, safe_serialization=True)\nstub.image = (\n    Image.debian_slim(python_version=\"3.11\")\n    .pip_install(\"diffusers~=0.19.1\", \"optimum[onnxruntime-gpu]~=1.10.1\")\n    .run_function(download_models)\n)\n```\n## Load model and run inference\nThe container lifecycle [`__enter__` function](https://modal.com/docs/guide/lifecycle-functions#container-lifecycle-beta)\nloads the model at startup. Then, we evaluate it in the `run_inference` function.\n```python\n@stub.cls(gpu=\"A10G\")\nclass StableDiffusion:\n    def __enter__(self):\n        from optimum.onnxruntime import ORTStableDiffusionPipeline\n        self.pipe = ORTStableDiffusionPipeline.from_pretrained(\n            cache_path,\n            revision=\"fp16\",\n            provider=\"CUDAExecutionProvider\",\n            device_map=\"auto\",\n        )\n    @method()\n    def run_inference(\n        self, prompt: str, steps: int, batch_size: int\n    ) -> list[bytes]:\n        # Run pipeline\n        images = self.pipe(\n            [prompt] * batch_size,\n            num_inference_steps=steps,\n            guidance_scale=7.0,\n        ).images\n        # Convert to PNG bytes\n        image_output = []\n        for image in images:\n            with io.BytesIO() as buf:\n                image.save(buf, format=\"PNG\")\n                image_output.append(buf.getvalue())\n        return image_output\n```\nCall this script with the Modal CLI: `modal run stable_diffusion_cli.py --prompt \"a photo of a castle floating on clouds\"`.\n```python\n@stub.local_entrypoint()\ndef entrypoint(\n    prompt: str = \"martha stewart at burning man\",\n    samples: int = 3,\n    steps: int = 20,\n    batch_size: int = 3,\n):\n    print(\n        f\"prompt => {prompt}, steps => {steps}, samples => {samples}, batch_size => {batch_size}\"\n    )\n    dir = Path(\"/tmp/stable-diffusion\")\n    if not dir.exists():\n        dir.mkdir(exist_ok=True, parents=True)\n    sd = StableDiffusion()\n    for i in range(samples):\n        t0 = time.time()\n        images = sd.run_inference.remote(prompt, steps, batch_size)\n        total_time = time.time() - t0\n        print(\n            f\"Sample {i} took {total_time:.3f}s ({(total_time)/len(images):.3f}s / image).\"\n        )\n        for j, image_bytes in enumerate(images):\n            output_path = dir / f\"output_{i}_{j}.png\"\n            print(f\"Saving it to {output_path}\")\n            with open(output_path, \"wb\") as f:\n                f.write(image_bytes)\n```\n"}
{"text": "# Hosting any LLaMA 2 model with Text Generation Inference (TGI)\nIn this example, we show how to run an optimized inference server using [Text Generation Inference (TGI)](https://github.com/huggingface/text-generation-inference)\nwith performance advantages over standard text generation pipelines including:\n- continuous batching, so multiple generations can take place at the same time on a single container\n- PagedAttention, an optimization that increases throughput.\nThis example deployment, [accessible here](https://modal-labs--tgi-app.modal.run), can serve LLaMA 2 70B with\n70 second cold starts, up to 200 tokens/s of throughput and per-token latency of 55ms.\n## Setup\nFirst we import the components we need from `modal`.\n```python\nfrom pathlib import Path\nfrom modal import Image, Mount, Secret, Stub, asgi_app, gpu, method\n```\nNext, we set which model to serve, taking care to specify the GPU configuration required\nto fit the model into VRAM, and the quantization method (`bitsandbytes` or `gptq`) if desired.\nNote that quantization does degrade token generation performance significantly.\nAny model supported by TGI can be chosen here.\n```python\nGPU_CONFIG = gpu.A100(memory=80, count=2)\nMODEL_ID = \"meta-llama/Llama-2-70b-chat-hf\"\n```\nAdd `[\"--quantize\", \"gptq\"]` for TheBloke GPTQ models.\n```python\nLAUNCH_FLAGS = [\"--model-id\", MODEL_ID]\n```\n## Define a container image\nWe want to create a Modal image which has the Huggingface model cache pre-populated.\nThe benefit of this is that the container no longer has to re-download the model from Huggingface -\ninstead, it will take advantage of Modal's internal filesystem for faster cold starts. On\nthe largest 70B model, the 135GB model can be loaded in as little as 70 seconds.\n### Download the weights\nWe can use the included utilities to download the model weights (and convert to safetensors, if necessary)\nas part of the image build.\n```python\ndef download_model():\n    import subprocess\n    subprocess.run([\"text-generation-server\", \"download-weights\", MODEL_ID])\n```\n### Image definition\nWe\u2019ll start from a Dockerhub image recommended by TGI, and override the default `ENTRYPOINT` for\nModal to run its own which enables seamless serverless deployments.\nNext we run the download step to pre-populate the image with our model weights.\nFor this step to work on a gated model such as LLaMA 2, the HUGGING_FACE_HUB_TOKEN environment\nvariable must be set ([reference](https://github.com/huggingface/text-generation-inference#using-a-private-or-gated-model)).\nAfter [creating a HuggingFace access token](https://huggingface.co/settings/tokens),\nhead to the [secrets page](https://modal.com/secrets) to create a Modal secret.\nThe key should be `HUGGING_FACE_HUB_TOKEN` and the value should be your access token.\nFinally, we install the `text-generation` client to interface with TGI's Rust webserver over `localhost`.\n```python\nimage = (\n    Image.from_registry(\"ghcr.io/huggingface/text-generation-inference:1.0.3\")\n    .dockerfile_commands(\"ENTRYPOINT []\")\n    .run_function(download_model, secret=Secret.from_name(\"huggingface\"))\n    .pip_install(\"text-generation\")\n)\nstub = Stub(\"example-tgi-\" + MODEL_ID.split(\"/\")[-1], image=image)\n```\n## The model class\nThe inference function is best represented with Modal's [class syntax](/docs/guide/lifecycle-functions).\nThe class syntax is a special representation for a Modal function which splits logic into two parts:\n1. the `__enter__` method, which runs once per container when it starts up, and\n2. the `@method()` function, which runs per inference request.\nThis means the model is loaded into the GPUs, and the backend for TGI is launched just once when each\ncontainer starts, and this state is cached for each subsequent invocation of the function.\nNote that on start-up, we must wait for the Rust webserver to accept connections before considering the\ncontainer ready.\nHere, we also\n- specify the secret so the `HUGGING_FACE_HUB_TOKEN` environment variable is set\n- specify how many A100s we need per container\n- specify that each container is allowed to handle up to 10 inputs (i.e. requests) simultaneously\n- keep idle containers for 10 minutes before spinning down\n- lift the timeout of each request.\n```python\n@stub.cls(\n    secret=Secret.from_name(\"huggingface\"),\n    gpu=GPU_CONFIG,\n    allow_concurrent_inputs=10,\n    container_idle_timeout=60 * 10,\n    timeout=60 * 60,\n)\nclass Model:\n    def __enter__(self):\n        import socket\n        import subprocess\n        import time\n        from text_generation import AsyncClient\n        self.launcher = subprocess.Popen(\n            [\"text-generation-launcher\"] + LAUNCH_FLAGS\n        )\n        self.client = AsyncClient(\"http://0.0.0.0:80\", timeout=60)\n        self.template = \"\"\"<s>[INST] <<SYS>>\n{system}\n<</SYS>>\n{user} [/INST] \"\"\"\n        # Poll until webserver at 0.0.0.0:80 accepts connections before running inputs.\n        def webserver_ready():\n            try:\n                socket.create_connection((\"0.0.0.0\", 80), timeout=1).close()\n                return True\n            except (socket.timeout, ConnectionRefusedError):\n                return False\n        while not webserver_ready():\n            time.sleep(1.0)\n        print(\"Webserver ready!\")\n    def __exit__(self, _exc_type, _exc_value, _traceback):\n        self.launcher.terminate()\n    @method()\n    async def generate(self, question: str):\n        prompt = self.template.format(system=\"\", user=question)\n        result = await self.client.generate(prompt, max_new_tokens=1024)\n        return result.generated_text\n    @method()\n    async def generate_stream(self, question: str):\n        prompt = self.template.format(system=\"\", user=question)\n        async for response in self.client.generate_stream(\n            prompt, max_new_tokens=1024\n        ):\n            if not response.token.special:\n                yield response.token.text\n```\n## Run the model\nWe define a [`local_entrypoint`](/docs/guide/apps#entrypoints-for-ephemeral-apps) to invoke\nour remote function. You can run this script locally with `modal run text_generation_inference.py`.\n```python\n@stub.local_entrypoint()\ndef main():\n    print(\n        Model().generate.remote(\n            \"Implement a Python function to compute the Fibonacci numbers.\"\n        )\n    )\n```\n## Serve the model\nOnce we deploy this model with `modal deploy text_generation_inference.py`, we can serve it\nbehind an ASGI app front-end. The front-end code (a single file of Alpine.js) is available\n[here](https://github.com/modal-labs/modal-examples/blob/main/06_gpu_and_ml/llm-frontend/index.html).\nYou can try our deployment [here](https://modal-labs--tgi-app.modal.run).\n```python\nfrontend_path = Path(__file__).parent / \"llm-frontend\"\n@stub.function(\n    mounts=[Mount.from_local_dir(frontend_path, remote_path=\"/assets\")],\n    keep_warm=1,\n    allow_concurrent_inputs=10,\n    timeout=60 * 10,\n)\n@asgi_app(label=\"tgi-app\")\ndef app():\n    import json\n    import fastapi\n    import fastapi.staticfiles\n    from fastapi.responses import StreamingResponse\n    web_app = fastapi.FastAPI()\n    @web_app.get(\"/stats\")\n    async def stats():\n        stats = await Model().generate_stream.get_current_stats.aio()\n        return {\n            \"backlog\": stats.backlog,\n            \"num_total_runners\": stats.num_total_runners,\n        }\n    @web_app.get(\"/completion/{question}\")\n    async def completion(question: str):\n        from urllib.parse import unquote\n        async def generate():\n            async for text in Model().generate_stream.remote_gen.aio(\n                unquote(question)\n            ):\n                yield f\"data: {json.dumps(dict(text=text), ensure_ascii=False)}\\n\\n\"\n        return StreamingResponse(generate(), media_type=\"text/event-stream\")\n    web_app.mount(\n        \"/\", fastapi.staticfiles.StaticFiles(directory=\"/assets\", html=True)\n    )\n    return web_app\n```\n## Invoke the model from other apps\nOnce the model is deployed, we can invoke inference from other apps, sharing the same pool\nof GPU containers with all other apps we might need.\n```\n$ python\n>>> import modal\n>>> f = modal.Function.lookup(\"example-tgi-Llama-2-70b-chat-hf\", \"Model.generate\")\n>>> f.remote(\"What is the story about the fox and grapes?\")\n'The story about the fox and grapes ...\n```\n"}
{"text": "# News article summarizer\nIn this example we scrape news articles from the [New York Times'\nScience section](https://www.nytimes.com/section/science) and summarize them\nusing Google's deep learning summarization model [Pegasus](https://ai.googleblog.com/2020/06/pegasus-state-of-art-model-for.html).\nWe log the resulting summaries to the terminal, but you can do whatever you want with the\nsummaries afterwards: saving to a CSV file, sending to Slack, etc.\n```python\nimport os\nimport re\nfrom dataclasses import dataclass\nfrom typing import List\nimport modal\nstub = modal.Stub(name=\"example-news-summarizer\")\n```\n## Building Images and Downloading Pre-trained Model\nWe start by defining our images. In Modal, each function can use a different\nimage. This is powerful because you add only the dependencies you need for\neach function.\nThe first image contains dependencies for running our model. We also download the\npre-trained model into the image using the `from_pretrained` method.\nThis caches the model so that we don't have to download it on every function call.\nThe model will be saved at `/cache` when this function is called at image build time;\nsubsequent calls of this function at runtime will then load the model from `/cache`.\n```python\ndef fetch_model(local_files_only: bool = False):\n    from transformers import PegasusForConditionalGeneration, PegasusTokenizer\n    tokenizer = PegasusTokenizer.from_pretrained(\n        \"google/pegasus-xsum\",\n        cache_dir=\"/cache\",\n        local_files_only=local_files_only,\n    )\n    model = PegasusForConditionalGeneration.from_pretrained(\n        \"google/pegasus-xsum\",\n        cache_dir=\"/cache\",\n        local_files_only=local_files_only,\n    )\n    return model, tokenizer\nstub[\"deep_learning_image\"] = (\n    modal.Image.debian_slim()\n    .pip_install(\"transformers==4.16.2\", \"torch\", \"sentencepiece\")\n    .run_function(fetch_model)\n)\n```\nDefining the scraping image is very similar. This image only contains the packages required\nto scrape the New York Times website, though; so it's much smaller.\n```python\nstub[\"scraping_image\"] = modal.Image.debian_slim().pip_install(\n    \"requests\", \"beautifulsoup4\", \"lxml\"\n)\nif stub.is_inside(stub[\"scraping_image\"]):\n    import requests\n    from bs4 import BeautifulSoup\n```\n## Collect Data\nCollecting data happens in two stages: first a list of URL articles\nusing the NYT API then scrape the NYT web page for each of those articles\nto collect article texts.\n```python\n@dataclass\nclass NYArticle:\n    title: str\n    image_url: str = \"\"\n    url: str = \"\"\n    summary: str = \"\"\n    text: str = \"\"\n```\nIn order to connect to the NYT API, you will need to sign up at [NYT Developer Portal](https://developer.nytimes.com/),\ncreate an Stub then grab an API key. Then head to Modal and create a [Secret](https://modal.com/docs/guide/secrets) called `nytimes`.\nCreate an environment variable called `NYTIMES_API_KEY` with your API key.\n```python\n@stub.function(\n    secret=modal.Secret.from_name(\"nytimes\"), image=stub[\"scraping_image\"]\n)\ndef latest_science_stories(n_stories: int = 5) -> List[NYArticle]:\n    # query api for latest science articles\n    params = {\n        \"api-key\": os.environ[\"NYTIMES_API_KEY\"],\n    }\n    nyt_api_url = \"https://api.nytimes.com/svc/topstories/v2/science.json\"\n    response = requests.get(nyt_api_url, params=params)\n    # extract data from articles and return list of NYArticle objects\n    results = response.json()\n    reject_urls = {\"null\", \"\", None}\n    articles = [\n        NYArticle(\n            title=u[\"title\"],\n            image_url=u.get(\"multimedia\")[0][\"url\"]\n            if u.get(\"multimedia\")\n            else \"\",\n            url=u.get(\"url\"),\n        )\n        for u in results[\"results\"]\n        if u.get(\"url\") not in reject_urls\n    ]\n    # select only a handful of articles; this usually returns 25 articles\n    articles = articles[:n_stories]\n    print(f\"Retrieved {len(articles)} from the NYT Top Stories API\")\n    return articles\n```\nThe NYT API only gives us article URLs but it doesn't include the article text. We'll get the article URLs\nfrom the API then scrape each URL for the article body. We'll be using\n[Beautiful Soup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) for that.\n```python\n@stub.function(image=stub[\"scraping_image\"])\ndef scrape_nyc_article(url: str) -> str:\n    print(f\"Scraping article => {url}\")\n    # fetch article; simulate desktop browser\n    headers = {\n        \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_2) AppleWebKit/601.3.9 (KHTML, like Gecko) Version/9.0.2 Safari/601.3.9\"\n    }\n    response = requests.get(url, headers=headers)\n    soup = BeautifulSoup(response.text, \"lxml\")\n    # get all text paragraphs & construct single string with article text\n    article_text = \"\"\n    article_section = soup.find_all(\n        \"div\", {\"class\": re.compile(r\"\\bStoryBodyCompanionColumn\\b\")}\n    )\n    if article_section:\n        paragraph_tags = article_section[0].find_all(\"p\")\n        article_text = \" \".join([p.get_text() for p in paragraph_tags])\n    # return article with scraped text\n    return article_text\n```\nNow the summarization function. We use `huggingface`'s Pegasus tokenizer and model implementation to\ngenerate a summary of the model. You can learn more about Pegasus does in the [HuggingFace\ndocumentation](https://huggingface.co/docs/transformers/model_doc/pegasus). Use `gpu=\"any\"` to speed-up inference.\n```python\n@stub.function(\n    image=stub[\"deep_learning_image\"],\n    gpu=False,\n    memory=4096,\n)\ndef summarize_article(text: str) -> str:\n    print(f\"Summarizing text with {len(text)} characters.\")\n    # `local_files_only` is set to `True` because we expect to read the model\n    # files saved in the image.\n    model, tokenizer = fetch_model(local_files_only=True)\n    # summarize text\n    batch = tokenizer(\n        [text], truncation=True, padding=\"longest\", return_tensors=\"pt\"\n    ).to(\"cpu\")\n    translated = model.generate(**batch)\n    summary = tokenizer.batch_decode(translated, skip_special_tokens=True)[0]\n    return summary\n```\n## Create a Scheduled Function\nPut everything together and schedule it to run every day. You can also use `modal.Cron` for a\nmore advanced scheduling interface.\n```python\n@stub.function(schedule=modal.Period(days=1))\ndef trigger():\n    articles = latest_science_stories.remote()\n    # parallelize article scraping\n    for i, text in enumerate(scrape_nyc_article.map([a.url for a in articles])):\n        articles[i].text = text\n    # parallelize summarization\n    for i, summary in enumerate(\n        summarize_article.map([a.text for a in articles if len(a.text) > 0])\n    ):\n        articles[i].summary = summary\n    # show all summaries in the terminal\n    for article in articles:\n        print(f'Summary of \"{article.title}\" => {article.summary}')\n```\nCreate a new Modal scheduled function with:\n```shell\nmodal deploy --name news_summarizer news_summarizer.py\n```\nYou can also run this entire Modal app in debugging mode before.\ncall it with `modal run news_summarizer.py`\n```python\n@stub.local_entrypoint()\ndef main():\n    trigger.remote()\n```\nAnd that's it. You will now generate deep learning summaries from the latest\nNYT Science articles every day.\n"}
{"text": "\n# Publish interactive datasets with Datasette\n![Datasette user interface](./covid_datasette_ui.png)\nThis example shows how to serve a Datasette application on Modal. The published dataset\nis COVID-19 case data from Johns Hopkins University which is refreshed daily.\nTry it out for yourself at [modal-labs-example-covid-datasette-app.modal.run/covid-19](https://modal-labs-example-covid-datasette-app.modal.run/covid-19/johns_hopkins_csse_daily_reports).\nSome Modal features it uses:\n* Network file systems: a persisted volume lets us store and grow the published dataset over time\n* Scheduled functions: the underlying dataset is refreshed daily, so we schedule a function to run daily\n* Webhooks: exposes the Datasette application for web browser interaction and API requests.\n## Basic setup\nLet's get started writing code.\nFor the Modal container image we need a few Python packages,\nincluding `GitPython`, which we'll use to download the dataset.\n```python\nimport asyncio\nimport pathlib\nimport shutil\nimport tempfile\nfrom datetime import datetime, timedelta\nfrom modal import Image, NetworkFileSystem, Period, Stub, asgi_app\nstub = Stub(\"example-covid-datasette\")\ndatasette_image = (\n    Image.debian_slim()\n    .pip_install(\n        \"datasette~=0.63.2\",\n        \"flufl.lock\",\n        \"GitPython\",\n        \"sqlite-utils\",\n    )\n    .apt_install(\"git\")\n)\n```\n## Persistent dataset storage\nTo separate database creation and maintenance from serving, we'll need the underlying\ndatabase file to be stored persistently. To achieve this we use a [`NetworkFileSystem`](/docs/guide/shared-volumes),\na writable volume that can be attached to Modal functions and persisted across function runs.\n```python\nvolume = NetworkFileSystem.persisted(\"covid-dataset-cache-vol\")\nCACHE_DIR = \"/cache\"\nLOCK_FILE = str(pathlib.Path(CACHE_DIR, \"lock-reports\"))\nREPO_DIR = pathlib.Path(CACHE_DIR, \"COVID-19\")\nDB_PATH = pathlib.Path(CACHE_DIR, \"covid-19.db\")\n```\n## Getting a dataset\nJohns Hopkins has been publishing up-to-date COVID-19 pandemic data on GitHub since early February 2020, and\nas of late September 2022 daily reporting is still rolling in. Their dataset is what this example will use to\nshow off Modal and Datasette's capabilities.\nThe full git repository size for the dataset is over 6GB, but we only need to shallow clone around 300MB.\n```python\n@stub.function(\n    image=datasette_image,\n    network_file_systems={CACHE_DIR: volume},\n    retries=2,\n)\ndef download_dataset(cache=True):\n    import git\n    from flufl.lock import Lock\n    if REPO_DIR.exists() and cache:\n        print(f\"Dataset already present and {cache=}. Skipping download.\")\n        return\n    elif REPO_DIR.exists():\n        print(\n            \"Acquiring lock before deleting dataset, which may be in use by other runs.\"\n        )\n        with Lock(LOCK_FILE, default_timeout=timedelta(hours=1)):\n            shutil.rmtree(REPO_DIR)\n        print(\"Cleaned dataset before re-downloading.\")\n    git_url = \"https://github.com/CSSEGISandData/COVID-19\"\n    git.Repo.clone_from(git_url, REPO_DIR, depth=1)\n```\n## Data munging\nThis dataset is no swamp, but a bit of data cleaning is still in order. The following two\nfunctions are used to read a handful of `.csv` files from the git repository and cleaning the\nrows data before inserting into SQLite. You can see that the daily reports are somewhat inconsistent\nin their column names.\n```python\ndef load_daily_reports():\n    jhu_csse_base = REPO_DIR\n    reports_path = (\n        jhu_csse_base / \"csse_covid_19_data\" / \"csse_covid_19_daily_reports\"\n    )\n    daily_reports = list(reports_path.glob(\"*.csv\"))\n    for filepath in daily_reports:\n        yield from load_report(filepath)\ndef load_report(filepath):\n    import csv\n    mm, dd, yyyy = filepath.stem.split(\"-\")\n    with filepath.open() as fp:\n        for row in csv.DictReader(fp):\n            province_or_state = (\n                row.get(\"\\ufeffProvince/State\")\n                or row.get(\"Province/State\")\n                or row.get(\"Province_State\")\n                or None\n            )\n            country_or_region = row.get(\"Country_Region\") or row.get(\n                \"Country/Region\"\n            )\n            yield {\n                \"day\": f\"{yyyy}-{mm}-{dd}\",\n                \"country_or_region\": country_or_region.strip()\n                if country_or_region\n                else None,\n                \"province_or_state\": province_or_state.strip()\n                if province_or_state\n                else None,\n                \"confirmed\": int(float(row[\"Confirmed\"] or 0)),\n                \"deaths\": int(float(row[\"Deaths\"] or 0)),\n                \"recovered\": int(float(row[\"Recovered\"] or 0)),\n                \"active\": int(row[\"Active\"]) if row.get(\"Active\") else None,\n                \"last_update\": row.get(\"Last Update\")\n                or row.get(\"Last_Update\")\n                or None,\n            }\n```\n## Inserting into SQLite\nWith the CSV processing out of the way, we're ready to create an SQLite DB and feed data into it.\nImportantly, the `prep_db` function mounts the same network file system used by `download_dataset()`, and\nrows are batch inserted with progress logged after each batch, as the full COVID-19 has millions\nof rows and does take some time to be fully inserted.\nA more sophisticated implementation would only load new data instead of performing a full refresh,\nbut for this example things are kept simple.\n```python\ndef chunks(it, size):\n    import itertools\n    return iter(lambda: tuple(itertools.islice(it, size)), ())\n@stub.function(\n    image=datasette_image,\n    network_file_systems={CACHE_DIR: volume},\n    timeout=900,\n)\ndef prep_db():\n    import sqlite_utils\n    from flufl.lock import Lock\n    print(\"Loading daily reports...\")\n    records = load_daily_reports()\n    with Lock(\n        LOCK_FILE,\n        lifetime=timedelta(minutes=2),\n        default_timeout=timedelta(hours=1),\n    ) as lck, tempfile.NamedTemporaryFile() as tmp:\n        db = sqlite_utils.Database(tmp.name)\n        table = db[\"johns_hopkins_csse_daily_reports\"]\n        batch_size = 100_000\n        for i, batch in enumerate(chunks(records, size=batch_size)):\n            truncate = True if i == 0 else False\n            table.insert_all(batch, batch_size=batch_size, truncate=truncate)\n            lck.refresh()\n            print(f\"Inserted {len(batch)} rows into DB.\")\n        table.create_index([\"day\"], if_not_exists=True)\n        table.create_index([\"province_or_state\"], if_not_exists=True)\n        table.create_index([\"country_or_region\"], if_not_exists=True)\n        print(\"Syncing DB with network volume.\")\n        DB_PATH.parent.mkdir(parents=True, exist_ok=True)\n        shutil.copyfile(tmp.name, DB_PATH)\n```\n## Keeping fresh\nJohns Hopkins commits new data to the dataset repository every day, so we\nsetup a [scheduled](/docs/guide/cron) Modal function to run automatically once every 24 hours.\n```python\n@stub.function(schedule=Period(hours=24), timeout=1000)\ndef refresh_db():\n    print(f\"Running scheduled refresh at {datetime.now()}\")\n    download_dataset.remote(cache=False)\n    prep_db.remote()\n```\n## Webhook\nHooking up the SQLite database to a Modal webhook is as simple as it gets.\nThe Modal `@stub.asgi_app` decorator wraps a few lines of code: one `import` and a few\nlines to instantiate the `Datasette` instance and return a reference to its ASGI app object.\n```python\n@stub.function(\n    image=datasette_image,\n    network_file_systems={CACHE_DIR: volume},\n)\n@asgi_app()\ndef app():\n    from datasette.app import Datasette\n    ds = Datasette(files=[DB_PATH])\n    asyncio.run(ds.invoke_startup())\n    return ds.app()\n```\n## Publishing to the web\nRun this script using `modal run covid_datasette.py` and it will create the database.\nYou can run this script using `modal serve covid_datasette.py` and it will create a\nshort-lived web URL that exists until you terminate the script.\nWhen publishing the interactive Datasette app you'll want to create a persistent URL.\nThis is achieved by deploying the script with `modal deploy covid_datasette.py`.\n```python\n@stub.local_entrypoint()\ndef run():\n    print(\"Downloading COVID-19 dataset...\")\n    download_dataset.remote()\n    print(\"Prepping SQLite DB...\")\n    prep_db.remote()\n```\nYou can go explore the data over at [modal-labs-covid-datasette-app.modal.run/covid-19/](https://modal-labs-example-covid-datasette-app.modal.run/covid-19/johns_hopkins_csse_daily_reports).\n"}
{"text": "\n# Run Falcon-40B with bitsandbytes\nIn this example, we download the full-precision weights of the Falcon-40B LLM but load it in 4-bit using\nTim Dettmer's [`bitsandbytes`](https://github.com/TimDettmers/bitsandbytes) library. This enables it to fit\ninto a single GPU (A100 40GB).\nDue to the current limitations of the library, the inference speed is a little over 2 tokens/second and due\nto the sheer size of the model, the cold start time on Modal is around 2 minutes.\nFor faster cold start at the expense of inference speed, check out\n[Running Falcon-40B with AutoGPTQ](/docs/guide/ex/falcon_gptq).\n## Setup\nFirst we import the components we need from `modal`.\n```python\nfrom modal import Image, Stub, gpu, method, web_endpoint\n```\nSpec for an image where falcon-40b-instruct is cached locally\n```python\ndef download_falcon_40b():\n    from huggingface_hub import snapshot_download\n    model_name = \"tiiuae/falcon-40b-instruct\"\n    snapshot_download(model_name)\nimage = (\n    Image.micromamba()\n    .micromamba_install(\n        \"cudatoolkit=11.7\",\n        \"cudnn=8.1.0\",\n        \"cuda-nvcc\",\n        \"scipy\",\n        channels=[\"conda-forge\", \"nvidia\"],\n    )\n    .apt_install(\"git\")\n    .pip_install(\n        \"bitsandbytes==0.39.0\",\n        \"bitsandbytes-cuda117==0.26.0.post2\",\n        \"peft @ git+https://github.com/huggingface/peft.git\",\n        \"transformers @ git+https://github.com/huggingface/transformers.git\",\n        \"accelerate @ git+https://github.com/huggingface/accelerate.git\",\n        \"hf-transfer~=0.1\",\n        \"torch==2.0.0\",\n        \"torchvision==0.15.1\",\n        \"sentencepiece==0.1.97\",\n        \"huggingface_hub==0.14.1\",\n        \"einops==0.6.1\",\n    )\n    # Use huggingface's hi-perf hf-transfer library to download this large model.\n    .env({\"HF_HUB_ENABLE_HF_TRANSFER\": \"1\"})\n    .run_function(download_falcon_40b)\n)\nstub = Stub(image=image, name=\"example-falcon-bnb\")\n```\n## The model class\nNext, we write the model code. We want Modal to load the model into memory just once every time a container starts up,\nso we use [class syntax](/docs/guide/lifecycle-functions) and the __enter__` method.\nWithin the [@stub.cls](/docs/reference/modal.Stub#cls) decorator, we use the [gpu parameter](/docs/guide/gpu)\nto specify that we want to run our function on an [A100 GPU](/pricing). We also allow each call 10 mintues to complete,\nand request the runner to stay live for 5 minutes after its last request.\nWe load the model in 4-bit using the `bitsandbytes` library.\nThe rest is just using the [pipeline()](https://huggingface.co/docs/transformers/en/main_classes/pipelines)\nabstraction from the `transformers` library. Refer to the documentation for more parameters and tuning.\n```python\n@stub.cls(\n    gpu=gpu.A100(),  # Use A100s\n    timeout=60 * 10,  # 10 minute timeout on inputs\n    container_idle_timeout=60 * 5,  # Keep runner alive for 5 minutes\n)\nclass Falcon40B_4bit:\n    def __enter__(self):\n        import torch\n        from transformers import (\n            AutoModelForCausalLM,\n            AutoTokenizer,\n            BitsAndBytesConfig,\n        )\n        model_name = \"tiiuae/falcon-40b-instruct\"\n        nf4_config = BitsAndBytesConfig(\n            load_in_4bit=True,\n            bnb_4bit_quant_type=\"nf4\",\n            bnb_4bit_use_double_quant=False,\n            bnb_4bit_compute_dtype=torch.bfloat16,\n        )\n        model = AutoModelForCausalLM.from_pretrained(\n            model_name,\n            trust_remote_code=True,\n            local_files_only=True,  # Model is downloaded to cache dir\n            device_map=\"auto\",\n            quantization_config=nf4_config,\n        )\n        model.eval()\n        tokenizer = AutoTokenizer.from_pretrained(\n            model_name,\n            trust_remote_code=True,\n            local_files_only=True,\n            device_map=\"auto\",\n        )\n        tokenizer.bos_token_id = 1\n        self.model = torch.compile(model)\n        self.tokenizer = tokenizer\n    @method()\n    def generate(self, prompt: str):\n        from threading import Thread\n        from transformers import GenerationConfig, TextIteratorStreamer\n        tokenized = self.tokenizer(prompt, return_tensors=\"pt\")\n        input_ids = tokenized.input_ids\n        input_ids = input_ids.to(self.model.device)\n        generation_config = GenerationConfig(\n            do_sample=True,\n            temperature=0.1,\n            max_new_tokens=512,\n        )\n        streamer = TextIteratorStreamer(\n            self.tokenizer, skip_special_tokens=True\n        )\n        generate_kwargs = dict(\n            input_ids=input_ids,\n            generation_config=generation_config,\n            return_dict_in_generate=True,\n            eos_token_id=self.tokenizer.eos_token_id,\n            pad_token_id=self.tokenizer.eos_token_id,\n            bos_token_id=self.tokenizer.bos_token_id,\n            attention_mask=tokenized.attention_mask,\n            output_scores=True,\n            streamer=streamer,\n        )\n        thread = Thread(target=self.model.generate, kwargs=generate_kwargs)\n        thread.start()\n        for new_text in streamer:\n            print(new_text, end=\"\")\n            yield new_text\n        thread.join()\n```\n## Run the model\nWe define a [`local_entrypoint`](/docs/guide/apps#entrypoints-for-ephemeral-apps) to call our remote function\nsequentially for a list of inputs. You can run this locally with `modal run -q falcon_bitsandbytes.py`. The `-q` flag\nenables streaming to work in the terminal output.\n```python\nprompt_template = (\n    \"A chat between a curious human user and an artificial intelligence assistant. The assistant give a helpful, detailed, and accurate answer to the user's question.\"\n    \"\\n\\nUser:\\n{}\\n\\nAssistant:\\n\"\n)\n@stub.local_entrypoint()\ndef cli(prompt: str = None):\n    question = (\n        prompt\n        or \"What are the main differences between Python and JavaScript programming languages?\"\n    )\n    model = Falcon40B_4bit()\n    for text in model.generate.remote_gen(prompt_template.format(question)):\n        print(text, end=\"\", flush=True)\n```\n## Serve the model\nFinally, we can serve the model from a web endpoint with `modal deploy falcon_bitsandbytes.py`. If\nyou visit the resulting URL with a question parameter in your URL, you can view the model's\nstream back a response.\nYou can try our deployment [here](https://modal-labs--example-falcon-bnb-get.modal.run/?question=How%20do%20planes%20work?).\n```python\n@stub.function(timeout=60 * 10)\n@web_endpoint()\ndef get(question: str):\n    from itertools import chain\n    from fastapi.responses import StreamingResponse\n    model = Falcon40B_4bit()\n    return StreamingResponse(\n        chain(\n            (\"Loading model (100GB). This usually takes around 110s ...\\n\\n\"),\n            model.generate.remote(prompt_template.format(question)),\n        ),\n        media_type=\"text/event-stream\",\n    )\n```\n"}
{"text": "# Use DuckDB to analyze lots of datasets in parallel\nThe Taxi and Limousine Commission of NYC posts\n[datasets](https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page)\nwith all trips in New York City.\nThey are all Parquet files, which are very well suited for\n[DuckDB](https://duckdb.org/) which has excellent\n[Parquet support](https://duckdb.org/docs/data/parquet).\nIn fact, DuckDB lets us query remote Parquet data\n[over HTTP](https://duckdb.org/docs/guides/import/http_import)\nwhich is excellent for what we want to do here.\nRunning this script should generate a plot like this in just 10-20 seconds,\nprocessing a few gigabytes of data:\n![nyc taxi chart](./nyc_taxi_chart.png)\n## Basic setup\nWe need various imports and to define an image with DuckDB installed:\n```python\nimport io\nimport os\nfrom datetime import datetime\nimport modal\nstub = modal.Stub(\n    \"example-duckdb-nyc-taxi\",\n    image=modal.Image.debian_slim().pip_install(\"matplotlib\", \"duckdb\"),\n)\n```\n## DuckDB Modal function\nDefining the function that queries the data.\nThis lets us run a SQL query against a remote Parquet file over HTTP\nOur query is pretty simple: it just aggregates total count numbers by date,\nbut we also have some filters that remove garbage data (days that are outside\nthe range).\n```python\n@stub.function()\ndef get_data(year, month):\n    import duckdb\n    url = f\"https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_{year:04d}-{month:02d}.parquet\"\n    print(\"processing\", url, \"...\")\n    con = duckdb.connect(database=\":memory:\")\n    con.execute(\"install httpfs\")  # TODO: bake into the image\n    con.execute(\"load httpfs\")\n    q = \"\"\"\n    with sub as (\n        select tpep_pickup_datetime::date d, count(1) c\n        from read_parquet(?)\n        group by 1\n    )\n    select d, c from sub\n    where date_part('year', d) = ?  -- filter out garbage\n    and date_part('month', d) = ?   -- same\n    \"\"\"\n    con.execute(q, (url, year, month))\n    return list(con.fetchall())\n```\n## Plot results\nLet's define a separate function which:\n1. Parallelizes over all files and dispatches calls to the previous function\n2. Aggregate the data and plot the result\n```python\n@stub.function()\ndef create_plot():\n    from matplotlib import pyplot\n    # Map over all inputs and combine the data\n    inputs = [\n        (year, month)\n        for year in range(2018, 2023)\n        for month in range(1, 13)\n        if (year, month) <= (2022, 6)\n    ]\n    data: list[list[tuple[datetime, int]]] = [\n        [] for i in range(7)\n    ]  # Initialize a list for every weekday\n    for r in get_data.starmap(inputs):\n        for d, c in r:\n            data[d.weekday()].append((d, c))\n    # Initialize plotting\n    pyplot.style.use(\"ggplot\")\n    pyplot.figure(figsize=(16, 9))\n    # For each weekday, plot\n    for i, weekday in enumerate(\n        [\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\"]\n    ):\n        data[i].sort()\n        dates = [d for d, _ in data[i]]\n        counts = [c for _, c in data[i]]\n        pyplot.plot(dates, counts, linewidth=3, alpha=0.8, label=weekday)\n    # Plot annotations\n    pyplot.title(\"Number of NYC yellow taxi trips by weekday, 2018-2022\")\n    pyplot.ylabel(\"Number of daily trips\")\n    pyplot.legend()\n    pyplot.tight_layout()\n    # Dump PNG and return\n    with io.BytesIO() as buf:\n        pyplot.savefig(buf, format=\"png\", dpi=300)\n        return buf.getvalue()\n```\n## Entrypoint\nFinally, we have some simple entrypoint code that kicks everything off.\nNote that the plotting function returns raw PNG data that we store locally.\nRun this local entrypoint with `modal run`.\n```python\n@stub.local_entrypoint()\ndef main():\n    output_dir = \"/tmp/nyc\"\n    os.makedirs(output_dir, exist_ok=True)\n    fn = os.path.join(output_dir, \"nyc_taxi_chart.png\")\n    png_data = create_plot.remote()\n    with open(fn, \"wb\") as f:\n        f.write(png_data)\n    print(f\"wrote output to {fn}\")\n```\n"}
{"text": "\n# Llama 2 inference with MLC\n[Machine Learning Compilation (MLC)](https://mlc.ai/mlc-llm/) is high-performance tool for serving\nLLMs including Llama 2. We will use the [`mlc_chat`](https://mlc.ai/mlc-llm/docs/index.html) package\nand the pre-compiled Llama 2 binaries to run inference using a Modal GPU.\nThis example is adapted from this [MLC chat collab](https://colab.research.google.com/github/mlc-ai/notebooks/blob/main/mlc-llm/tutorial_chat_module_getting_started.ipynb#scrollTo=yYwjsCOK7Jij).\n```python\nimport queue\nimport threading\nimport time\nfrom typing import Dict, Generator, List\nimport modal\n```\n## Imports and global settings\nDetermine which [GPU](https://modal.com/docs/guide/gpu#gpu-acceleration) you want to use.\n```python\nGPU: str = \"a10g\"\n```\nChose model size. At the time of writing MLC chat only\nprovides compiled binaries for Llama 7b and 13b.\n```python\nLLAMA_MODEL_SIZE: str = \"13b\"\n```\nDefine the image and [Modal Stub](https://modal.com/docs/reference/modal.Stub#modalstub).\nWe use an [official NVIDIA CUDA 12.1 image](https://hub.docker.com/r/nvidia/cuda)\nto match MLC CUDA requirements.\n```python\nimage = (\n    modal.Image.from_registry(\n        \"nvidia/cuda:12.1.0-cudnn8-runtime-ubuntu22.04\",\n        setup_dockerfile_commands=[\n            \"RUN apt-get update\",\n            \"RUN apt-get install -y python3 python3-pip python-is-python3 git curl\",\n            # Install git lfs\n            \"RUN curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.deb.sh | bash\",\n            \"RUN apt install git-lfs -y\",\n        ],\n    ).run_commands(\n        \"pip3 install --pre --force-reinstall mlc-ai-nightly-cu121 mlc-chat-nightly-cu121 -f https://mlc.ai/wheels\"\n    )\n    # \"These commands will download many prebuilt libraries as well as the chat\n    # configuration for Llama-2-7b that mlc_chat needs\" [...]\n    .run_commands(\n        \"mkdir -p dist/prebuilt\",\n        \"git clone https://github.com/mlc-ai/binary-mlc-llm-libs.git dist/prebuilt/lib\",\n        f\"cd dist/prebuilt && git clone https://huggingface.co/mlc-ai/mlc-chat-Llama-2-{LLAMA_MODEL_SIZE}-chat-hf-q4f16_1\",\n    )\n)\nstub = modal.Stub(\"mlc-inference\", image=image)\nLOADING_MESSAGE: str = f\"\"\"\n                      #%%%%%%%%%%%%(         #%%%%%%%%%%%%#\n                    ,%##%,         %%      .%#(%/         %%.\n                   %%.  .%#         (%*   (%*   %%         *%/\n                 .%#      %%.        .%% %%      (%*         %%\n                #%,        ,%%%%%%%%%%%%%/        .%%         (%*\n               %%         (%*         %%*%(         #%,         %%\n             (%*         %%         *%(   %%         .%#         #%,\n            %%         *%/         %%.     *%(         #%,        .%#\n          /%(         %%         .%#         %%         .%%%%%%%%%%%%%.\n           (%/      ,%#         #%,           /%(      .%%         (%,\n             %%    %%.        .%%               %%    (%*         %%\n              (%*.%#         (%*                 /%/ %%         /%/\n                %%%%%%%%%%%%%%                     %%%%%%%%%%%%%%\n                      LOADING => Llama 2 ({LLAMA_MODEL_SIZE}) [{GPU}]\n\"\"\"\n```\n## Define Modal function\nThe `generate` function will load MLC chat and the compiled model into\nmemory and run inference on an input prompt. This is a generator, streaming\ntokens back to the client as they are generated.\n```python\n@stub.function(gpu=GPU)\ndef generate(prompt: str) -> Generator[Dict[str, str], None, None]:\n    from mlc_chat import ChatModule\n    from mlc_chat.callback import DeltaCallback\n    yield {\n        \"type\": \"loading\",\n        \"message\": LOADING_MESSAGE + \"\\n\\n\",\n    }\n    class QueueCallback(DeltaCallback):\n        \"\"\"Stream the output of the chat module to client.\"\"\"\n        def __init__(self, callback_interval: float):\n            super().__init__()\n            self.queue: queue.Queue = queue.Queue()\n            self.stopped = False\n            self.callback_interval = callback_interval\n        def delta_callback(self, delta_message: str):\n            self.stopped = False\n            self.queue.put(delta_message)\n        def stopped_callback(self):\n            self.stopped = True\n    cm = ChatModule(\n        model=f\"/dist/prebuilt/mlc-chat-Llama-2-{LLAMA_MODEL_SIZE}-chat-hf-q4f16_1\",\n        lib_path=f\"/dist/prebuilt/lib/Llama-2-{LLAMA_MODEL_SIZE}-chat-hf-q4f16_1-cuda.so\",\n    )\n    queue_callback = QueueCallback(callback_interval=1)\n    # Generate tokens in a background thread so we can yield tokens\n    # to caller as a generator.\n    def _generate():\n        cm.generate(\n            prompt=prompt,\n            progress_callback=queue_callback,\n        )\n    background_thread = threading.Thread(target=_generate)\n    background_thread.start()\n    # Yield as a generator to caller function and spawn\n    # text-to-speech functions.\n    while not queue_callback.stopped:\n        yield {\"type\": \"output\", \"message\": queue_callback.queue.get()}\n```\n## Run model\nCreate a local Modal entrypoint that call sthe `generate` function.\nThis uses the `curses` to render tokens as they are streamed back\nfrom Modal.\nRun this locally with `modal run -q mlc_inference.py --prompt \"What is serverless computing?\"`\n```python\n@stub.local_entrypoint()\ndef main(prompt: str):\n    import curses\n    def _generate(stdscr):\n        buffer: List[str] = []\n        def _buffered_message():\n            return \"\".join(buffer) + (\"\\n\" * 4)\n        start = time.time()\n        for payload in generate.remote_gen(prompt):\n            message = payload[\"message\"]\n            if payload[\"type\"] == \"loading\":\n                stdscr.clear()\n                stdscr.addstr(0, 0, message)\n                stdscr.refresh()\n            else:\n                buffer.append(message)\n                stdscr.clear()\n                stdscr.addstr(0, 0, _buffered_message())\n                stdscr.refresh()\n        n_tokens = len(buffer)\n        elapsed = time.time() - start\n        print(\n            f\"[DONE] {n_tokens} tokens generated in {elapsed:.2f}s ({n_tokens / elapsed:.0f} tok/s). Press any key to exit.\"\n        )\n        stdscr.getch()\n    curses.wrapper(_generate)\n```\n"}
{"text": "# Install scikit-learn in a custom image\nThis builds a custom image which installs the sklearn (scikit-learn) Python package in it.\nIt's an example of how you can use packages, even if you don't have them installed locally.\nFirst, the imports\n```python\nimport time\nimport modal\n```\nNext, define an stub, with a custom image that installs `sklearn`.\n```python\nstub = modal.Stub(\n    \"import-sklearn\",\n    image=modal.Image.debian_slim()\n    .apt_install(\"libgomp1\")\n    .pip_install(\"scikit-learn\"),\n)\n```\nThe `stub.is_inside()` lets us conditionally run code in the global scope.\nThis is needed because we might not have sklearn and numpy installed locally,\nbut we know they are installed inside the custom image. `stub.is_inside()`\nwill return `False` when we run this locally, but `True` when it runs in the cloud.\n```python\nif stub.is_inside():\n    import numpy as np\n    from sklearn import datasets, linear_model\n```\nNow, let's define a function that uses one of scikit-learn's built-in datasets\nand fits a very simple model (linear regression) to it\n```python\n@stub.function()\ndef fit():\n    print(\"Inside run!\")\n    t0 = time.time()\n    diabetes_X, diabetes_y = datasets.load_diabetes(return_X_y=True)\n    diabetes_X = diabetes_X[:, np.newaxis, 2]\n    regr = linear_model.LinearRegression()\n    regr.fit(diabetes_X, diabetes_y)\n    return time.time() - t0\n```\nFinally, let's trigger the run locally. We also time this. Note that the first time we run this,\nit will build the image. This might take 1-2 min. When we run this subsequent times, the image\nis already build, and it will run much much faster.\n```python\nif __name__ == \"__main__\":\n    t0 = time.time()\n    with stub.run():\n        t = fit.remote()\n        print(\"Function time spent:\", t)\n    print(\"Full time spent:\", time.time() - t0)\n```\n"}
{"text": "# Async functions\nModal natively supports async/await syntax using asyncio.\nFirst, let's import some global stuff.\n```python\nimport sys\nimport modal\nstub = modal.Stub(\"example-hello-world-async\")\n```\n## Defining a function\nNow, let's define a function. The wrapped function can be synchronous or\nasynchronous, but calling it in either context will still work.\nLet's stick to a normal synchronous function\n```python\n@stub.function()\ndef f(i):\n    if i % 2 == 0:\n        print(\"hello\", i)\n    else:\n        print(\"world\", i, file=sys.stderr)\n    return i * i\n```\n## Running the app with asyncio\nLet's make the main entrypoint asynchronous. In async contexts, we should\ncall the function using `await` or iterate over the map using `async for`.\nOtherwise we would block the event loop while our call is being run.\n```python\n@stub.local_entrypoint()\nasync def run_async():\n    # Call the function using .remote.aio() in order to run it asynchronously\n    print(await f.remote.aio(1000))\n    # Parallel map.\n    total = 0\n    # Call .map asynchronously using using f.map.aio(...)\n    async for ret in f.map.aio(range(20)):\n        total += ret\n    print(total)\n```\n"}
{"text": "# Fast inference with vLLM (Llama 2 13B)\nIn this example, we show how to run basic inference, using [`vLLM`](https://github.com/vllm-project/vllm)\nto take advantage of PagedAttention, which speeds up sequential inferences with optimized key-value caching.\n`vLLM` also supports a use case as a FastAPI server which we will explore in a future guide. This example\nwalks through setting up an environment that works with `vLLM ` for basic inference.\nWe are running the Llama 2 13B model here, and you can expect 30 second cold starts and well over 100 tokens/second.\nThe larger the batch of prompts, the higher the throughput. For example, with the 60 prompts below,\nwe can produce 24k tokens in 39 seconds, which is around 600 tokens/second.\nTo run\n[any of the other supported models](https://vllm.readthedocs.io/en/latest/models/supported_models.html),\nsimply replace the model name in the download step. You may also need to enable `trust_remote_code` for MPT models (see comment below)..\n## Setup\nFirst we import the components we need from `modal`.\n```python\nimport os\nfrom modal import Image, Secret, Stub, method\n```\n## Define a container image\nWe want to create a Modal image which has the model weights pre-saved to a directory. The benefit of this\nis that the container no longer has to re-download the model from Huggingface - instead, it will take\nadvantage of Modal's internal filesystem for faster cold starts.\n### Download the weights\nSince the weights are gated on HuggingFace, we must request access in two places:\n- on the [model card page](https://huggingface.co/meta-llama/Llama-2-13b-chat-hf)\n- accept the license [on the Meta website](https://ai.meta.com/resources/models-and-libraries/llama-downloads/).\nNext, [create a HuggingFace access token](https://huggingface.co/settings/tokens).\nTo access the token in a Modal function, we can create a secret on the [secrets page](https://modal.com/secrets).\nNow the token will be available via the environment variable named `HUGGINGFACE_TOKEN`. Functions that inject this secret will have access to the environment variable.\nWe can download the model to a particular directory using the HuggingFace utility function `snapshot_download`.\nTip: avoid using global variables in this function. Changes to code outside this function will not be detected and the download step will not re-run.\n```python\ndef download_model_to_folder():\n    from huggingface_hub import snapshot_download\n    snapshot_download(\n        \"meta-llama/Llama-2-13b-chat-hf\",\n        local_dir=\"/model\",\n        token=os.environ[\"HUGGINGFACE_TOKEN\"],\n    )\nMODEL_DIR = \"/model\"\n```\n### Image definition\nWe\u2019ll start from a Dockerhub image recommended by `vLLM`, upgrade the older\nversion of `torch` to a new one specifically built for CUDA 11.8. Next, we install `vLLM` from source to get the latest updates.\nFinally, we\u2019ll use run_function to run the function defined above to ensure the weights of the model\nare saved within the container image.\n```python\nimage = (\n    Image.from_registry(\"nvcr.io/nvidia/pytorch:22.12-py3\")\n    .pip_install(\n        \"torch==2.0.1\", index_url=\"https://download.pytorch.org/whl/cu118\"\n    )\n    # Pinned to 08/15/2023\n    .pip_install(\n        \"vllm @ git+https://github.com/vllm-project/vllm.git@805de738f618f8b47ab0d450423d23db1e636fa2\",\n        \"typing-extensions==4.5.0\",  # >=4.6 causes typing issues\n    )\n    # Use the barebones hf-transfer package for maximum download speeds. No progress bar, but expect 700MB/s.\n    .pip_install(\"hf-transfer~=0.1\")\n    .env({\"HF_HUB_ENABLE_HF_TRANSFER\": \"1\"})\n    .run_function(\n        download_model_to_folder,\n        secret=Secret.from_name(\"huggingface\"),\n        timeout=60 * 20,\n    )\n)\nstub = Stub(\"example-vllm-inference\", image=image)\n```\n## The model class\nThe inference function is best represented with Modal's [class syntax](/docs/guide/lifecycle-functions) and the `__enter__` method.\nThis enables us to load the model into memory just once every time a container starts up, and keep it cached\non the GPU for each subsequent invocation of the function.\nThe `vLLM` library allows the code to remain quite clean.\n```python\n@stub.cls(gpu=\"A100\", secret=Secret.from_name(\"huggingface\"))\nclass Model:\n    def __enter__(self):\n        from vllm import LLM\n        # Load the model. Tip: MPT models may require `trust_remote_code=true`.\n        self.llm = LLM(MODEL_DIR)\n        self.template = \"\"\"<s>[INST] <<SYS>>\n{system}\n<</SYS>>\n{user} [/INST] \"\"\"\n    @method()\n    def generate(self, user_questions):\n        from vllm import SamplingParams\n        prompts = [\n            self.template.format(system=\"\", user=q) for q in user_questions\n        ]\n        sampling_params = SamplingParams(\n            temperature=0.75,\n            top_p=1,\n            max_tokens=800,\n            presence_penalty=1.15,\n        )\n        result = self.llm.generate(prompts, sampling_params)\n        num_tokens = 0\n        for output in result:\n            num_tokens += len(output.outputs[0].token_ids)\n            print(output.prompt, output.outputs[0].text, \"\\n\\n\", sep=\"\")\n        print(f\"Generated {num_tokens} tokens\")\n```\n## Run the model\nWe define a [`local_entrypoint`](/docs/guide/apps#entrypoints-for-ephemeral-apps) to call our remote function\nsequentially for a list of inputs. You can run this locally with `modal run vllm_inference.py`.\n```python\n@stub.local_entrypoint()\ndef main():\n    model = Model()\n    questions = [\n        # Coding questions\n        \"Implement a Python function to compute the Fibonacci numbers.\",\n        \"Write a Rust function that performs binary exponentiation.\",\n        \"How do I allocate memory in C?\",\n        \"What are the differences between Javascript and Python?\",\n        \"How do I find invalid indices in Postgres?\",\n        \"How can you implement a LRU (Least Recently Used) cache in Python?\",\n        \"What approach would you use to detect and prevent race conditions in a multithreaded application?\",\n        \"Can you explain how a decision tree algorithm works in machine learning?\",\n        \"How would you design a simple key-value store database from scratch?\",\n        \"How do you handle deadlock situations in concurrent programming?\",\n        \"What is the logic behind the A* search algorithm, and where is it used?\",\n        \"How can you design an efficient autocomplete system?\",\n        \"What approach would you take to design a secure session management system in a web application?\",\n        \"How would you handle collision in a hash table?\",\n        \"How can you implement a load balancer for a distributed system?\",\n        # Literature\n        \"What is the fable involving a fox and grapes?\",\n        \"Write a story in the style of James Joyce about a trip to the Australian outback in 2083, to see robots in the beautiful desert.\",\n        \"Who does Harry turn into a balloon?\",\n        \"Write a tale about a time-traveling historian who's determined to witness the most significant events in human history.\",\n        \"Describe a day in the life of a secret agent who's also a full-time parent.\",\n        \"Create a story about a detective who can communicate with animals.\",\n        \"What is the most unusual thing about living in a city floating in the clouds?\",\n        \"In a world where dreams are shared, what happens when a nightmare invades a peaceful dream?\",\n        \"Describe the adventure of a lifetime for a group of friends who found a map leading to a parallel universe.\",\n        \"Tell a story about a musician who discovers that their music has magical powers.\",\n        \"In a world where people age backwards, describe the life of a 5-year-old man.\",\n        \"Create a tale about a painter whose artwork comes to life every night.\",\n        \"What happens when a poet's verses start to predict future events?\",\n        \"Imagine a world where books can talk. How does a librarian handle them?\",\n        \"Tell a story about an astronaut who discovered a planet populated by plants.\",\n        \"Describe the journey of a letter traveling through the most sophisticated postal service ever.\",\n        \"Write a tale about a chef whose food can evoke memories from the eater's past.\",\n        # History\n        \"What were the major contributing factors to the fall of the Roman Empire?\",\n        \"How did the invention of the printing press revolutionize European society?\",\n        \"What are the effects of quantitative easing?\",\n        \"How did the Greek philosophers influence economic thought in the ancient world?\",\n        \"What were the economic and philosophical factors that led to the fall of the Soviet Union?\",\n        \"How did decolonization in the 20th century change the geopolitical map?\",\n        \"What was the influence of the Khmer Empire on Southeast Asia's history and culture?\",\n        # Thoughtfulness\n        \"Describe the city of the future, considering advances in technology, environmental changes, and societal shifts.\",\n        \"In a dystopian future where water is the most valuable commodity, how would society function?\",\n        \"If a scientist discovers immortality, how could this impact society, economy, and the environment?\",\n        \"What could be the potential implications of contact with an advanced alien civilization?\",\n        # Math\n        \"What is the product of 9 and 8?\",\n        \"If a train travels 120 kilometers in 2 hours, what is its average speed?\",\n        \"Think through this step by step. If the sequence a_n is defined by a_1 = 3, a_2 = 5, and a_n = a_(n-1) + a_(n-2) for n > 2, find a_6.\",\n        \"Think through this step by step. Calculate the sum of an arithmetic series with first term 3, last term 35, and total terms 11.\",\n        \"Think through this step by step. What is the area of a triangle with vertices at the points (1,2), (3,-4), and (-2,5)?\",\n        \"Think through this step by step. Solve the following system of linear equations: 3x + 2y = 14, 5x - y = 15.\",\n        # Facts\n        \"Who was Emperor Norton I, and what was his significance in San Francisco's history?\",\n        \"What is the Voynich manuscript, and why has it perplexed scholars for centuries?\",\n        \"What was Project A119 and what were its objectives?\",\n        \"What is the 'Dyatlov Pass incident' and why does it remain a mystery?\",\n        \"What is the 'Emu War' that took place in Australia in the 1930s?\",\n        \"What is the 'Phantom Time Hypothesis' proposed by Heribert Illig?\",\n        \"Who was the 'Green Children of Woolpit' as per 12th-century English legend?\",\n        \"What are 'zombie stars' in the context of astronomy?\",\n        \"Who were the 'Dog-Headed Saint' and the 'Lion-Faced Saint' in medieval Christian traditions?\",\n        \"What is the story of the 'Globsters', unidentified organic masses washed up on the shores?\",\n    ]\n    model.generate.remote(questions)\n```\n"}
{"text": "\n# Hacker News Slackbot\nIn this example, we use Modal to deploy a cron job that periodically queries Hacker News for\nnew posts matching a given search term, and posts the results to Slack.\n## Import and define the stub\nLet's start off with imports, and defining a Modal stub.\n```python\nimport os\nfrom datetime import datetime, timedelta\nimport modal\nstub = modal.Stub(\"example-hn-bot\")\n```\nNow, let's define an image that has the `slack-sdk` package installed, in which we can run a function\nthat posts a slack message.\n```python\nslack_sdk_image = modal.Image.debian_slim().pip_install(\"slack-sdk\")\n```\n## Defining the function and importing the secret\nOur Slack bot will need access to a bot token. We can use Modal's [Secrets](/secrets) interface to accomplish this.\nTo quickly create a Slack bot secret, navigate to the [create secret](/secrets/create) page, select the Slack secret template\nfrom the list options, and follow the instructions in the \"Where to find the credentials?\" panel.\nName your secret `hn-bot-slack`, so that the code in this example still works.\nNow, we define the function `post_to_slack`, which simply instantiates the Slack client using our token,\nand then uses it to post a message to a given channel name.\n```python\n@stub.function(\n    image=slack_sdk_image, secret=modal.Secret.from_name(\"hn-bot-slack\")\n)\nasync def post_to_slack(message: str):\n    import slack_sdk\n    client = slack_sdk.WebClient(token=os.environ[\"SLACK_BOT_TOKEN\"])\n    client.chat_postMessage(channel=\"hn-alerts\", text=message)\n```\n## Searching Hacker News\nWe are going to use Algolia's [Hacker News Search API](https://hn.algolia.com/api) to query for posts\nmatching a given search term in the past X days. Let's define our search term and query period.\n```python\nQUERY = \"serverless\"\nWINDOW_SIZE_DAYS = 1\n```\nLet's also define an image that has the `requests` package installed, so we can query the API.\n```python\nrequests_image = modal.Image.debian_slim().pip_install(\"requests\")\n```\nWe can now define our main entrypoint, that queries Algolia for the term, and calls `post_to_slack`\non all the results. We specify a [schedule](/docs/guide/cron) in the function decorator, which\nmeans that our function will run automatically at the given interval.\n```python\n@stub.function(image=requests_image)\ndef search_hackernews():\n    import requests\n    url = \"http://hn.algolia.com/api/v1/search\"\n    threshold = datetime.utcnow() - timedelta(days=WINDOW_SIZE_DAYS)\n    params = {\n        \"query\": QUERY,\n        \"numericFilters\": f\"created_at_i>{threshold.timestamp()}\",\n    }\n    response = requests.get(url, params).json()\n    urls = [item[\"url\"] for item in response[\"hits\"] if item[\"url\"]]\n    print(f\"Query returned {len(urls)} items.\")\n    for _ in post_to_slack.map(urls):\n        pass\n```\n## Test running\nWe can now test run our scheduled function as follows: `modal run hackernews_alerts.py::stub.search_hackernews`\n## Defining the schedule and deploying\nLet's define a function that will be called by Modal every day\n```python\n@stub.function(schedule=modal.Period(days=1))\ndef run_daily():\n    search_hackernews.remote()\n```\nIn order to deploy this as a persistent cron job, you can run `modal deploy hackernews_alerts.py`,\nOnce the job is deployed, visit the [apps page](/apps) page to see\nits execution history, logs and other stats.\n"}
{"text": "\n```python\nimport modal\nstub = modal.Stub(\"example-generators\")\n@stub.function()\ndef f(i):\n    for j in range(i):\n        yield j\n@stub.local_entrypoint()\ndef main():\n    for r in f.remote_gen(10):\n        print(r)\n    for r in f.map(range(5)):\n        print(r)\n```\n"}
{"text": "\n```python\nimport os\nimport shutil\nimport subprocess\nfrom pathlib import Path\nimport modal\nLOCAL_PROJECT_ROOT = Path(__file__).parent / \"meltano_project\"\nREMOTE_PROJECT_ROOT = \"/meltano_project\"\nPERSISTED_VOLUME_PATH = \"/persisted\"\nREMOTE_DB_PATH = Path(f\"{PERSISTED_VOLUME_PATH}/meltano.db\")\nREMOTE_LOGS_PATH = Path(f\"{REMOTE_PROJECT_ROOT}/.meltano/logs\")\nPERSISTED_LOGS_DIR = Path(f\"{PERSISTED_VOLUME_PATH}/logs\")\nmeltano_source_mount = modal.Mount.from_local_dir(\n    LOCAL_PROJECT_ROOT,\n    remote_path=REMOTE_PROJECT_ROOT,\n    condition=lambda path: not any(p.startswith(\".\") for p in Path(path).parts),\n)\nstorage = modal.NetworkFileSystem.persisted(\"meltano_volume\")\nmeltano_conf = modal.Secret.from_dict(\n    {\n        \"MELTANO_PROJECT_ROOT\": REMOTE_PROJECT_ROOT,\n        \"MELTANO_DATABASE_URI\": f\"sqlite:///{REMOTE_DB_PATH}\",\n        \"SQLITE_WAREHOUSE\": f\"{PERSISTED_VOLUME_PATH}/jaffle_shop_raw\",\n        \"MELTANO_ENVIRONMENT\": \"modal\",\n    }\n)\ndef install_project_deps():\n    os.environ[\n        \"MELTANO_DATABASE_URI\"\n    ] = \"sqlite:////.empty_meltano.db\"  # dummy during installation\n    subprocess.check_call([\"meltano\", \"install\"])\n    # delete empty logs dir, so running containers can add a symlink instead\n    shutil.rmtree(REMOTE_LOGS_PATH, ignore_errors=True)\nmeltano_img = (\n    modal.Image.debian_slim()\n    .apt_install(\"git\")\n    .pip_install(\"meltano\")\n    .copy_mount(meltano_source_mount)\n    .run_function(install_project_deps, secret=meltano_conf)\n)\nstub = modal.Stub(\n    image=meltano_img,\n    secrets=[meltano_conf],\n)\ndef symlink_logs():\n    # symlink logs so that they end up in persisted network file system\n    # we can get rid of this if meltano gets a way to configure\n    # the logging directory\n    if not REMOTE_LOGS_PATH.exists():\n        PERSISTED_LOGS_DIR.mkdir(exist_ok=True, parents=True)\n        REMOTE_LOGS_PATH.symlink_to(PERSISTED_LOGS_DIR)\n```\nRun this example using `modal run meltano_modal.py::extract_and_load`\n```python\n@stub.function(\n    network_file_systems={PERSISTED_VOLUME_PATH: storage},\n    schedule=modal.Period(days=1),\n)\ndef extract_and_load():\n    symlink_logs()\n    subprocess.call(\n        [\"meltano\", \"run\", \"download_sample_data\", \"tap-csv\", \"target-sqlite\"]\n    )\n```\nInteractive sqlite3 exploration using `modal run meltano_modal.py::explore`\n```python\n@stub.function(\n    interactive=True,\n    network_file_systems={PERSISTED_VOLUME_PATH: storage},\n    timeout=86400,\n    image=modal.Image.debian_slim().apt_install(\"sqlite3\"),\n    secrets=[meltano_conf],\n)\ndef explore():\n    # explore the output database interactively using the sqlite3 shell\n    os.execlp(\"sqlite3\", \"sqlite3\", os.environ[\"SQLITE_WAREHOUSE\"] + \".db\")\n```\n"}
{"text": "\n# Face detection on YouTube videos\nThis is an example that uses\n[OpenCV](https://github.com/opencv/opencv-python)\nas well as video utilities\n[pytube](https://pytube.io/en/latest/)\nand\n[moviepy](https://zulko.github.io/moviepy/)\nto process video files in parallel.\nThe face detection is a pretty simple model built into OpenCV\nand is not state of the art.\n## The result\n<center>\n<video controls>\n<source src=\"./youtube_face_detection.mp4\" type=\"video/mp4\">\n<track kind=\"captions\" />\n</video>\n</center>\n## The Python code\nWe start by setting up the container image we need.\nThis requires installing a few dependencies needed for OpenCV as well as downloading the face detection model\n```python\nimport os\nimport modal\nOUTPUT_DIR = \"/tmp/\"\nFACE_CASCADE_FN = \"haarcascade_frontalface_default.xml\"\nimage = (\n    modal.Image.debian_slim()\n    .apt_install(\"libgl1-mesa-glx\", \"libglib2.0-0\", \"wget\", \"git\")\n    .run_commands(\n        f\"wget https://raw.githubusercontent.com/opencv/opencv/master/data/haarcascades/{FACE_CASCADE_FN} -P /root\"\n    )\n    .pip_install(\n        \"pytube @ git+https://github.com/modal-labs/pytube\",\n        \"opencv-python~=4.7.0.72\",\n        \"moviepy~=1.0.3\",\n    )\n)\nstub = modal.Stub(\"example-youtube-face-detection\", image=image)\nif stub.is_inside():\n    import cv2\n    import moviepy.editor\n    import pytube\n```\nFor temporary storage and sharing of downloaded movie clips, we use a network file system.\n```python\nstub.net_file_system = modal.NetworkFileSystem.new()\n```\n### Face detection function\nThe face detection function takes three arguments:\n* A filename to the source clip\n* A time slice denoted by start and a stop in seconds\nThe function extracts the subclip from the movie file (which is stored on the network file system),\nruns face detection on every frame in its slice,\nand stores the resulting video back to the shared storage.\n```python\n@stub.function(\n    network_file_systems={\"/clips\": stub.net_file_system}, timeout=600\n)\ndef detect_faces(fn, start, stop):\n    # Extract the subclip from the video\n    clip = moviepy.editor.VideoFileClip(fn).subclip(start, stop)\n    # Load face detector\n    face_cascade = cv2.CascadeClassifier(f\"/root/{FACE_CASCADE_FN}\")\n    # Run face detector on frames\n    imgs = []\n    for img in clip.iter_frames():\n        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n        faces = face_cascade.detectMultiScale(gray, 1.1, 4)\n        for x, y, w, h in faces:\n            cv2.rectangle(img, (x, y), (x + w, y + h), (255, 0, 0), 2)\n        imgs.append(img)\n    # Create mp4 of result\n    out_clip = moviepy.editor.ImageSequenceClip(imgs, fps=clip.fps)\n    out_fn = f\"/clips/{start:04d}.mp4\"\n    out_clip.write_videofile(out_fn)\n    return out_fn\n```\n### Modal entrypoint function\nThis 'entrypoint' into Modal controls the main flow of the program:\n1. Download the video from YouTube\n2. Fan-out face detection of individual 1s clips\n3. Stitch the results back into a new video\n```python\n@stub.function(network_file_systems={\"/clips\": stub.net_file_system}, retries=1)\ndef process_video(url):\n    print(f\"Downloading video from '{url}'\")\n    yt = pytube.YouTube(url)\n    stream = yt.streams.filter(file_extension=\"mp4\").first()\n    fn = stream.download(output_path=\"/clips/\", max_retries=5)\n    # Get duration\n    duration = moviepy.editor.VideoFileClip(fn).duration\n    # Create (start, stop) intervals\n    intervals = [(fn, offset, offset + 1) for offset in range(int(duration))]\n    print(\"Processing each range of 1s intervals using a Modal map\")\n    out_fns = list(detect_faces.starmap(intervals))\n    print(\"Converting detections to video clips\")\n    out_clips = [moviepy.editor.VideoFileClip(out_fn) for out_fn in out_fns]\n    print(\"Concatenating results\")\n    final_clip = moviepy.editor.concatenate_videoclips(out_clips)\n    final_fn = \"/clips/out.mp4\"\n    final_clip.write_videofile(final_fn)\n    # Return the full image data\n    with open(final_fn, \"rb\") as f:\n        return os.path.basename(fn), f.read()\n```\n### Local entrypoint\nThe code we run locally to fire up the Modal job is quite simple\n* Take a YouTube URL on the command line\n* Run the Modal function\n* Store the output data\n```python\n@stub.local_entrypoint()\ndef main(youtube_url: str = \"https://www.youtube.com/watch?v=dQw4w9WgXcQ\"):\n    fn, movie_data = process_video.remote(youtube_url)\n    abs_fn = os.path.join(OUTPUT_DIR, fn)\n    print(f\"writing results to {abs_fn}\")\n    with open(abs_fn, \"wb\") as f:\n        f.write(movie_data)\n```\n## Running the script\nRunning this script should take approximately a minute or less.\nIt might output a lot of warnings to standard error.\nThese are generally harmless.\nNote that we don't preserve the sound in the video.\n## Further directions\nAs you can tell from the resulting video, this face detection model is not state of the art.\nIt has plenty of false positives (non-faces being labeled faces) and false negatives (real faces not being labeled).\nFor better model, consider a modern one based on deep learning.\n"}
{"text": "\n```python\nimport os\nimport modal\nstub = modal.Stub(\"example-linkscraper\")\nplaywright_image = modal.Image.debian_slim(\n    python_version=\"3.10\"\n).run_commands(  # Doesn't work with 3.11 yet\n    \"apt-get install -y software-properties-common\",\n    \"apt-add-repository non-free\",\n    \"apt-add-repository contrib\",\n    \"apt-get update\",\n    \"pip install playwright==1.30.0\",\n    \"playwright install-deps chromium\",\n    \"playwright install chromium\",\n)\n@stub.function(image=playwright_image)\nasync def get_links(url: str) -> set[str]:\n    from playwright.async_api import async_playwright\n    async with async_playwright() as p:\n        browser = await p.chromium.launch()\n        page = await browser.new_page()\n        await page.goto(url)\n        links = await page.eval_on_selector_all(\n            \"a[href]\", \"elements => elements.map(element => element.href)\"\n        )\n        await browser.close()\n    return set(links)\nslack_sdk_image = modal.Image.debian_slim().pip_install(\"slack-sdk\")\n@stub.function(\n    image=slack_sdk_image, secret=modal.Secret.from_name(\"scraper-slack-secret\")\n)\ndef bot_token_msg(channel, message):\n    import slack_sdk\n    print(f\"Posting {message} to #{channel}\")\n    client = slack_sdk.WebClient(token=os.environ[\"SLACK_BOT_TOKEN\"])\n    client.chat_postMessage(channel=channel, text=message)\n@stub.function()\ndef scrape():\n    links_of_interest = [\"http://modal.com\"]\n    for links in get_links.map(links_of_interest):\n        for link in links:\n            bot_token_msg.remote(\"scraped-links\", link)\n@stub.function(schedule=modal.Period(days=1))\ndef daily_scrape():\n    scrape.remote()\n@stub.local_entrypoint()\ndef run():\n    scrape.remote()\n```\n"}
{"text": "\n```python\nimport time\nfrom datetime import datetime\nimport modal\nstub = modal.Stub(\"example-schedule-simple\")\n@stub.function(schedule=modal.Period(seconds=5))\ndef print_time_1():\n    print(\n        f'Printing with period 5 seconds: {datetime.now().strftime(\"%m/%d/%Y, %H:%M:%S\")}'\n    )\n@stub.function(schedule=modal.Cron(\"* * * * *\"))\ndef print_time_2():\n    print(\n        f'Printing with cron every minute: {datetime.now().strftime(\"%m/%d/%Y, %H:%M:%S\")}'\n    )\nif __name__ == \"__main__\":\n    with stub.run():\n        time.sleep(10)\n```\n"}
{"text": "\n# Stable Diffusion CLI\nThis example shows Stable Diffusion 1.5 with a number of optimizations\nthat makes it run faster on Modal. The example takes about 10s to cold start\nand about 1.0s per image generated.\nTo use the new XL 1.0 model, see the example posted [here](/docs/guide/ex/stable_diffusion_xl).\nFor instance, here are 9 images produced by the prompt\n`An 1600s oil painting of the New York City skyline`\n![stable diffusion slackbot](./stable_diffusion_montage.png)\nThere is also a [Stable Diffusion Slack bot example](/docs/guide/ex/stable_diffusion_slackbot)\nwhich does not have all the optimizations, but shows how you can set up a Slack command to\ntrigger Stable Diffusion.\n## Optimizations used in this example\nAs mentioned, we use a few optimizations to run this faster:\n* Use [run_function](/docs/reference/modal.Image#run_function) to download the model while building the container image\n* Use a [container lifecycle method](https://modal.com/docs/guide/lifecycle-functions) to initialize the model on container startup\n* Use A10G GPUs\n* Use 16 bit floating point math\n## Basic setup\n```python\nfrom __future__ import annotations\nimport io\nimport time\nfrom pathlib import Path\nfrom modal import Image, Stub, method\n```\nAll Modal programs need a [`Stub`](/docs/reference/modal.Stub) \u2014 an object that acts as a recipe for\nthe application. Let's give it a friendly name.\n```python\nstub = Stub(\"stable-diffusion-cli\")\n```\n## Model dependencies\nYour model will be running remotely inside a container. We will be installing\nall the model dependencies in the next step. We will also be \"baking the model\"\ninto the image by running a Python function as a part of building the image.\nThis lets us start containers much faster, since all the data that's needed is\nalready inside the image.\n```python\nmodel_id = \"runwayml/stable-diffusion-v1-5\"\ncache_path = \"/vol/cache\"\ndef download_models():\n    import diffusers\n    import torch\n    # Download scheduler configuration. Experiment with different schedulers\n    # to identify one that works best for your use-case.\n    scheduler = diffusers.DPMSolverMultistepScheduler.from_pretrained(\n        model_id,\n        subfolder=\"scheduler\",\n        cache_dir=cache_path,\n    )\n    scheduler.save_pretrained(cache_path, safe_serialization=True)\n    # Downloads all other models.\n    pipe = diffusers.StableDiffusionPipeline.from_pretrained(\n        model_id,\n        revision=\"fp16\",\n        torch_dtype=torch.float16,\n        cache_dir=cache_path,\n    )\n    pipe.save_pretrained(cache_path, safe_serialization=True)\nimage = (\n    Image.debian_slim(python_version=\"3.10\")\n    .pip_install(\n        \"accelerate\",\n        \"diffusers[torch]>=0.15.1\",\n        \"ftfy\",\n        \"torchvision\",\n        \"transformers~=4.25.1\",\n        \"triton\",\n        \"safetensors\",\n    )\n    .pip_install(\n        \"torch==2.0.1+cu117\",\n        find_links=\"https://download.pytorch.org/whl/torch_stable.html\",\n    )\n    .pip_install(\"xformers\", pre=True)\n    .run_function(download_models)\n)\nstub.image = image\n```\n## Using container lifecycle methods\nModal lets you implement code that runs every time a container starts. This\ncan be a huge optimization when you're calling a function multiple times,\nsince Modal reuses the same containers when possible.\nThe way to implement this is to turn the Modal function into a method on a\nclass that also implement the Python context manager interface, meaning it\nhas the `__enter__` method (the `__exit__` method is optional).\nWe have also have applied a few model optimizations to make the model run\nfaster. On an A10G, the model takes about 6.5s to load into memory, and then\n1.6s per generation on average. On a T4, it takes 13s to load and 3.7s per\ngeneration. Other optimizations are also available [here](https://huggingface.co/docs/diffusers/optimization/fp16#memory-and-speed).\nThis is our Modal function. The function runs through the `StableDiffusionPipeline` pipeline.\nIt sends the PIL image back to our CLI where we save the resulting image in a local file.\n```python\n@stub.cls(gpu=\"A10G\")\nclass StableDiffusion:\n    def __enter__(self):\n        import diffusers\n        import torch\n        torch.backends.cuda.matmul.allow_tf32 = True\n        scheduler = diffusers.DPMSolverMultistepScheduler.from_pretrained(\n            cache_path,\n            subfolder=\"scheduler\",\n            solver_order=2,\n            prediction_type=\"epsilon\",\n            thresholding=False,\n            algorithm_type=\"dpmsolver++\",\n            solver_type=\"midpoint\",\n            denoise_final=True,  # important if steps are <= 10\n            low_cpu_mem_usage=True,\n            device_map=\"auto\",\n        )\n        self.pipe = diffusers.StableDiffusionPipeline.from_pretrained(\n            cache_path,\n            scheduler=scheduler,\n            low_cpu_mem_usage=True,\n            device_map=\"auto\",\n        )\n        self.pipe.enable_xformers_memory_efficient_attention()\n    @method()\n    def run_inference(\n        self, prompt: str, steps: int = 20, batch_size: int = 4\n    ) -> list[bytes]:\n        import torch\n        with torch.inference_mode():\n            with torch.autocast(\"cuda\"):\n                images = self.pipe(\n                    [prompt] * batch_size,\n                    num_inference_steps=steps,\n                    guidance_scale=7.0,\n                ).images\n        # Convert to PNG bytes\n        image_output = []\n        for image in images:\n            with io.BytesIO() as buf:\n                image.save(buf, format=\"PNG\")\n                image_output.append(buf.getvalue())\n        return image_output\n```\nThis is the command we'll use to generate images. It takes a `prompt`,\n`samples` (the number of images you want to generate), `steps` which\nconfigures the number of inference steps the model will make, and `batch_size`\nwhich determines how many images to generate for a given prompt.\n```python\n@stub.local_entrypoint()\ndef entrypoint(\n    prompt: str, samples: int = 5, steps: int = 10, batch_size: int = 1\n):\n    print(\n        f\"prompt => {prompt}, steps => {steps}, samples => {samples}, batch_size => {batch_size}\"\n    )\n    dir = Path(\"/tmp/stable-diffusion\")\n    if not dir.exists():\n        dir.mkdir(exist_ok=True, parents=True)\n    sd = StableDiffusion()\n    for i in range(samples):\n        t0 = time.time()\n        images = sd.run_inference.remote(prompt, steps, batch_size)\n        total_time = time.time() - t0\n        print(\n            f\"Sample {i} took {total_time:.3f}s ({(total_time)/len(images):.3f}s / image).\"\n        )\n        for j, image_bytes in enumerate(images):\n            output_path = dir / f\"output_{j}_{i}.png\"\n            print(f\"Saving it to {output_path}\")\n            with open(output_path, \"wb\") as f:\n                f.write(image_bytes)\n```\nAnd this is our entrypoint; where the CLI is invoked. Explore CLI options\nwith: `modal run stable_diffusion_cli.py --help`\n## Performance\nThis example can generate pictures in about a second, with startup time of about 10s for the first picture.\nSee distribution of latencies below. This data was gathered by running 500 requests in sequence (meaning only\nthe first request incurs a cold start). As you can see, the 90th percentile is 1.2s and the 99th percentile is 2.30s.\n![latencies](./stable_diffusion_latencies.png)\n"}
{"text": "\n```python\nfrom modal import Image, Stub, wsgi_app\nstub = Stub(\n    \"example-web-flask\",\n    image=Image.debian_slim().pip_install(\"flask\"),\n)\n@stub.function()\n@wsgi_app()\ndef flask_app():\n    from flask import Flask, request\n    web_app = Flask(__name__)\n    @web_app.get(\"/\")\n    def home():\n        return \"Hello Flask World!\"\n    @web_app.post(\"/foo\")\n    def foo():\n        return request.json\n    return web_app\n```\n"}
{"text": "# `modal serve`\nRun a web endpoint(s) associated with a Modal stub and hot-reload code.\n**Examples:**\n```bash\nmodal serve hello_world.py\n```\n**Usage**:\n```shell\nmodal serve [OPTIONS] STUB_REF\n```\n**Arguments**:\n* `STUB_REF`: Path to a Python file with a stub.  [required]\n**Options**:\n* `--timeout FLOAT`\n* `--env TEXT`: Environment to interact with\nIf none is specified, Modal will use the default environment of your current profile (can also be specified via the environment variable MODAL_ENVIRONMENT).\nIf neither is set, Modal will assume there is only one environment in the active workspace and use that one, or raise an error if there are multiple environments.\n* `--help`: Show this message and exit.\n"}
{"text": "# `modal token`\nManage tokens.\n**Usage**:\n```shell\nmodal token [OPTIONS] COMMAND [ARGS]...\n```\n**Options**:\n* `--help`: Show this message and exit.\n**Commands**:\n* `new`: Creates a new token by using an...\n* `set`: Set account credentials for connecting to...\n## `modal token new`\nCreates a new token by using an authenticated web session.\n**Usage**:\n```shell\nmodal token new [OPTIONS]\n```\n**Options**:\n* `--profile TEXT`: Modal profile to set credentials for. You can switch the currently active Modal profile with the `modal profile` command. If unspecified, uses `default` profile.\n* `--no-verify / --no-no-verify`: [default: no-no-verify]\n* `--source TEXT`\n* `--help`: Show this message and exit.\n## `modal token set`\nSet account credentials for connecting to Modal. If not provided with the command, you will be prompted to enter your credentials.\n**Usage**:\n```shell\nmodal token set [OPTIONS]\n```\n**Options**:\n* `--token-id TEXT`: Account token ID.\n* `--token-secret TEXT`: Account token secret.\n* `--profile TEXT`: Modal profile to set credentials for. You can switch the currently active Modal profile with the `modal profile` command. If unspecified, uses `default` profile.\n* `--no-verify / --no-no-verify`: [default: no-no-verify]\n* `--help`: Show this message and exit.\n"}
{"text": "# `modal config`\nManage client configuration for the current profile.\nRefer to https://modal.com/docs/reference/modal.config for a full explanation\nof what these options mean, and how to set them.\n**Usage**:\n```shell\nmodal config [OPTIONS] COMMAND [ARGS]...\n```\n**Options**:\n* `--help`: Show this message and exit.\n**Commands**:\n* `set-environment`: Set the default Modal environment for the...\n* `show`: Show configuration values for the current...\n## `modal config set-environment`\nSet the default Modal environment for the active profile\nThe default environment of a profile is used when no --env flag is passed to `modal run`, `modal deploy` etc.\nIf no default environment is set, and there exists multiple environments in a workspace, an error will be raised\nwhen running a command that requires an environment.\n**Usage**:\n```shell\nmodal config set-environment [OPTIONS] ENVIRONMENT_NAME\n```\n**Arguments**:\n* `ENVIRONMENT_NAME`: [required]\n**Options**:\n* `--help`: Show this message and exit.\n## `modal config show`\nShow configuration values for the current profile (debug command).\n**Usage**:\n```shell\nmodal config show [OPTIONS]\n```\n**Options**:\n* `--help`: Show this message and exit.\n"}
{"text": "# `modal nfs`\nRead and edit `modal.NetworkFileSystem` file systems.\n**Usage**:\n```shell\nmodal nfs [OPTIONS] COMMAND [ARGS]...\n```\n**Options**:\n* `--help`: Show this message and exit.\n**Commands**:\n* `create`: Create a named network file system.\n* `get`: Download a file from a network file system.\n* `list`: List the names of all network file systems.\n* `ls`: List files and directories in a network...\n* `put`: Upload a file or directory to a network...\n* `rm`: Delete a file or directory from a network...\n## `modal nfs create`\nCreate a named network file system.\n**Usage**:\n```shell\nmodal nfs create [OPTIONS] NAME\n```\n**Arguments**:\n* `NAME`: [required]\n**Options**:\n* `--cloud TEXT`: Cloud provider to create the file system in. One of aws|gcp.  [default: aws]\n* `--env TEXT`: Environment to interact with\nIf none is specified, Modal will use the default environment of your current profile (can also be specified via the environment variable MODAL_ENVIRONMENT).\nIf neither is set, Modal will assume there is only one environment in the active workspace and use that one, or raise an error if there are multiple environments.\n* `--help`: Show this message and exit.\n## `modal nfs get`\nDownload a file from a network file system.\nSpecifying a glob pattern (using any `*` or `**` patterns) as the `remote_path` will download all matching *files*, preserving\nthe source directory structure for the matched files.\nFor example, to download an entire network file system into `dump_volume`:\n```bash\nmodal nfs get <volume-name> \"**\" dump_volume\n```\nUse \"-\" (a hyphen) as LOCAL_DESTINATION to write contents of file to stdout (only for non-glob paths).\n**Usage**:\n```shell\nmodal nfs get [OPTIONS] VOLUME_NAME REMOTE_PATH [LOCAL_DESTINATION]\n```\n**Arguments**:\n* `VOLUME_NAME`: [required]\n* `REMOTE_PATH`: [required]\n* `[LOCAL_DESTINATION]`: [default: .]\n**Options**:\n* `--force / --no-force`: [default: no-force]\n* `--env TEXT`: Environment to interact with\nIf none is specified, Modal will use the default environment of your current profile (can also be specified via the environment variable MODAL_ENVIRONMENT).\nIf neither is set, Modal will assume there is only one environment in the active workspace and use that one, or raise an error if there are multiple environments.\n* `--help`: Show this message and exit.\n## `modal nfs list`\nList the names of all network file systems.\n**Usage**:\n```shell\nmodal nfs list [OPTIONS]\n```\n**Options**:\n* `--env TEXT`: Environment to interact with\nIf none is specified, Modal will use the default environment of your current profile (can also be specified via the environment variable MODAL_ENVIRONMENT).\nIf neither is set, Modal will assume there is only one environment in the active workspace and use that one, or raise an error if there are multiple environments.\n* `--json / --no-json`: [default: no-json]\n* `--help`: Show this message and exit.\n## `modal nfs ls`\nList files and directories in a network file system.\n**Usage**:\n```shell\nmodal nfs ls [OPTIONS] VOLUME_NAME [PATH]\n```\n**Arguments**:\n* `VOLUME_NAME`: [required]\n* `[PATH]`: [default: /]\n**Options**:\n* `--env TEXT`: Environment to interact with\nIf none is specified, Modal will use the default environment of your current profile (can also be specified via the environment variable MODAL_ENVIRONMENT).\nIf neither is set, Modal will assume there is only one environment in the active workspace and use that one, or raise an error if there are multiple environments.\n* `--help`: Show this message and exit.\n## `modal nfs put`\nUpload a file or directory to a network file system.\nRemote parent directories will be created as needed.\nEnding the REMOTE_PATH with a forward slash (/), it's assumed to be a directory and the file will be uploaded with its current name under that directory.\n**Usage**:\n```shell\nmodal nfs put [OPTIONS] VOLUME_NAME LOCAL_PATH [REMOTE_PATH]\n```\n**Arguments**:\n* `VOLUME_NAME`: [required]\n* `LOCAL_PATH`: [required]\n* `[REMOTE_PATH]`: [default: /]\n**Options**:\n* `--env TEXT`: Environment to interact with\nIf none is specified, Modal will use the default environment of your current profile (can also be specified via the environment variable MODAL_ENVIRONMENT).\nIf neither is set, Modal will assume there is only one environment in the active workspace and use that one, or raise an error if there are multiple environments.\n* `--help`: Show this message and exit.\n## `modal nfs rm`\nDelete a file or directory from a network file system.\n**Usage**:\n```shell\nmodal nfs rm [OPTIONS] VOLUME_NAME REMOTE_PATH\n```\n**Arguments**:\n* `VOLUME_NAME`: [required]\n* `REMOTE_PATH`: [required]\n**Options**:\n* `-r, --recursive`: Delete directory recursively\n* `--env TEXT`: Environment to interact with\nIf none is specified, Modal will use the default environment of your current profile (can also be specified via the environment variable MODAL_ENVIRONMENT).\nIf neither is set, Modal will assume there is only one environment in the active workspace and use that one, or raise an error if there are multiple environments.\n* `--help`: Show this message and exit.\n"}
{"text": "# `modal secret`\nManage secrets.\n**Usage**:\n```shell\nmodal secret [OPTIONS] COMMAND [ARGS]...\n```\n**Options**:\n* `--help`: Show this message and exit.\n**Commands**:\n* `create`: Create a new secret, or overwrite an...\n* `list`: List your published secrets.\n## `modal secret create`\nCreate a new secret, or overwrite an existing one.\n**Usage**:\n```shell\nmodal secret create [OPTIONS] SECRET_NAME KEYVALUES...\n```\n**Arguments**:\n* `SECRET_NAME`: [required]\n* `KEYVALUES...`: Space-separated KEY=VALUE items  [required]\n**Options**:\n* `--env TEXT`: Environment to interact with\nIf none is specified, Modal will use the default environment of your current profile (can also be specified via the environment variable MODAL_ENVIRONMENT).\nIf neither is set, Modal will assume there is only one environment in the active workspace and use that one, or raise an error if there are multiple environments.\n* `--help`: Show this message and exit.\n## `modal secret list`\nList your published secrets.\n**Usage**:\n```shell\nmodal secret list [OPTIONS]\n```\n**Options**:\n* `--env TEXT`: Environment to interact with\nIf none is specified, Modal will use the default environment of your current profile (can also be specified via the environment variable MODAL_ENVIRONMENT).\nIf neither is set, Modal will assume there is only one environment in the active workspace and use that one, or raise an error if there are multiple environments.\n* `--json / --no-json`: [default: no-json]\n* `--help`: Show this message and exit.\n"}
{"text": "# `modal vol`\nDeprecated, use `modal volume` instead.\n**Usage**:\n```shell\nmodal vol [OPTIONS]\n```\n**Options**:\n* `--help`: Show this message and exit.\n"}
{"text": "# `modal environment`\nCreate and interact with Environments\nEnvironments are sub-divisons of workspaces, allowing you to deploy the same app\nin different namespaces. Each environment has their own set of Secrets and any\nlookups performed from an app in an environment will by default look for entities\nin the same environment.\nTypical use cases for environments include having one for development and one for\nproduction, to prevent overwriting production apps when developing new features\nwhile still being able to deploy changes to a live environment.\n**Usage**:\n```shell\nmodal environment [OPTIONS] COMMAND [ARGS]...\n```\n**Options**:\n* `--help`: Show this message and exit.\n**Commands**:\n* `create`: Create a new environment in the current...\n* `delete`: Delete an environment in the current...\n* `list`: List all environments in the current...\n* `update`: Update the name or web suffix of an...\n## `modal environment create`\nCreate a new environment in the current workspace\n**Usage**:\n```shell\nmodal environment create [OPTIONS] NAME\n```\n**Arguments**:\n* `NAME`: Name of the new environment. Must be unique. Case sensitive  [required]\n**Options**:\n* `--help`: Show this message and exit.\n## `modal environment delete`\nDelete an environment in the current workspace\nDeletes all apps in the selected environment and deletes the environment irrevocably.\n**Usage**:\n```shell\nmodal environment delete [OPTIONS] NAME\n```\n**Arguments**:\n* `NAME`: Name of the environment to be deleted. Case sensitive  [required]\n**Options**:\n* `--confirm / --no-confirm`: Set this flag to delete without prompting for confirmation  [default: no-confirm]\n* `--help`: Show this message and exit.\n## `modal environment list`\nList all environments in the current workspace\n**Usage**:\n```shell\nmodal environment list [OPTIONS]\n```\n**Options**:\n* `--json / --no-json`: [default: no-json]\n* `--help`: Show this message and exit.\n## `modal environment update`\nUpdate the name or web suffix of an environment\n**Usage**:\n```shell\nmodal environment update [OPTIONS] CURRENT_NAME\n```\n**Arguments**:\n* `CURRENT_NAME`: [required]\n**Options**:\n* `--set-name TEXT`: New name of the environment\n* `--set-web-suffix TEXT`: New web suffix of environment (empty string is no suffix)\n* `--help`: Show this message and exit.\n"}
{"text": "# `modal deploy`\nDeploy a Modal stub as an application.\n**Usage**:\n```shell\nmodal deploy [OPTIONS] STUB_REF\n```\n**Arguments**:\n* `STUB_REF`: Path to a Python file with a stub.  [required]\n**Options**:\n* `--name TEXT`: Name of the deployment.\n* `--env TEXT`: Environment to interact with\nIf none is specified, Modal will use the default environment of your current profile (can also be specified via the environment variable MODAL_ENVIRONMENT).\nIf neither is set, Modal will assume there is only one environment in the active workspace and use that one, or raise an error if there are multiple environments.\n* `--help`: Show this message and exit.\n"}
{"text": "# `modal app`\nManage deployed and running apps.\n**Usage**:\n```shell\nmodal app [OPTIONS] COMMAND [ARGS]...\n```\n**Options**:\n* `--help`: Show this message and exit.\n**Commands**:\n* `list`: List all running or recently running Modal...\n* `logs`: Output logs for a running app.\n* `stop`: Stop an app.\n## `modal app list`\nList all running or recently running Modal apps for the current account\n**Usage**:\n```shell\nmodal app list [OPTIONS]\n```\n**Options**:\n* `--env TEXT`: Environment to interact with\nIf none is specified, Modal will use the default environment of your current profile (can also be specified via the environment variable MODAL_ENVIRONMENT).\nIf neither is set, Modal will assume there is only one environment in the active workspace and use that one, or raise an error if there are multiple environments.\n* `--json / --no-json`: [default: no-json]\n* `--help`: Show this message and exit.\n## `modal app logs`\nOutput logs for a running app.\n**Usage**:\n```shell\nmodal app logs [OPTIONS] APP_ID\n```\n**Arguments**:\n* `APP_ID`: [required]\n**Options**:\n* `--help`: Show this message and exit.\n## `modal app stop`\nStop an app.\n**Usage**:\n```shell\nmodal app stop [OPTIONS] APP_ID\n```\n**Arguments**:\n* `APP_ID`: [required]\n**Options**:\n* `--help`: Show this message and exit.\n"}
{"text": "# `modal profile`\nSet the active Modal profile.\n**Usage**:\n```shell\nmodal profile [OPTIONS] COMMAND [ARGS]...\n```\n**Options**:\n* `--help`: Show this message and exit.\n**Commands**:\n* `activate`: Change the active Modal profile.\n* `current`: Print the active Modal profile.\n* `list`: List all Modal profiles that are defined.\n## `modal profile activate`\nChange the active Modal profile.\n**Usage**:\n```shell\nmodal profile activate [OPTIONS] PROFILE\n```\n**Arguments**:\n* `PROFILE`: Modal profile to activate.  [required]\n**Options**:\n* `--help`: Show this message and exit.\n## `modal profile current`\nPrint the active Modal profile.\n**Usage**:\n```shell\nmodal profile current [OPTIONS]\n```\n**Options**:\n* `--help`: Show this message and exit.\n## `modal profile list`\nList all Modal profiles that are defined.\n**Usage**:\n```shell\nmodal profile list [OPTIONS]\n```\n**Options**:\n* `--json / --no-json`: [default: no-json]\n* `--help`: Show this message and exit.\n"}
{"text": "# `modal shell`\nRun an interactive shell inside a Modal image.\n**Examples:**\nStart a bash shell using the spec for `my_function` in your stub:\n```bash\nmodal shell hello_world.py::my_function\n```\nNote that you can select the function interactively if you omit the function name.\nStart a `python` shell:\n```bash\nmodal shell hello_world.py --cmd=python\n```\n**Usage**:\n```shell\nmodal shell [OPTIONS] FUNC_REF\n```\n**Arguments**:\n* `FUNC_REF`: Path to a Python file with a Stub or Modal function whose container to run.  [required]\n**Options**:\n* `--cmd TEXT`: Command to run inside the Modal image.  [default: /bin/bash]\n* `--env TEXT`: Environment to interact with\nIf none is specified, Modal will use the default environment of your current profile (can also be specified via the environment variable MODAL_ENVIRONMENT).\nIf neither is set, Modal will assume there is only one environment in the active workspace and use that one, or raise an error if there are multiple environments.\n* `--help`: Show this message and exit.\n"}
{"text": "# `modal volume`\n[Preview] Read and edit `modal.Volume` volumes.\nThis command is in preview and may change in the future.\nPrevious users of `modal.NetworkFileSystem` should replace their usage with\nthe `modal nfs` command instead.\n**Usage**:\n```shell\nmodal volume [OPTIONS] COMMAND [ARGS]...\n```\n**Options**:\n* `--help`: Show this message and exit.\n**Commands**:\n* `ls`: List files and directories in a...\n## `modal volume ls`\nList files and directories in a modal.Volume volume.\n**Usage**:\n```shell\nmodal volume ls [OPTIONS] VOLUME_NAME [PATH]\n```\n**Arguments**:\n* `VOLUME_NAME`: [required]\n* `[PATH]`: [default: /]\n**Options**:\n* `--env TEXT`: Environment to interact with\nIf none is specified, Modal will use the default environment of your current profile (can also be specified via the environment variable MODAL_ENVIRONMENT).\nIf neither is set, Modal will assume there is only one environment in the active workspace and use that one, or raise an error if there are multiple environments.\n* `--help`: Show this message and exit.\n"}
{"text": "# `modal run`\nRun a Modal function or local entrypoint\n    `FUNC_REF` should be of the format `{file or module}::{function name}`.\n    Alternatively, you can refer to the function via the stub:\n    `{file or module}::{stub variable name}.{function name}`\n    **Examples:**\n    To run the hello_world function (or local entrypoint) in my_app.py:\n    ```bash\n    modal run my_app.py::hello_world\n    ```\n    If your module only has a single stub called `stub` and your stub has a\n    single local entrypoint (or single function), you can omit the stub and\n    function parts:\n    ```bash\n    modal run my_app.py\n    ```\n    Instead of pointing to a file, you can also use the Python module path:\n    ```bash\n    modal run my_project.my_app\n    ```\n    \n**Usage**:\n```shell\nmodal run [OPTIONS] FUNC_REF\n```\n**Options**:\n* `-q, --quiet`: Don't show Modal progress indicators.\n* `-d, --detach`: Don't stop the app if the local process dies or disconnects.\n* `--help`: Show this message and exit.\n"}
{"text": "# `modal env`\n[Deprecated, use `modal profile` instead] Set the current environment.\n**Usage**:\n```shell\nmodal env [OPTIONS] COMMAND [ARGS]...\n```\n**Options**:\n* `--help`: Show this message and exit.\n**Commands**:\n* `activate`: [Deprecated, use `modal profile` instead]...\n* `current`: [Deprecated, use `modal profile` instead]...\n* `list`: [Deprecated, use `modal profile` instead]...\n## `modal env activate`\n[Deprecated, use `modal profile` instead] Change the active Modal environment.\n**Usage**:\n```shell\nmodal env activate [OPTIONS] ENV\n```\n**Arguments**:\n* `ENV`: Modal environment to activate.  [required]\n**Options**:\n* `--help`: Show this message and exit.\n## `modal env current`\n[Deprecated, use `modal profile` instead] Print the active Modal environment.\n**Usage**:\n```shell\nmodal env current [OPTIONS]\n```\n**Options**:\n* `--help`: Show this message and exit.\n## `modal env list`\n[Deprecated, use `modal profile` instead] List all Modal environments that are defined.\n**Usage**:\n```shell\nmodal env list [OPTIONS]\n```\n**Options**:\n* `--help`: Show this message and exit.\n"}
{"text": "# modal.runner\n## modal.runner.deploy_stub\n```python\nasync def deploy_stub(\n    stub,\n    name: str = None,\n    namespace=api_pb2.DEPLOYMENT_NAMESPACE_WORKSPACE,\n    client=None,\n    stdout=None,\n    show_progress=True,\n    object_entity=\"ap\",\n    environment_name: Optional[str] = None,\n) -> _App:\n```\nDeploy an app and export its objects persistently.\nTypically, using the command-line tool `modal deploy <module or script>`\nshould be used, instead of this method.\n**Usage:**\n```python\nif __name__ == \"__main__\":\n    deploy_stub(stub)\n```\nDeployment has two primary purposes:\n* Persists all of the objects in the app, allowing them to live past the\n  current app run. For schedules this enables headless \"cron\"-like\n  functionality where scheduled functions continue to be invoked after\n  the client has disconnected.\n* Allows for certain kinds of these objects, _deployment objects_, to be\n  referred to and used by other apps.\n## modal.runner.interactive_shell\n```python\nasync def interactive_shell(_function: _Function, cmd: str, environment_name: str = \"\"):\n```\nRun an interactive shell (like `bash`) within the image for this app.\nThis is useful for online debugging and interactive exploration of the\ncontents of this image. If `cmd` is optionally provided, it will be run\ninstead of the default shell inside this image.\n**Example**\n```python\nimport modal\nstub = modal.Stub(image=modal.Image.debian_slim().apt_install(\"vim\"))\n```\nYou can now run this using\n```bash\nmodal shell script.py --cmd /bin/bash\n```\n## modal.runner.run_stub\n```python\n@contextlib.asynccontextmanager\ndef run_stub(\n    stub,\n    client: Optional[_Client] = None,\n    stdout=None,\n    show_progress: bool = True,\n    detach: bool = False,\n    output_mgr: Optional[OutputManager] = None,\n    environment_name: Optional[str] = None,\n    shell=False,\n) -> AsyncGenerator[_App, None]:\n```\n## modal.runner.serve_update\n```python\nasync def serve_update(\n    stub,\n    existing_app_id: str,\n    is_ready: Event,\n    environment_name: str,\n) -> None:\n    # Used by child process to reinitialize a served app\n```\n"}
{"text": "# modal.exception\n## modal.exception.AuthError\n```python\nclass AuthError(modal.exception.Error)\n```\nRaised when a client has missing or invalid authentication.\n## modal.exception.ConnectionError\n```python\nclass ConnectionError(modal.exception.Error)\n```\nRaised when an issue occurs while connecting to the Modal servers.\n## modal.exception.DeprecationError\n```python\nclass DeprecationError(UserWarning)\n```\nUserWarning category emitted when a deprecated Modal feature or API is used.\n## modal.exception.ExecutionError\n```python\nclass ExecutionError(modal.exception.Error)\n```\nRaised when something unexpected happened during runtime.\n## modal.exception.FunctionTimeoutError\n```python\nclass FunctionTimeoutError(modal.exception.TimeoutError)\n```\nRaised when a Function exceeds its execution duration limit and times out.\n## modal.exception.InvalidError\n```python\nclass InvalidError(modal.exception.Error)\n```\nRaised when user does something invalid.\n## modal.exception.MountUploadTimeoutError\n```python\nclass MountUploadTimeoutError(modal.exception.TimeoutError)\n```\nRaised when a Mount upload times out.\n## modal.exception.NotFoundError\n```python\nclass NotFoundError(modal.exception.Error)\n```\nRaised when a requested resource was not found.\n## modal.exception.PendingDeprecationError\n```python\nclass PendingDeprecationError(UserWarning)\n```\nSoon to be deprecated feature. Only used intermittently because of multi-repo concerns.\n## modal.exception.RemoteError\n```python\nclass RemoteError(modal.exception.Error)\n```\nRaised when an error occurs on the Modal server.\n## modal.exception.SandboxTerminatedError\n```python\nclass SandboxTerminatedError(modal.exception.Error)\n```\nRaised when a Sandbox is terminated for an internal reason.\n## modal.exception.SandboxTimeoutError\n```python\nclass SandboxTimeoutError(modal.exception.TimeoutError)\n```\nRaised when a Sandbox exceeds its execution duration limit and times out.\n## modal.exception.TimeoutError\n```python\nclass TimeoutError(modal.exception.Error)\n```\nBase class for Modal timeouts.\n## modal.exception.VersionError\n```python\nclass VersionError(modal.exception.Error)\n```\nRaised when the current client version of Modal is unsupported.\n## modal.exception.deprecation_error\n```python\ndef deprecation_error(deprecated_on: date, msg: str):\n```\n## modal.exception.deprecation_warning\n```python\ndef deprecation_warning(deprecated_on: date, msg: str, pending=False):\n```\nUtility for getting the proper stack entry.\nSee the implementation of the built-in [warnings.warn](https://docs.python.org/3/library/warnings.html#available-functions).\n"}
{"text": "# modal.shared_volume\n## modal.shared_volume.SharedVolume\n```python\nclass SharedVolume(modal.network_file_system.NetworkFileSystem)\n```\n```python\ndef __init__(self, *args, **kwargs) -> None:\n```\n`SharedVolume(...)` is deprecated. Please use `NetworkFileSystem.new(...)` instead.\n### from_id\n```python\n@classmethod\ndef from_id(cls: Type[O], object_id: str, client: Optional[_Client] = None) -> O:\n```\nGet an object of this type from a unique object id (retrieved from `obj.object_id`)\n### object_id\n```python\n@property\ndef object_id(self):\n```\n### is_hydrated\n```python\ndef is_hydrated(self) -> bool:\n```\n### persist\n```python\ndef persist(\n    self,\n    label: str,\n    namespace=api_pb2.DEPLOYMENT_NAMESPACE_WORKSPACE,\n    environment_name: Optional[str] = None,\n    cloud: Optional[str] = None,\n) -> \"_NetworkFileSystem\":\n```\n`NetworkFileSystem().persist(\"my-volume\")` is deprecated. Use `NetworkFileSystem.persisted(\"my-volume\")` instead.\n### from_name\n```python\n@classmethod\ndef from_name(\n    cls: Type[O],\n    app_name: str,\n    tag: Optional[str] = None,\n    namespace=api_pb2.DEPLOYMENT_NAMESPACE_WORKSPACE,\n    environment_name: Optional[str] = None,\n) -> O:\n```\nReturns a reference to an Modal object of any type\nUseful for referring to already created/deployed objects, e.g., Secrets\n```python\nimport modal\nstub = modal.Stub()\n@stub.function(secret=modal.Secret.from_name(\"my-secret-name\"))\ndef some_function():\n    pass\n```\n### lookup\n```python\n@classmethod\ndef lookup(\n    cls: Type[O],\n    app_name: str,\n    tag: Optional[str] = None,\n    namespace=api_pb2.DEPLOYMENT_NAMESPACE_WORKSPACE,\n    client: Optional[_Client] = None,\n    environment_name: Optional[str] = None,\n) -> O:\n```\nGeneral purpose method to retrieve Modal objects such as functions, network file systems, and secrets.\n```python notest\nimport modal\nsquare = modal.Function.lookup(\"my-shared-app\", \"square\")\nassert square(3) == 9\nnfs = modal.NetworkFileSystem.lookup(\"my-nfs\")\nfor chunk in nfs.read_file(\"my_db_dump.csv\"):\n    ...\n```\n### new\n```python\n@staticmethod\ndef new(*args, **kwargs) -> \"_NetworkFileSystem\":\n```\n`SharedVolume.new(...)` is deprecated. Please use `NetworkFileSystem.new(...)` instead.\n### persisted\n```python\n@staticmethod\ndef persisted(*args, **kwargs) -> _NetworkFileSystem:\n```\n`SharedVolume.persisted(...)` is deprecated. Please use `NetworkFileSystem.persisted(...)` instead.\n### write_file\n```python\ndef write_file(self, remote_path: str, fp: BinaryIO) -> int:\n```\nWrite from a file object to a path on the network file system, atomically.\nWill create any needed parent directories automatically.\nIf remote_path ends with `/` it's assumed to be a directory and the\nfile will be uploaded with its current name to that directory.\n### read_file\n```python\ndef read_file(self, path: str) -> AsyncIterator[bytes]:\n```\nRead a file from the network file system\n### iterdir\n```python\ndef iterdir(self, path: str) -> AsyncIterator[api_pb2.SharedVolumeListFilesEntry]:\n```\nIterate over all files in a directory in the network file system.\n* Passing a directory path lists all files in the directory (names are relative to the directory)\n* Passing a file path returns a list containing only that file's listing description\n* Passing a glob path (including at least one * or ** sequence) returns all files matching that glob path (using absolute paths)\n### add_local_file\n```python\ndef add_local_file(\n    self, local_path: Union[Path, str], remote_path: Optional[Union[str, PurePosixPath, None]] = None\n):\n```\n### add_local_dir\n```python\ndef add_local_dir(\n    self,\n    local_path: Union[Path, str],\n    remote_path: Optional[Union[str, PurePosixPath, None]] = None,\n):\n```\n### listdir\n```python\ndef listdir(self, path: str) -> List[api_pb2.SharedVolumeListFilesEntry]:\n```\nList all files in a directory in the network file system.\n* Passing a directory path lists all files in the directory (names are relative to the directory)\n* Passing a file path returns a list containing only that file's listing description\n* Passing a glob path (including at least one * or ** sequence) returns all files matching that glob path (using absolute paths)\n### remove_file\n```python\ndef remove_file(self, path: str, recursive=False):\n```\nRemove a file in a network file system.\n"}
{"text": "# modal.app\n## modal.app.App\n```python\nclass App(object)\n```\nApps are the user representation of an actively running Modal process.\nYou can obtain an `App` from the `Stub.run()` context manager. While the app\nis running, you can get its `app_id`, `client`, and other useful properties\nfrom this object.\n```python\nimport modal\nstub = modal.Stub()\nstub.my_secret_object = modal.Secret.from_name(\"my-secret\")\nif __name__ == \"__main__\":\n    with stub.run() as app:\n        print(app.client)\n        print(app.app_id)\n        print(app.my_secret_object)\n```\n### client\n```python\n@property\ndef client(self) -> _Client:\n```\nA reference to the running App's server client.\n### app_id\n```python\n@property\ndef app_id(self) -> str:\n```\nA unique identifier for this running App.\n### disconnect\n```python\ndef disconnect(self):\n```\nTell the server the client has disconnected for this app. Terminates all running tasks\nfor ephemeral apps.\n### stop\n```python\ndef stop(self):\n```\nTell the server to stop this app, terminating all running tasks.\n### log_url\n```python\ndef log_url(self):\n```\n### init_container\n```python\n@staticmethod\ndef init_container(client: _Client, app_id: str, stub_name: str = \"\") -> \"_App\":\n```\nUsed by the container to bootstrap the app and all its objects. Not intended to be called by Modal users.\n### create_one_object\n```python\ndef create_one_object(self, obj: _Object, environment_name: str) -> None:\n```\n### deploy\n```python\ndef deploy(self, name: str, namespace, object_entity: str) -> str:\n```\n### spawn_sandbox\n```python\ndef spawn_sandbox(\n    self,\n    *entrypoint_args: str,\n    image: Optional[\"modal.image._Image\"] = None,  # The image to run as the container for the sandbox.\n    mounts: Sequence[\"modal.mount._Mount\"] = (),  # Mounts to attach to the sandbox.\n    secrets: Sequence[\"modal.secret._Secret\"] = (),  # Environment variables to inject into the sandbox.\n    timeout: Optional[int] = None,  # Maximum execution time of the sandbox in seconds.\n    workdir: Optional[str] = None,  # Working directory of the sandbox.\n    gpu: GPU_T = None,\n    cloud: Optional[str] = None,\n    cpu: Optional[float] = None,  # How many CPU cores to request. This is a soft limit.\n    memory: Optional[int] = None,  # How much memory to request, in MiB. This is a soft limit.\n) -> \"modal.sandbox._Sandbox\":\n```\nSandboxes are a way to run arbitrary commands in dynamically defined environments.\nThis function returns a [SandboxHandle](/docs/reference/modal.Sandbox#modalsandboxsandbox), which can be used to interact with the running sandbox.\nRefer to the [docs](/docs/guide/sandbox) on how to spawn and use sandboxes.\n## modal.app.container_app\nA reference to the running `modal.App`, accessible from within a running Modal function.\nUseful for accessing object handles for any Modal objects declared on the stub, e.g:\n```python\nstub = modal.Stub()\nstub.data = modal.Dict()\n@stub.function()\ndef store_something(key, value):\n    data: modal.Dict = modal.container_app.data\n    data.put(key, value)\n```\n## modal.app.is_local\n```python\ndef is_local() -> bool:\n```\nReturns if we are currently on the machine launching/deploying a Modal app\nReturns `True` when executed locally on the user's machine.\nReturns `False` when executed from a Modal container in the cloud.\n"}
{"text": "# modal.functions\n## modal.functions.Function\n```python\nclass Function(modal.object.Object)\n```\nFunctions are the basic units of serverless execution on Modal.\nGenerally, you will not construct a `Function` directly. Instead, use the\n`@stub.function()` decorator on the `Stub` object for your application.\n```python\ndef __init__(self):\n```\n### from_id\n```python\n@classmethod\ndef from_id(cls: Type[O], object_id: str, client: Optional[_Client] = None) -> O:\n```\nGet an object of this type from a unique object id (retrieved from `obj.object_id`)\n### object_id\n```python\n@property\ndef object_id(self):\n```\n### is_hydrated\n```python\ndef is_hydrated(self) -> bool:\n```\n### persist\n```python\ndef persist(\n    self, label: str, namespace=api_pb2.DEPLOYMENT_NAMESPACE_WORKSPACE, environment_name: Optional[str] = None\n):\n```\n`Object.persist` is deprecated for generic objects. See `NetworkFileSystem.persisted` or `Dict.persisted`.\n### from_name\n```python\n@classmethod\ndef from_name(\n    cls: Type[O],\n    app_name: str,\n    tag: Optional[str] = None,\n    namespace=api_pb2.DEPLOYMENT_NAMESPACE_WORKSPACE,\n    environment_name: Optional[str] = None,\n) -> O:\n```\nReturns a reference to an Modal object of any type\nUseful for referring to already created/deployed objects, e.g., Secrets\n```python\nimport modal\nstub = modal.Stub()\n@stub.function(secret=modal.Secret.from_name(\"my-secret-name\"))\ndef some_function():\n    pass\n```\n### lookup\n```python\n@classmethod\ndef lookup(\n    cls: Type[O],\n    app_name: str,\n    tag: Optional[str] = None,\n    namespace=api_pb2.DEPLOYMENT_NAMESPACE_WORKSPACE,\n    client: Optional[_Client] = None,\n    environment_name: Optional[str] = None,\n) -> O:\n```\nGeneral purpose method to retrieve Modal objects such as functions, network file systems, and secrets.\n```python notest\nimport modal\nsquare = modal.Function.lookup(\"my-shared-app\", \"square\")\nassert square(3) == 9\nnfs = modal.NetworkFileSystem.lookup(\"my-nfs\")\nfor chunk in nfs.read_file(\"my_db_dump.csv\"):\n    ...\n```\n### from_parametrized\n```python\n@staticmethod\ndef from_parametrized(base_function: \"_Function\", *args: Iterable[Any], **kwargs: Dict[str, Any]) -> \"_Function\":\n```\n### stub\n```python\n@property\ndef stub(self) -> \"modal.stub._Stub\":\n```\n### info\n```python\n@property\ndef info(self) -> FunctionInfo:\n```\n### web_url\n```python\n@property\ndef web_url(self) -> str:\n```\nURL of a Function running as a web endpoint.\n### is_generator\n```python\n@property\ndef is_generator(self) -> bool:\n```\n### map\n```python\n@warn_if_generator_is_not_consumed\ndef map(\n    self,\n    *input_iterators,  # one input iterator per argument in the mapped-over function/generator\n    kwargs={},  # any extra keyword arguments for the function\n    order_outputs=None,  # defaults to True for regular functions, False for generators\n    return_exceptions=False,  # whether to propogate exceptions (False) or aggregate them in the results list (True)\n) -> AsyncGenerator[Any, None]:\n```\nParallel map over a set of inputs.\nTakes one iterator argument per argument in the function being mapped over.\nExample:\n```python\n@stub.function()\ndef my_func(a):\n    return a ** 2\n@stub.local_entrypoint()\ndef main():\n    assert list(my_func.map([1, 2, 3, 4])) == [1, 4, 9, 16]\n```\nIf applied to a `stub.function`, `map()` returns one result per input and the output order\nis guaranteed to be the same as the input order. Set `order_outputs=False` to return results\nin the order that they are completed instead.\nIf applied to a `stub.generator`, the results are returned as they are finished and can be\nout of order. By yielding zero or more than once, mapping over generators can also be used\nas a \"flat map\".\n`return_exceptions` can be used to treat exceptions as successful results:\n```python\n@stub.function()\ndef my_func(a):\n    if a == 2:\n        raise Exception(\"ohno\")\n    return a ** 2\n@stub.local_entrypoint()\ndef main():\n    # [0, 1, UserCodeException(Exception('ohno'))]\n    print(list(my_func.map(range(3), return_exceptions=True)))\n```\n### for_each\n```python\ndef for_each(self, *input_iterators, kwargs={}, ignore_exceptions=False):\n```\nExecute function for all outputs, ignoring outputs\nConvenient alias for `.map()` in cases where the function just needs to be called.\nas the caller doesn't have to consume the generator to process the inputs.\n### starmap\n```python\n@warn_if_generator_is_not_consumed\ndef starmap(\n    self, input_iterator, kwargs={}, order_outputs=None, return_exceptions=False\n) -> AsyncGenerator[Any, None]:\n```\nLike `map` but spreads arguments over multiple function arguments\nAssumes every input is a sequence (e.g. a tuple).\nExample:\n```python\n@stub.function()\ndef my_func(a, b):\n    return a + b\n@stub.local_entrypoint()\ndef main():\n    assert list(my_func.starmap([(1, 2), (3, 4)])) == [3, 7]\n```\n### remote\n```python\ndef remote(self, *args, **kwargs) -> Awaitable[Any]:  # TODO: Generics/TypeVars\n```\nCalls the function remotely, executing it with the given arguments and returning the execution's result.\n### remote_gen\n```python\ndef remote_gen(self, *args, **kwargs) -> AsyncGenerator[Any, None]:  # TODO: Generics/TypeVars\n```\nCalls the generator remotely, executing it with the given arguments and returning the execution's result.\n### call\n```python\ndef call(self, *args, **kwargs) -> Awaitable[Any]:  # TODO: Generics/TypeVars\n```\n### shell\n```python\ndef shell(self, *args, **kwargs) -> None:\n```\n### local\n```python\n@synchronizer.nowrap\ndef local(self, *args, **kwargs) -> Any:\n    # TODO(erikbern): it would be nice to remove the nowrap thing, but right now that would cause\n    # \"user code\" to run on the synchronicity thread, which seems bad\n```\n### spawn\n```python\ndef spawn(self, *args, **kwargs) -> Optional[\"_FunctionCall\"]:\n```\nCalls the function with the given arguments, without waiting for the results.\nReturns a `modal.functions.FunctionCall` object, that can later be polled or waited for using `.get(timeout=...)`.\nConceptually similar to `multiprocessing.pool.apply_async`, or a Future/Promise in other contexts.\n*Note:* `.spawn()` on a modal generator function does call and execute the generator, but does not currently\nreturn a function handle for polling the result.\n### get_raw_f\n```python\ndef get_raw_f(self) -> Callable[..., Any]:\n```\nReturn the inner Python object wrapped by this Modal Function.\n### get_current_stats\n```python\ndef get_current_stats(self) -> FunctionStats:\n```\nReturn a `FunctionStats` object describing the current function's queue and runner counts.\n## modal.functions.FunctionCall\n```python\nclass FunctionCall(modal.object.Object)\n```\nA reference to an executed function call.\nConstructed using `.spawn(...)` on a Modal function with the same\narguments that a function normally takes. Acts as a reference to\nan ongoing function call that can be passed around and used to\npoll or fetch function results at some later time.\nConceptually similar to a Future/Promise/AsyncResult in other contexts and languages.\n```python\ndef __init__(self):\n```\n### from_id\n```python\n@classmethod\ndef from_id(cls: Type[O], object_id: str, client: Optional[_Client] = None) -> O:\n```\nGet an object of this type from a unique object id (retrieved from `obj.object_id`)\n### object_id\n```python\n@property\ndef object_id(self):\n```\n### is_hydrated\n```python\ndef is_hydrated(self) -> bool:\n```\n### persist\n```python\ndef persist(\n    self, label: str, namespace=api_pb2.DEPLOYMENT_NAMESPACE_WORKSPACE, environment_name: Optional[str] = None\n):\n```\n`Object.persist` is deprecated for generic objects. See `NetworkFileSystem.persisted` or `Dict.persisted`.\n### from_name\n```python\n@classmethod\ndef from_name(\n    cls: Type[O],\n    app_name: str,\n    tag: Optional[str] = None,\n    namespace=api_pb2.DEPLOYMENT_NAMESPACE_WORKSPACE,\n    environment_name: Optional[str] = None,\n) -> O:\n```\nReturns a reference to an Modal object of any type\nUseful for referring to already created/deployed objects, e.g., Secrets\n```python\nimport modal\nstub = modal.Stub()\n@stub.function(secret=modal.Secret.from_name(\"my-secret-name\"))\ndef some_function():\n    pass\n```\n### lookup\n```python\n@classmethod\ndef lookup(\n    cls: Type[O],\n    app_name: str,\n    tag: Optional[str] = None,\n    namespace=api_pb2.DEPLOYMENT_NAMESPACE_WORKSPACE,\n    client: Optional[_Client] = None,\n    environment_name: Optional[str] = None,\n) -> O:\n```\nGeneral purpose method to retrieve Modal objects such as functions, network file systems, and secrets.\n```python notest\nimport modal\nsquare = modal.Function.lookup(\"my-shared-app\", \"square\")\nassert square(3) == 9\nnfs = modal.NetworkFileSystem.lookup(\"my-nfs\")\nfor chunk in nfs.read_file(\"my_db_dump.csv\"):\n    ...\n```\n### get\n```python\ndef get(self, timeout: Optional[float] = None):\n```\nGet the result of the function call.\nThis function waits indefinitely by default. It takes an optional\n`timeout` argument that specifies the maximum number of seconds to wait,\nwhich can be set to `0` to poll for an output immediately.\nThe returned coroutine is not cancellation-safe.\n### get_call_graph\n```python\ndef get_call_graph(self) -> List[InputInfo]:\n```\nReturns a structure representing the call graph from a given root\ncall ID, along with the status of execution for each node.\nSee [`modal.call_graph`](/docs/reference/modal.call_graph) reference page\nfor documentation on the structure of the returned `InputInfo` items.\n### cancel\n```python\ndef cancel(self):\n```\n## modal.functions.FunctionStats\n```python\nclass FunctionStats(object)\n```\nSimple data structure storing stats for a running function.\n```python\ndef __init__(self, backlog: int, num_active_runners: int, num_total_runners: int) -> None\n```\n## modal.functions.PartialFunction\n```python\nclass PartialFunction(object)\n```\nIntermediate function, produced by @method or @web_endpoint\n```python\ndef __init__(\n    self,\n    raw_f: Callable[..., Any],\n    webhook_config: Optional[api_pb2.WebhookConfig] = None,\n    is_generator: Optional[bool] = None,\n):\n```\n### initialize_cls\n```python\n@staticmethod\ndef initialize_cls(user_cls: type, functions: Dict[str, _Function]):\n```\n### initialize_obj\n```python\n@staticmethod\ndef initialize_obj(user_obj, functions: Dict[str, _Function]):\n```\n## modal.functions.asgi_app\n```python\n@typechecked\ndef asgi_app(\n    label: Optional[str] = None,  # Label for created endpoint. Final subdomain will be <workspace>--<label>.modal.run.\n    wait_for_response: bool = True,  # Whether requests should wait for and return the function response.\n    custom_domains: Optional[\n        Iterable[str]\n    ] = None,  # Create an endpoint using a custom domain fully-qualified domain name.\n) -> Callable[[Callable[..., Any]], _PartialFunction]:\n```\nDecorator for registering an ASGI app with a Modal function.\nAsynchronous Server Gateway Interface (ASGI) is a standard for Python\nsynchronous and asynchronous apps, supported by all popular Python web\nlibraries. This is an advanced decorator that gives full flexibility in\ndefining one or more web endpoints on Modal.\n**Usage:**\n```python\nfrom typing import Callable\n@stub.function()\n@modal.asgi_app()\ndef create_asgi() -> Callable:\n    ...\n```\nTo learn how to use Modal with popular web frameworks, see the\n[guide on web endpoints](https://modal.com/docs/guide/webhooks).\nThe two `wait_for_response` modes for webhooks are as follows:\n* wait_for_response=True - tries to fulfill the request on the original URL, but returns a 302 redirect after ~150s to a result URL (original URL with an added `__modal_function_id=fc-1234abcd` query parameter)\n* wait_for_response=False - immediately returns a 202 ACCEPTED response with a JSON payload: `{\"result_url\": \"...\"}` containing the result \"redirect\" url from above (which in turn redirects to itself every 150s)\n## modal.functions.current_input_id\n```python\ndef current_input_id() -> str:\n```\nReturns the input ID for the currently processed input.\nCan only be called from Modal function (i.e. in a container context).\n```python\nfrom modal import current_input_id\n@stub.function()\ndef process_stuff():\n    print(f\"Starting to process {current_input_id()}\")\n```\n## modal.functions.gather\n```python\nasync def gather(*function_calls: _FunctionCall):\n```\nWait until all Modal function calls have results before returning\nAccepts a variable number of FunctionCall objects as returned by `Function.spawn()`.\nReturns a list of results from each function call, or raises an exception\nof the first failing function call.\nE.g.\n```python notest\nfunction_call_1 = slow_func_1.spawn()\nfunction_call_2 = slow_func_2.spawn()\nresult_1, result_2 = gather(function_call_1, function_call_2)\n```\n## modal.functions.method\n```python\ndef method(\n    *,\n    # Set this to True if it's a non-generator function returning\n    # a [sync/async] generator object\n    is_generator: Optional[bool] = None,\n) -> Callable[[Callable[..., Any]], _PartialFunction]:\n```\nDecorator for methods that should be transformed into a Modal Function registered against this class's stub.\n**Usage:**\n```python\n@stub.cls(cpu=8)\nclass MyCls:\n    @modal.method()\n    def f(self):\n        ...\n```\n## modal.functions.web_endpoint\n```python\n@typechecked\ndef web_endpoint(\n    method: str = \"GET\",  # REST method for the created endpoint.\n    label: Optional[str] = None,  # Label for created endpoint. Final subdomain will be <workspace>--<label>.modal.run.\n    wait_for_response: bool = True,  # Whether requests should wait for and return the function response.\n    custom_domains: Optional[\n        Iterable[str]\n    ] = None,  # Create an endpoint using a custom domain fully-qualified domain name.\n) -> Callable[[Callable[..., Any]], _PartialFunction]:\n```\nRegister a basic web endpoint with this application.\nThis is the simple way to create a web endpoint on Modal. The function\nbehaves as a [FastAPI](https://fastapi.tiangolo.com/) handler and should\nreturn a response object to the caller.\nEndpoints created with `@stub.web_endpoint` are meant to be simple, single\nrequest handlers and automatically have\n[CORS](https://developer.mozilla.org/en-US/docs/Web/HTTP/CORS) enabled.\nFor more flexibility, use `@stub.asgi_app`.\nTo learn how to use Modal with popular web frameworks, see the\n[guide on web endpoints](https://modal.com/docs/guide/webhooks).\nAll webhook requests have a 150s maximum request time for the HTTP request itself. However, the underlying functions can\nrun for longer and return results to the caller on completion.\nThe two `wait_for_response` modes for webhooks are as follows:\n* `wait_for_response=True` - tries to fulfill the request on the original URL, but returns a 302 redirect after ~150s to a result URL (original URL with an added `__modal_function_id=...` query parameter)\n* `wait_for_response=False` - immediately returns a 202 ACCEPTED response with a JSON payload: `{\"result_url\": \"...\"}` containing the result \"redirect\" URL from above (which in turn redirects to itself every ~150s)\n## modal.functions.wsgi_app\n```python\n@typechecked\ndef wsgi_app(\n    label: Optional[str] = None,  # Label for created endpoint. Final subdomain will be <workspace>--<label>.modal.run.\n    wait_for_response: bool = True,  # Whether requests should wait for and return the function response.\n    custom_domains: Optional[\n        Iterable[str]\n    ] = None,  # Create an endpoint using a custom domain fully-qualified domain name.\n) -> Callable[[Callable[..., Any]], _PartialFunction]:\n```\nDecorator for registering a WSGI app with a Modal function.\nWeb Server Gateway Interface (WSGI) is a standard for synchronous Python web apps.\nIt has been [succeeded by the ASGI interface](https://asgi.readthedocs.io/en/latest/introduction.html#wsgi-compatibility) which is compatible with ASGI and supports\nadditional functionality such as web sockets. Modal supports ASGI via [`asgi_app`](/docs/reference/modal.asgi_app).\n**Usage:**\n```python\nfrom typing import Callable\n@stub.function()\n@modal.wsgi_app()\ndef create_wsgi() -> Callable:\n    ...\n```\nTo learn how to use this decorator with popular web frameworks, see the\n[guide on web endpoints](https://modal.com/docs/guide/webhooks).\nFor documentation on this decorator's arguments see [`asgi_app`](/docs/reference/modal.asgi_app).\n"}
{"text": "# modal.Retries\n```python\nclass Retries(object)\n```\nAdds a retry policy to a Modal function.\n**Usage**\n```python\nimport modal\nstub = modal.Stub()\n# Basic configuration.\n# This sets a policy of max 4 retries with 1-second delay between failures.\n@stub.function(retries=4)\ndef f():\n    pass\n# Fixed-interval retries with 3-second delay between failures.\n@stub.function(\n    retries=modal.Retries(\n        max_retries=2,\n        backoff_coefficient=1.0,\n        initial_delay=3.0,\n    )\n)\ndef g():\n    pass\n# Exponential backoff, with retry delay doubling after each failure.\n@stub.function(\n    retries=modal.Retries(\n        max_retries=4,\n        backoff_coefficient=2.0,\n        initial_delay=1.0,\n    )\n)\ndef h():\n    pass\n```\n```python\ndef __init__(\n    self,\n    *,\n    # The maximum number of retries that can be made in the presence of failures.\n    max_retries: int,\n    # Coefficent controlling how much the retry delay increases each retry attempt.\n    # A backoff coefficient of 1.0 creates fixed-delay retries where the delay period will always equal the initial delay.\n    backoff_coefficient: float = 2.0,\n    # Number of seconds that must elapse before the first retry occurs.\n    initial_delay: float = 1.0,\n    # Maximum length of retry delay in seconds, preventing the delay from growing infinitely.\n    max_delay: float = 60.0,\n):\n```\nConstruct a new retries policy, supporting exponential and fixed-interval delays via a backoff coefficient.\n"}
{"text": "# modal.Client\n```python\nclass Client(object)\n```\n```python\ndef __init__(\n    self,\n    server_url,\n    client_type,\n    credentials,\n    version=__version__,\n    *,\n    no_verify=False,\n):\n```\n## stub\n```python\n@property\ndef stub(self):\n```\n## verify\n```python\n@classmethod\ndef verify(cls, server_url, credentials):\n```\n## unauthenticated_client\n```python\n@classmethod\ndef unauthenticated_client(cls, server_url: str):\n    # Create a connection with no credentials\n    # To be used with the token flow\n```\n## start_token_flow\n```python\ndef start_token_flow(self, utm_source: Optional[str] = None) -> Tuple[str, str]:\n    # Create token creation request\n    # Send some strings identifying the computer (these are shown to the user for security reasons)\n```\n## finish_token_flow\n```python\ndef finish_token_flow(self, token_flow_id) -> Tuple[str, str]:\n    # Wait for token forever\n```\n## from_env\n```python\n@classmethod\ndef from_env(cls, _override_config=None) -> \"_Client\":\n```\n## set_env_client\n```python\n@classmethod\ndef set_env_client(cls, client):\n```\nJust used from tests.\n"}
{"text": "# modal.Stub\n```python\nclass Stub(object)\n```\nA `Stub` is a description of how to create a Modal application.\nThe stub object principally describes Modal objects (`Function`, `Image`,\n`Secret`, etc.) associated with the application. It has three responsibilities:\n* Syncing of identities across processes (your local Python interpreter and\n  every Modal worker active in your application).\n* Making Objects stay alive and not be garbage collected for as long as the\n  app lives (see App lifetime below).\n* Manage log collection for everything that happens inside your code.\n**Registering functions with an app**\nThe most common way to explicitly register an Object with an app is through the\n`@stub.function()` decorator. It both registers the annotated function itself and\nother passed objects, like schedules and secrets, with the app:\n```python\nimport modal\nstub = modal.Stub()\n@stub.function(\n    secret=modal.Secret.from_name(\"some_secret\"),\n    schedule=modal.Period(days=1),\n)\ndef foo():\n    pass\n```\nIn this example, the secret and schedule are registered with the app.\n```python\n@typechecked\ndef __init__(\n    self,\n    name: Optional[str] = None,\n    *,\n    image: Optional[_Image] = None,  # default image for all functions (default is `modal.Image.debian_slim()`)\n    mounts: Sequence[_Mount] = [],  # default mounts for all functions\n    secrets: Sequence[_Secret] = [],  # default secrets for all functions\n    **blueprint: _Object,  # any Modal Object dependencies (Dict, Queue, etc.)\n) -> None:\n```\nConstruct a new app stub, optionally with default image, mounts, secrets\nAny \"blueprint\" objects are loaded as part of running or deploying the app,\nand are accessible by name on the running container app, e.g.:\n```python\nstub = modal.Stub(key_value_store=modal.Dict())\n@stub.function()\ndef store_something(key: str, value: str):\n    stub.app.key_value_store.put(key, value)\n```\n## name\n```python\n@property\ndef name(self) -> Optional[str]:\n```\nThe user-provided name of the Stub.\n## app\n```python\n@property\ndef app(self) -> Optional[_App]:\n```\nReference to the currently running app, if any.\n## description\n```python\n@property\ndef description(self) -> Optional[str]:\n```\nThe Stub's `name`, if available, or a fallback descriptive identifier.\n## set_description\n```python\ndef set_description(self, description: str):\n```\n## get_objects\n```python\ndef get_objects(self) -> List[Tuple[str, _Object]]:\n```\nUsed by the container app to initialize objects.\n## is_inside\n```python\n@typechecked\ndef is_inside(self, image: Optional[_Image] = None) -> bool:\n```\nReturns if the program is currently running inside a container for this app.\n## run\n```python\n@asynccontextmanager\ndef run(\n    self,\n    client: Optional[_Client] = None,\n    stdout=None,\n    show_progress: bool = True,\n    detach: bool = False,\n    output_mgr: Optional[OutputManager] = None,\n) -> AsyncGenerator[_App, None]:\n```\nContext manager that runs an app on Modal.\nUse this as the main entry point for your Modal application. All calls\nto Modal functions should be made within the scope of this context\nmanager, and they will correspond to the current app.\nSee the documentation for the [`App`](modal.App) class for more details.\n## registered_functions\n```python\n@property\ndef registered_functions(self) -> Dict[str, _Function]:\n```\nAll modal.Function objects registered on the stub.\n## registered_entrypoints\n```python\n@property\ndef registered_entrypoints(self) -> Dict[str, _LocalEntrypoint]:\n```\nAll local CLI entrypoints registered on the stub.\n## registered_web_endpoints\n```python\n@property\ndef registered_web_endpoints(self) -> List[str]:\n```\nNames of web endpoint (ie. webhook) functions registered on the stub.\n## local_entrypoint\n```python\ndef local_entrypoint(self, name: Optional[str] = None) -> Callable[[Callable[..., Any]], None]:\n```\nDecorate a function to be used as a CLI entrypoint for a Modal App.\nThese functions can be used to define code that runs locally to set up the app,\nand act as an entrypoint to start Modal functions from. Note that regular\nModal functions can also be used as CLI entrypoints, but unlike `local_entrypoint`,\nthose functions are executed remotely directly.\n**Example**\n```python\n@stub.local_entrypoint()\ndef main():\n    some_modal_function.call()\n```\nYou can call the function using `modal run` directly from the CLI:\n```shell\nmodal run stub_module.py\n```\nNote that an explicit [`stub.run()`](/docs/reference/modal.Stub#run) is not needed, as an\n[app](/docs/guide/apps) is automatically created for you.\n**Multiple Entrypoints**\nIf you have multiple `local_entrypoint` functions, you can qualify the name of your stub and function:\n```shell\nmodal run stub_module.py::stub.some_other_function\n```\n**Parsing Arguments**\nIf your entrypoint function take arguments with primitive types, `modal run` automatically parses them as\nCLI options. For example, the following function can be called with `modal run stub_module.py --foo 1 --bar \"hello\"`:\n```python\n@stub.local_entrypoint()\ndef main(foo: int, bar: str):\n    some_modal_function.call(foo, bar)\n```\nCurrently, `str`, `int`, `float`, `bool`, and `datetime.datetime` are supported. Use `modal run stub_module.py --help` for more\ninformation on usage.\n## function\n```python\n@typechecked\ndef function(\n    self,\n    image: Optional[_Image] = None,  # The image to run as the container for the function\n    schedule: Optional[Schedule] = None,  # An optional Modal Schedule for the function\n    secret: Optional[_Secret] = None,  # An optional Modal Secret with environment variables for the container\n    secrets: Sequence[_Secret] = (),  # Plural version of `secret` when multiple secrets are needed\n    gpu: GPU_T = None,  # GPU specification as string (\"any\", \"T4\", \"A10G\", ...) or object (`modal.GPU.A100()`, ...)\n    serialized: bool = False,  # Whether to send the function over using cloudpickle.\n    mounts: Sequence[_Mount] = (),\n    shared_volumes: Dict[\n        Union[str, os.PathLike], _NetworkFileSystem\n    ] = {},  # Deprecated, use `network_file_systems` instead\n    network_file_systems: Dict[Union[str, os.PathLike], _NetworkFileSystem] = {},\n    allow_cross_region_volumes: bool = False,  # Whether using network file systems from other regions is allowed.\n    volumes: Dict[Union[str, os.PathLike], _Volume] = {},  # Experimental. Do not use!\n    cpu: Optional[float] = None,  # How many CPU cores to request. This is a soft limit.\n    memory: Optional[int] = None,  # How much memory to request, in MiB. This is a soft limit.\n    proxy: Optional[_Proxy] = None,  # Reference to a Modal Proxy to use in front of this function.\n    retries: Optional[Union[int, Retries]] = None,  # Number of times to retry each input in case of failure.\n    concurrency_limit: Optional[\n        int\n    ] = None,  # An optional maximum number of concurrent containers running the function (use keep_warm for minimum).\n    allow_concurrent_inputs: Optional[int] = None,  # Number of inputs the container may fetch to run concurrently.\n    container_idle_timeout: Optional[int] = None,  # Timeout for idle containers waiting for inputs to shut down.\n    timeout: Optional[int] = None,  # Maximum execution time of the function in seconds.\n    interactive: bool = False,  # Whether to run the function in interactive mode./\n    keep_warm: Optional[\n        int\n    ] = None,  # An optional minimum number of containers to always keep warm (use concurrency_limit for maximum).\n    name: Optional[str] = None,  # Sets the Modal name of the function within the stub\n    is_generator: Optional[\n        bool\n    ] = None,  # Set this to True if it's a non-generator function returning a [sync/async] generator object\n    cloud: Optional[str] = None,  # Cloud provider to run the function on. Possible values are aws, gcp, oci, auto.\n) -> Callable[..., _Function]:\n```\nDecorator to register a new Modal function with this stub.\n## cls\n```python\ndef cls(\n    self,\n    image: Optional[_Image] = None,  # The image to run as the container for the function\n    secret: Optional[_Secret] = None,  # An optional Modal Secret with environment variables for the container\n    secrets: Sequence[_Secret] = (),  # Plural version of `secret` when multiple secrets are needed\n    gpu: GPU_T = None,  # GPU specification as string (\"any\", \"T4\", \"A10G\", ...) or object (`modal.GPU.A100()`, ...)\n    serialized: bool = False,  # Whether to send the function over using cloudpickle.\n    mounts: Sequence[_Mount] = (),\n    shared_volumes: Dict[\n        Union[str, os.PathLike], _NetworkFileSystem\n    ] = {},  # Deprecated, use `network_file_systems` instead\n    network_file_systems: Dict[Union[str, os.PathLike], _NetworkFileSystem] = {},\n    allow_cross_region_volumes: bool = False,  # Whether using network file systems from other regions is allowed.\n    volumes: Dict[Union[str, os.PathLike], _Volume] = {},  # Experimental. Do not use!\n    cpu: Optional[float] = None,  # How many CPU cores to request. This is a soft limit.\n    memory: Optional[int] = None,  # How much memory to request, in MiB. This is a soft limit.\n    proxy: Optional[_Proxy] = None,  # Reference to a Modal Proxy to use in front of this function.\n    retries: Optional[Union[int, Retries]] = None,  # Number of times to retry each input in case of failure.\n    concurrency_limit: Optional[int] = None,  # Limit for max concurrent containers running the function.\n    allow_concurrent_inputs: Optional[int] = None,  # Number of inputs the container may fetch to run concurrently.\n    container_idle_timeout: Optional[int] = None,  # Timeout for idle containers waiting for inputs to shut down.\n    timeout: Optional[int] = None,  # Maximum execution time of the function in seconds.\n    interactive: bool = False,  # Whether to run the function in interactive mode.\n    keep_warm: Optional[int] = None,  # An optional number of containers to always keep warm.\n    cloud: Optional[str] = None,  # Cloud provider to run the function on. Possible values are aws, gcp, oci, auto.\n) -> Callable[[CLS_T], CLS_T]:\n```\n"}
{"text": "# modal.create_package_mounts\n```python\n@typechecked\ndef create_package_mounts(module_names: Sequence[str]) -> List[_Mount]:\n```\n"}
{"text": "# modal.Proxy\n```python\nclass Proxy(modal.object.Object)\n```\nProxy objects are used to setup secure tunnel connections to a private remote address, for example\na database.\nCurrently `modal.Proxy` objects must be setup with the assistance of Modal staff. If you require a proxy\nplease contact us.\n```python\ndef __init__(self):\n```\n## from_id\n```python\n@classmethod\ndef from_id(cls: Type[O], object_id: str, client: Optional[_Client] = None) -> O:\n```\nGet an object of this type from a unique object id (retrieved from `obj.object_id`)\n## object_id\n```python\n@property\ndef object_id(self):\n```\n## is_hydrated\n```python\ndef is_hydrated(self) -> bool:\n```\n## persist\n```python\ndef persist(\n    self, label: str, namespace=api_pb2.DEPLOYMENT_NAMESPACE_WORKSPACE, environment_name: Optional[str] = None\n):\n```\n`Object.persist` is deprecated for generic objects. See `NetworkFileSystem.persisted` or `Dict.persisted`.\n## from_name\n```python\n@classmethod\ndef from_name(\n    cls: Type[O],\n    app_name: str,\n    tag: Optional[str] = None,\n    namespace=api_pb2.DEPLOYMENT_NAMESPACE_WORKSPACE,\n    environment_name: Optional[str] = None,\n) -> O:\n```\nReturns a reference to an Modal object of any type\nUseful for referring to already created/deployed objects, e.g., Secrets\n```python\nimport modal\nstub = modal.Stub()\n@stub.function(secret=modal.Secret.from_name(\"my-secret-name\"))\ndef some_function():\n    pass\n```\n## lookup\n```python\n@classmethod\ndef lookup(\n    cls: Type[O],\n    app_name: str,\n    tag: Optional[str] = None,\n    namespace=api_pb2.DEPLOYMENT_NAMESPACE_WORKSPACE,\n    client: Optional[_Client] = None,\n    environment_name: Optional[str] = None,\n) -> O:\n```\nGeneral purpose method to retrieve Modal objects such as functions, network file systems, and secrets.\n```python notest\nimport modal\nsquare = modal.Function.lookup(\"my-shared-app\", \"square\")\nassert square(3) == 9\nnfs = modal.NetworkFileSystem.lookup(\"my-nfs\")\nfor chunk in nfs.read_file(\"my_db_dump.csv\"):\n    ...\n```\n"}
{"text": "# modal.queue\n## modal.queue.Queue\n```python\nclass Queue(modal.object.Object)\n```\nA distributed, FIFO Queue available to Modal apps.\nThe queue can contain any object serializable by `cloudpickle`.\n```python\nstub.some_queue = modal.Queue.new()\nif __name__ == \"__main__\":\n    with stub.run():\n        stub.some_queue.put({\"some\": \"object\"})\n```\n```python\ndef __init__(self):\n```\n`Queue({...})` is deprecated. Please use `Queue.new({...})` instead.\n### from_id\n```python\n@classmethod\ndef from_id(cls: Type[O], object_id: str, client: Optional[_Client] = None) -> O:\n```\nGet an object of this type from a unique object id (retrieved from `obj.object_id`)\n### object_id\n```python\n@property\ndef object_id(self):\n```\n### is_hydrated\n```python\ndef is_hydrated(self) -> bool:\n```\n### persist\n```python\ndef persist(\n    self, label: str, namespace=api_pb2.DEPLOYMENT_NAMESPACE_WORKSPACE, environment_name: Optional[str] = None\n):\n```\n`Object.persist` is deprecated for generic objects. See `NetworkFileSystem.persisted` or `Dict.persisted`.\n### from_name\n```python\n@classmethod\ndef from_name(\n    cls: Type[O],\n    app_name: str,\n    tag: Optional[str] = None,\n    namespace=api_pb2.DEPLOYMENT_NAMESPACE_WORKSPACE,\n    environment_name: Optional[str] = None,\n) -> O:\n```\nReturns a reference to an Modal object of any type\nUseful for referring to already created/deployed objects, e.g., Secrets\n```python\nimport modal\nstub = modal.Stub()\n@stub.function(secret=modal.Secret.from_name(\"my-secret-name\"))\ndef some_function():\n    pass\n```\n### lookup\n```python\n@classmethod\ndef lookup(\n    cls: Type[O],\n    app_name: str,\n    tag: Optional[str] = None,\n    namespace=api_pb2.DEPLOYMENT_NAMESPACE_WORKSPACE,\n    client: Optional[_Client] = None,\n    environment_name: Optional[str] = None,\n) -> O:\n```\nGeneral purpose method to retrieve Modal objects such as functions, network file systems, and secrets.\n```python notest\nimport modal\nsquare = modal.Function.lookup(\"my-shared-app\", \"square\")\nassert square(3) == 9\nnfs = modal.NetworkFileSystem.lookup(\"my-nfs\")\nfor chunk in nfs.read_file(\"my_db_dump.csv\"):\n    ...\n```\n### new\n```python\n@staticmethod\ndef new():\n```\n### persisted\n```python\n@staticmethod\ndef persisted(\n    label: str, namespace=api_pb2.DEPLOYMENT_NAMESPACE_WORKSPACE, environment_name: Optional[str] = None\n) -> \"_Queue\":\n```\n### get\n```python\ndef get(self, block: bool = True, timeout: Optional[float] = None) -> Optional[Any]:\n```\nRemove and return the next object in the queue.\nIf `block` is `True` (the default) and the queue is empty, `get` will wait indefinitely for\nan object, or until `timeout` if a `timeout` is specified. Raises the native Python `queue.Empty`\nexception if the `timeout` is reached.\nIf `block` is `False`, `get` returns `None` immediately if the queue is empty. The `timeout` is\nignored in this case.\n### get_many\n```python\ndef get_many(self, n_values: int, block: bool = True, timeout: Optional[float] = None) -> List[Any]:\n```\nRemove and return up to `n_values` objects from the queue.\nIf `block` is `True` (the default) and the queue is empty, `get` will wait indefinitely for\nthe next object, or until `timeout` if a `timeout` is specified. Raises the native Python `queue.Empty`\nexception if the `timeout` is reached. Returns as many objects as are available (less then `n_values`)\nas soon as the queue becomes non-empty.\nIf `block` is `False`, `get` returns `None` immediately if the queue is empty. The `timeout` is\nignored in this case.\n### put\n```python\ndef put(self, v: Any) -> None:\n```\nAdd an object to the end of the queue.\n### put_many\n```python\ndef put_many(self, vs: List[Any]) -> None:\n```\nAdd several objects to the end of the queue.\n### len\n```python\ndef len(self) -> int:\n```\nReturn the number of objects in the queue.\n"}
{"text": "# modal.Image\n```python\nclass Image(modal.object.Object)\n```\nBase class for container images to run functions in.\nDo not construct this class directly; instead use one of its static factory methods,\nsuch as `modal.Image.debian_slim`, `modal.Image.from_registry`, or `modal.Image.conda`.\n```python\ndef __init__(self):\n```\n## from_id\n```python\n@classmethod\ndef from_id(cls: Type[O], object_id: str, client: Optional[_Client] = None) -> O:\n```\nGet an object of this type from a unique object id (retrieved from `obj.object_id`)\n## object_id\n```python\n@property\ndef object_id(self):\n```\n## is_hydrated\n```python\ndef is_hydrated(self) -> bool:\n```\n## persist\n```python\ndef persist(\n    self, label: str, namespace=api_pb2.DEPLOYMENT_NAMESPACE_WORKSPACE, environment_name: Optional[str] = None\n):\n```\n`Object.persist` is deprecated for generic objects. See `NetworkFileSystem.persisted` or `Dict.persisted`.\n## from_name\n```python\n@classmethod\ndef from_name(\n    cls: Type[O],\n    app_name: str,\n    tag: Optional[str] = None,\n    namespace=api_pb2.DEPLOYMENT_NAMESPACE_WORKSPACE,\n    environment_name: Optional[str] = None,\n) -> O:\n```\nReturns a reference to an Modal object of any type\nUseful for referring to already created/deployed objects, e.g., Secrets\n```python\nimport modal\nstub = modal.Stub()\n@stub.function(secret=modal.Secret.from_name(\"my-secret-name\"))\ndef some_function():\n    pass\n```\n## lookup\n```python\n@classmethod\ndef lookup(\n    cls: Type[O],\n    app_name: str,\n    tag: Optional[str] = None,\n    namespace=api_pb2.DEPLOYMENT_NAMESPACE_WORKSPACE,\n    client: Optional[_Client] = None,\n    environment_name: Optional[str] = None,\n) -> O:\n```\nGeneral purpose method to retrieve Modal objects such as functions, network file systems, and secrets.\n```python notest\nimport modal\nsquare = modal.Function.lookup(\"my-shared-app\", \"square\")\nassert square(3) == 9\nnfs = modal.NetworkFileSystem.lookup(\"my-nfs\")\nfor chunk in nfs.read_file(\"my_db_dump.csv\"):\n    ...\n```\n## extend\n```python\ndef extend(self, **kwargs) -> \"_Image\":\n```\nExtend an image (named \"base\") with additional options or commands.\nThis is a low-level command. Generally, you should prefer using functions\nlike `Image.pip_install` or `Image.apt_install` if possible.\n**Example**\n```python notest\nimage = modal.Image.debian_slim().extend(\n    dockerfile_commands=[\n        \"FROM base\",\n        \"WORKDIR /pkg\",\n        'RUN echo \"hello world\" > hello.txt',\n    ],\n    secrets=[secret1, secret2],\n)\n```\n## copy_mount\n```python\n@typechecked\ndef copy_mount(self, mount: _Mount, remote_path: Union[str, Path] = \".\") -> \"_Image\":\n```\nCopy the entire contents of a `modal.Mount` into an image.\nUseful when files only available locally are required during the image\nbuild process.\n**Example**\n```python\nstatic_images_dir = \"./static\"\n# place all static images in root of mount\nmount = modal.Mount.from_local_dir(static_images_dir, remote_path=\"/\")\n# place mount's contents into /static directory of image.\nimage = modal.Image.debian_slim().copy_mount(mount, remote_path=\"/static\")\n```\n## copy_local_file\n```python\ndef copy_local_file(self, local_path: Union[str, Path], remote_path: Union[str, Path] = \"./\") -> \"_Image\":\n```\nCopy a file into the image as a part of building it.\nThis works in a similar way to `COPY` in a `Dockerfile`.\n## copy_local_dir\n```python\ndef copy_local_dir(self, local_path: Union[str, Path], remote_path: Union[str, Path] = \".\") -> \"_Image\":\n```\nCopy a directory into the image as a part of building the image.\nThis works in a similar way to `COPY` in a `Dockerfile`.\n## pip_install\n```python\n@typechecked\ndef pip_install(\n    self,\n    *packages: Union[str, List[str]],  # A list of Python packages, eg. [\"numpy\", \"matplotlib>=3.5.0\"]\n    find_links: Optional[str] = None,  # Passes -f (--find-links) pip install\n    index_url: Optional[str] = None,  # Passes -i (--index-url) to pip install\n    extra_index_url: Optional[str] = None,  # Passes --extra-index-url to pip install\n    pre: bool = False,  # Passes --pre (allow pre-releases) to pip install\n    force_build: bool = False,\n    secrets: Sequence[_Secret] = [],\n    gpu: GPU_T = None,\n) -> \"_Image\":\n```\nInstall a list of Python packages using pip.\n**Example**\n```python\nimage = modal.Image.debian_slim().pip_install(\"click\", \"httpx~=0.23.3\")\n```\n## pip_install_private_repos\n```python\n@typechecked\ndef pip_install_private_repos(\n    self,\n    *repositories: str,\n    git_user: str,\n    gpu: GPU_T = None,\n    secrets: Sequence[_Secret] = [],\n    force_build: bool = False,\n) -> \"_Image\":\n```\nInstall a list of Python packages from private git repositories using pip.\nThis method currently supports Github and Gitlab only.\n- **Github:** Provide a `modal.Secret` that contains a `GITHUB_TOKEN` key-value pair\n- **Gitlab:** Provide a `modal.Secret` that contains a `GITLAB_TOKEN` key-value pair\nThese API tokens should have permissions to read the list of private repositories provided as arguments.\nWe recommend using Github's ['fine-grained' access tokens](https://github.blog/2022-10-18-introducing-fine-grained-personal-access-tokens-for-github/).\nThese tokens are repo-scoped, and avoid granting read permission across all of a user's private repos.\n**Example**\n```python\nimage = (\n    modal.Image\n    .debian_slim()\n    .pip_install_private_repos(\n        \"github.com/ecorp/private-one@1.0.0\",\n        \"github.com/ecorp/private-two@main\"\n        \"github.com/ecorp/private-three@d4776502\"\n        # install from 'inner' directory on default branch.\n        \"github.com/ecorp/private-four#subdirectory=inner\",\n        git_user=\"erikbern\",\n        secrets=[modal.Secret.from_name(\"github-read-private\")],\n    )\n)\n```\n## pip_install_from_requirements\n```python\n@typechecked\ndef pip_install_from_requirements(\n    self,\n    requirements_txt: str,  # Path to a requirements.txt file.\n    find_links: Optional[str] = None,\n    force_build: bool = False,\n    *,\n    secrets: Sequence[_Secret] = [],\n    gpu: GPU_T = None,\n) -> \"_Image\":\n```\nInstall a list of Python packages from a local `requirements.txt` file.\n## pip_install_from_pyproject\n```python\n@typechecked\ndef pip_install_from_pyproject(\n    self,\n    pyproject_toml: str,\n    optional_dependencies: List[str] = [],\n    force_build: bool = False,\n    *,\n    secrets: Sequence[_Secret] = [],\n    gpu: GPU_T = None,\n) -> \"_Image\":\n```\nInstall dependencies specified by a local `pyproject.toml` file.\n`optional_dependencies` is a list of the keys of the\noptional-dependencies section(s) of the `pyproject.toml` file\n(e.g. test, doc, experiment, etc). When provided,\nall of the packages in each listed section are installed as well.\n## poetry_install_from_file\n```python\n@typechecked\ndef poetry_install_from_file(\n    self,\n    poetry_pyproject_toml: str,\n    poetry_lockfile: Optional[\n        str\n    ] = None,  # Path to the lockfile. If not provided, uses poetry.lock in the same directory.\n    ignore_lockfile: bool = False,  # If set to True, it will not use poetry.lock\n    old_installer: bool = False,  # If set to True, use old installer. See https://github.com/python-poetry/poetry/issues/3336\n    force_build: bool = False,\n    with_: List[str] = [],\n    without: List[str] = [],\n    only: List[str] = [],\n    *,\n    secrets: Sequence[_Secret] = [],\n    gpu: GPU_T = None,\n) -> \"_Image\":\n```\nInstall poetry *dependencies* specified by a local `pyproject.toml` file.\nIf not provided as argument the path to the lockfile is inferred. However, the\nfile has to exist, unless `ignore_lockfile` is set to `True`.\nNote that the root project of the poetry project is not installed,\nonly the dependencies. For including local packages see `modal.Mount.from_local_python_packages`\n## dockerfile_commands\n```python\n@typechecked\ndef dockerfile_commands(\n    self,\n    dockerfile_commands: Union[str, List[str]],\n    context_files: Dict[str, str] = {},\n    secrets: Sequence[_Secret] = [],\n    gpu: GPU_T = None,\n    context_mount: Optional[\n        _Mount\n    ] = None,  # modal.Mount with local files to supply as build context for COPY commands\n    force_build: bool = False,\n) -> \"_Image\":\n```\nExtend an image with arbitrary Dockerfile-like commands.\n## run_commands\n```python\n@typechecked\ndef run_commands(\n    self,\n    *commands: Union[str, List[str]],\n    secrets: Sequence[_Secret] = [],\n    gpu: GPU_T = None,\n    force_build: bool = False,\n) -> \"_Image\":\n```\nExtend an image with a list of shell commands to run.\n## conda\n```python\n@staticmethod\n@typechecked\ndef conda(python_version: str = \"3.9\", force_build: bool = False) -> \"_Image\":\n```\nA Conda base image, using miniconda3 and derived from the official Docker Hub image.\n## conda_install\n```python\n@typechecked\ndef conda_install(\n    self,\n    *packages: Union[str, List[str]],  # A list of Python packages, eg. [\"numpy\", \"matplotlib>=3.5.0\"]\n    channels: List[str] = [],  # A list of Conda channels, eg. [\"conda-forge\", \"nvidia\"]\n    force_build: bool = False,\n    secrets: Sequence[_Secret] = [],\n    gpu: GPU_T = None,\n) -> \"_Image\":\n```\nInstall a list of additional packages using Conda. Note that in most cases, using `Image.micromamba()`\nis recommended over `Image.conda()`, as it leads to significantly faster image build times.\n## conda_update_from_environment\n```python\n@typechecked\ndef conda_update_from_environment(\n    self,\n    environment_yml: str,\n    force_build: bool = False,\n    *,\n    secrets: Sequence[_Secret] = [],\n    gpu: GPU_T = None,\n) -> \"_Image\":\n```\nUpdate a Conda environment using dependencies from a given environment.yml file.\n## micromamba\n```python\n@staticmethod\n@typechecked\ndef micromamba(\n    python_version: str = \"3.9\",\n    force_build: bool = False,\n) -> \"_Image\":\n```\nA Micromamba base image. Micromamba allows for fast building of small Conda-based containers.\n## micromamba_install\n```python\n@typechecked\ndef micromamba_install(\n    self,\n    *packages: Union[str, List[str]],  # A list of Python packages, eg. [\"numpy\", \"matplotlib>=3.5.0\"]\n    channels: List[str] = [],  # A list of Conda channels, eg. [\"conda-forge\", \"nvidia\"]\n    force_build: bool = False,\n    secrets: Sequence[_Secret] = [],\n    gpu: GPU_T = None,\n) -> \"_Image\":\n```\nInstall a list of additional packages using micromamba.\n## from_registry\n```python\n@staticmethod\n@typechecked\ndef from_registry(\n    tag: str,\n    setup_dockerfile_commands: List[str] = [],\n    force_build: bool = False,\n    **kwargs,\n) -> \"_Image\":\n```\nBuild a Modal image from a pre-existing image on Docker Hub.\nThis assumes the following about the image:\n- Python 3.7 or above is present, and is available as `python`.\n- `pip` is installed correctly.\n- The image is built for the `linux/amd64` platform.\nYou may use `setup_dockerfile_commands` to run Dockerfile commands\nbefore the remaining commands run. This might be useful if Python or pip is\nnot installed, or you need to set a `SHELL` for `python` to be available.\n**Example**\n```python\nmodal.Image.from_registry(\n  \"gisops/valhalla:latest\",\n  setup_dockerfile_commands=[\"RUN apt-get update\", \"RUN apt-get install -y python3-pip\"]\n)\n```\n## from_dockerhub\n```python\n@staticmethod\n@typechecked\ndef from_dockerhub(\n    tag: str,\n    setup_dockerfile_commands: List[str] = [],\n    force_build: bool = False,\n    **kwargs,\n) -> \"_Image\":\n```\n## from_gcp_artifact_registry\n```python\n@staticmethod\n@typechecked\ndef from_gcp_artifact_registry(\n    tag: str,\n    secret: Optional[_Secret] = None,\n    setup_dockerfile_commands: List[str] = [],\n    force_build: bool = False,\n    **kwargs,\n) -> \"_Image\":\n```\nBuild a Modal image from a pre-existing image in GCP Artifact Registry.\nYou will need to pass a `modal.Secret` containing your GCP service account key\nas `SERVICE_ACCOUNT_JSON`. This can be done from the [Secrets](/secrets) page.\nThe service account needs to have at least the \"Artifact Registry Reader\" role.\nFor the image, the same assumptions hold as `from_registry`:\n- Python 3.7 or above is present, and is available as `python`.\n- `pip` is installed correctly.\n- The image is built for the `linux/amd64` platform.\nYou may use `setup_dockerfile_commands` to run Dockerfile commands\nbefore the remaining commands run. This might be useful if Python or pip is\nnot installed, or you need to set a `SHELL` for `python` to be available.\n**Example**\n```python\nmodal.Image.from_gcp_artifact_registry(\n  \"us-east1-docker.pkg.dev/my-project-1234/my-repo/my-image:my-version\",\n  secret=modal.Secret.from_name(\"my-gcp-secret\"),\n  setup_dockerfile_commands=[\"RUN apt-get update\", \"RUN apt-get install -y python3-pip\"]\n)\n```\n## from_aws_ecr\n```python\n@staticmethod\n@typechecked\ndef from_aws_ecr(\n    tag: str,\n    secret: Optional[_Secret] = None,\n    setup_dockerfile_commands: List[str] = [],\n    force_build: bool = False,\n    **kwargs,\n) -> \"_Image\":\n```\nBuild a Modal image from a pre-existing image on a private AWS Elastic\nContainer Registry (ECR). You will need to pass a `modal.Secret` containing\nan AWS key (`AWS_ACCESS_KEY_ID`) and secret (`AWS_SECRET_ACCESS_KEY`)\nwith permissions to access the target ECR registry.\nRefer to [\"Private repository policies\"](https://docs.aws.amazon.com/AmazonECR/latest/userguide/repository-policies.html)\nfor details about IAM configuration.\nThe same assumptions hold from `from_registry`:\n- Python 3.7 or above is present, and is available as `python`.\n- `pip` is installed correctly.\n- The image is built for the `linux/amd64` platform.\nYou may use `setup_dockerfile_commands` to run Dockerfile commands\nbefore the remaining commands run. This might be useful if Python or pip is\nnot installed, or you need to set a `SHELL` for `python` to be available.\n**Example**\n```python\nmodal.Image.from_aws_ecr(\n  \"000000000000.dkr.ecr.us-east-1.amazonaws.com/my-private-registry:my-version\",\n  secret=modal.Secret.from_name(\"aws\"),\n  setup_dockerfile_commands=[\"RUN apt-get update\", \"RUN apt-get install -y python3-pip\"]\n)\n```\n## from_dockerfile\n```python\n@staticmethod\n@typechecked\ndef from_dockerfile(\n    path: Union[str, Path],\n    context_mount: Optional[\n        _Mount\n    ] = None,  # modal.Mount with local files to supply as build context for COPY commands\n    force_build: bool = False,\n    *,\n    secrets: Sequence[_Secret] = [],\n    gpu: GPU_T = None,\n) -> \"_Image\":\n```\nBuild a Modal image from a local Dockerfile.\nNote that the following must be true about the image you provide:\n- Python 3.7 or above needs to be present and available as `python`.\n- `pip` needs to be installed and available as `pip`.\n## debian_slim\n```python\n@staticmethod\n@typechecked\ndef debian_slim(python_version: Optional[str] = None, force_build: bool = False) -> \"_Image\":\n```\nDefault image, based on the official `python:X.Y.Z-slim-bullseye` Docker images.\n## apt_install\n```python\n@typechecked\ndef apt_install(\n    self,\n    *packages: Union[str, List[str]],  # A list of packages, e.g. [\"ssh\", \"libpq-dev\"]\n    force_build: bool = False,\n    secrets: Sequence[_Secret] = [],\n    gpu: GPU_T = None,\n) -> \"_Image\":\n```\nInstall a list of Debian packages using `apt`.\n**Example**\n```python\nimage = modal.Image.debian_slim().apt_install(\"git\")\n```\n## run_function\n```python\n@typechecked\ndef run_function(\n    self,\n    raw_f: Callable[[], Any],\n    *,\n    secret: Optional[_Secret] = None,  # An optional Modal Secret with environment variables for the container\n    secrets: Sequence[_Secret] = (),  # Plural version of `secret` when multiple secrets are needed\n    gpu: GPU_T = None,  # GPU specification as string (\"any\", \"T4\", \"A10G\", ...) or object (`modal.GPU.A100()`, ...)\n    mounts: Sequence[_Mount] = (),\n    shared_volumes: Dict[Union[str, os.PathLike], _NetworkFileSystem] = {},\n    network_file_systems: Dict[Union[str, os.PathLike], _NetworkFileSystem] = {},\n    cpu: Optional[float] = None,  # How many CPU cores to request. This is a soft limit.\n    memory: Optional[int] = None,  # How much memory to request, in MiB. This is a soft limit.\n    timeout: Optional[int] = 86400,  # Maximum execution time of the function in seconds.\n    force_build: bool = False,\n) -> \"_Image\":\n```\nRun user-defined function `raw_function` as an image build step. The function runs just like an ordinary Modal\nfunction, and any kwargs accepted by `@stub.function` (such as `Mount`s, `NetworkFileSystem`s, and resource requests) can\nbe supplied to it. After it finishes execution, a snapshot of the resulting container file system is saved as an image.\n**Note**\nOnly the source code of `raw_function` and the contents of `**kwargs` are used to determine whether the image has changed\nand needs to be rebuilt. If this function references other functions or variables, the image will not be rebuilt if you\nmake changes to them. You can force a rebuild by changing the function's source code itself.\n**Example**\n```python notest\ndef my_build_function():\n    open(\"model.pt\", \"w\").write(\"parameters!\")\nimage = (\n    modal.Image\n        .debian_slim()\n        .pip_install(\"torch\")\n        .run_function(my_build_function, secrets=[...], mounts=[...])\n)\n```\n## env\n```python\n@typechecked\ndef env(self, vars: Dict[str, str]) -> \"_Image\":\n```\nSets the environmental variables of the image.\n**Example**\n```python\nimage = (\n    modal.Image.conda()\n        .env({\"CONDA_OVERRIDE_CUDA\": \"11.2\"})\n        .conda_install(\"jax\", \"cuda-nvcc\", channels=[\"conda-forge\", \"nvidia\"])\n        .pip_install(\"dm-haiku\", \"optax\")\n)\n```\n## workdir\n```python\n@typechecked\ndef workdir(self, path: str) -> \"_Image\":\n```\nSets the working directory for subequent image build steps.\n**Example**\n```python\nimage = (\n    modal.Image.debian_slim()\n        .run_commands(\"git clone https://xyz app\")\n        .workdir(\"/app\")\n        .run_commands(\"yarn install\")\n)\n```\n"}
{"text": "# modal.Cron\n```python\nclass Cron(modal.schedule.Schedule)\n```\nCron jobs are a type of schedule, specified using the\n[Unix cron tab](https://crontab.guru/) syntax.\nThe alternative schedule type is the [`modal.Period`](/docs/reference/modal.Period).\n**Usage**\n```python\nimport modal\nstub = modal.Stub()\n@stub.function(schedule=modal.Cron(\"* * * * *\"))\ndef f():\n    print(\"This function will run every minute\")\n```\nWe can specify different schedules with cron strings, for example:\n```python\nmodal.Cron(\"5 4 * * *\")  # run at 4:05am every night\nmodal.Cron(\"0 9 * * 4\")  # runs every Thursday 9am\n```\n```python\ndef __init__(self, cron_string: str) -> None:\n```\nConstruct a schedule that runs according to a cron expression string.\n"}
{"text": "# modal.config\nModal intentionally keeps configurability to a minimum.\nThe main configuration options are the API tokens: the token id and the token secret.\nThese can be configured in two ways:\n1. By running the ``modal token set`` command.\n   This writes the tokens to ``.modal.toml`` file in your home directory.\n2. By setting the environment variables ``MODAL_TOKEN_ID`` and ``MODAL_TOKEN_SECRET``.\n   This takes precedence over the previous method.\n.modal.toml\n---------------\nThe ``.modal.toml`` file is generally stored in your home directory.\nIt should look like this::\n```toml\n[default]\ntoken_id = \"ak-12345...\"\ntoken_secret = \"as-12345...\"\n```\nYou can create this file manually, or you can run the ``modal token set ...``\ncommand (see below).\nSetting tokens using the CLI\n----------------------------\nYou can set a token by running the command::\n```bash\nmodal token set \\\n  --token-id <token id> \\\n  --token-secret <token secret>\n```\nThis will write the token id and secret to ``.modal.toml``.\nIf the token id or secret is provided as the string ``-`` (a single dash),\nthen it will be read in a secret way from stdin instead.\nOther configuration options\n---------------------------\nOther possible configuration options are:\n* ``loglevel`` (in the .toml file) / ``MODAL_LOGLEVEL`` (as an env var).\n  Defaults to ``WARNING``.\n  Set this to ``DEBUG`` to see a bunch of internal output.\n* ``logs_timeout`` (in the .toml file) / ``MODAL_LOGS_TIMEOUT`` (as an env var).\n  Defaults to 10.\n  Number of seconds to wait for logs to drain when closing the session,\n  before giving up.\n* ``automount`` (in the .toml file) / ``MODAL_AUTOMOUNT`` (as an env var).\n  Defaults to True.\n  By default, Modal automatically mounts modules imported in the current scope, that\n  are deemed to be \"local\". This can be turned off by setting this to False.\n* ``server_url`` (in the .toml file) / ``MODAL_SERVER_URL`` (as an env var).\n  Defaults to ``https://api.modal.com``.\n  Not typically meant to be used.\nMeta-configuration\n------------------\nSome \"meta-options\" are set using environment variables only:\n* ``MODAL_CONFIG_PATH`` lets you override the location of the .toml file,\n  by default ``~/.modal.toml``.\n* ``MODAL_PROFILE`` lets you use multiple sections in the .toml file\n  and switch between them. It defaults to \"default\".\n## modal.config.Config\n```python\nclass Config(object)\n```\nSingleton that holds configuration used by Modal internally.\n```python\ndef __init__(self):\n```\n### get\n```python\ndef get(self, key, profile=None):\n```\nLooks up a configuration value.\nWill check (in decreasing order of priority):\n1. Any environment variable of the form MODAL_FOO_BAR\n2. Settings in the user's .toml configuration file\n3. The default value of the setting\n### override_locally\n```python\ndef override_locally(self, key: str, value: str):\n    # Override setting in this process by overriding environment variable for the setting\n    #\n    # Does NOT write back to settings file etc.\n```\n### to_dict\n```python\ndef to_dict(self):\n```\n## modal.config.config_profiles\n```python\ndef config_profiles():\n```\nList the available modal profiles in the .modal.toml file.\n## modal.config.config_set_active_profile\n```python\ndef config_set_active_profile(env: str):\n```\nSet the user's active modal profile by writing it to the `.modal.toml` file.\n"}
{"text": "# modal.NetworkFileSystem\n```python\nclass NetworkFileSystem(modal.object.Object)\n```\nA shared, writable file system accessible by one or more Modal functions.\nBy attaching this file system as a mount to one or more functions, they can\nshare and persist data with each other.\n**Usage**\n```python\nimport modal\nvolume = modal.NetworkFileSystem.new()\nstub = modal.Stub()\n@stub.function(network_file_systems={\"/root/foo\": volume})\ndef f():\n    pass\n@stub.function(network_file_systems={\"/root/goo\": volume})\ndef g():\n    pass\n```\nIt is often the case that you would want to persist a network file system object\nseparately from the currently attached app. Refer to the persistence\n[guide section](/docs/guide/network-file-systems#persisting-volumes) to see how to\npersist this object across app runs.\nAlso see the CLI methods for accessing network file systems:\n```bash\nmodal nfs --help\n```\nA `NetworkFileSystem` can also be useful for some local scripting scenarios, e.g.:\n```python notest\nvol = modal.NetworkFileSystem.lookup(\"my-network-file-system\")\nfor chunk in vol.read_file(\"my_db_dump.csv\"):\n    ...\n```\n```python\ndef __init__(self):\n```\n## from_id\n```python\n@classmethod\ndef from_id(cls: Type[O], object_id: str, client: Optional[_Client] = None) -> O:\n```\nGet an object of this type from a unique object id (retrieved from `obj.object_id`)\n## object_id\n```python\n@property\ndef object_id(self):\n```\n## is_hydrated\n```python\ndef is_hydrated(self) -> bool:\n```\n## persist\n```python\ndef persist(\n    self,\n    label: str,\n    namespace=api_pb2.DEPLOYMENT_NAMESPACE_WORKSPACE,\n    environment_name: Optional[str] = None,\n    cloud: Optional[str] = None,\n) -> \"_NetworkFileSystem\":\n```\n`NetworkFileSystem().persist(\"my-volume\")` is deprecated. Use `NetworkFileSystem.persisted(\"my-volume\")` instead.\n## from_name\n```python\n@classmethod\ndef from_name(\n    cls: Type[O],\n    app_name: str,\n    tag: Optional[str] = None,\n    namespace=api_pb2.DEPLOYMENT_NAMESPACE_WORKSPACE,\n    environment_name: Optional[str] = None,\n) -> O:\n```\nReturns a reference to an Modal object of any type\nUseful for referring to already created/deployed objects, e.g., Secrets\n```python\nimport modal\nstub = modal.Stub()\n@stub.function(secret=modal.Secret.from_name(\"my-secret-name\"))\ndef some_function():\n    pass\n```\n## lookup\n```python\n@classmethod\ndef lookup(\n    cls: Type[O],\n    app_name: str,\n    tag: Optional[str] = None,\n    namespace=api_pb2.DEPLOYMENT_NAMESPACE_WORKSPACE,\n    client: Optional[_Client] = None,\n    environment_name: Optional[str] = None,\n) -> O:\n```\nGeneral purpose method to retrieve Modal objects such as functions, network file systems, and secrets.\n```python notest\nimport modal\nsquare = modal.Function.lookup(\"my-shared-app\", \"square\")\nassert square(3) == 9\nnfs = modal.NetworkFileSystem.lookup(\"my-nfs\")\nfor chunk in nfs.read_file(\"my_db_dump.csv\"):\n    ...\n```\n## new\n```python\n@typechecked\n@staticmethod\ndef new(cloud: Optional[str] = None) -> \"_NetworkFileSystem\":\n```\nConstruct a new network file system, which is empty by default.\n## persisted\n```python\n@staticmethod\ndef persisted(\n    label: str,\n    namespace=api_pb2.DEPLOYMENT_NAMESPACE_WORKSPACE,\n    environment_name: Optional[str] = None,\n    cloud: Optional[str] = None,\n) -> \"_NetworkFileSystem\":\n```\nDeploy a Modal app containing this object. This object can then be imported from other apps using\nthe returned reference, or by calling `modal.NetworkFileSystem.from_name(label)` (or the equivalent method\non respective class).\n**Example Usage**\n```python\nimport modal\nvolume = modal.NetworkFileSystem.persisted(\"my-volume\")\nstub = modal.Stub()\n# Volume refers to the same object, even across instances of `stub`.\n@stub.function(network_file_systems={\"/vol\": volume})\ndef f():\n    pass\n```\n## write_file\n```python\ndef write_file(self, remote_path: str, fp: BinaryIO) -> int:\n```\nWrite from a file object to a path on the network file system, atomically.\nWill create any needed parent directories automatically.\nIf remote_path ends with `/` it's assumed to be a directory and the\nfile will be uploaded with its current name to that directory.\n## read_file\n```python\ndef read_file(self, path: str) -> AsyncIterator[bytes]:\n```\nRead a file from the network file system\n## iterdir\n```python\ndef iterdir(self, path: str) -> AsyncIterator[api_pb2.SharedVolumeListFilesEntry]:\n```\nIterate over all files in a directory in the network file system.\n* Passing a directory path lists all files in the directory (names are relative to the directory)\n* Passing a file path returns a list containing only that file's listing description\n* Passing a glob path (including at least one * or ** sequence) returns all files matching that glob path (using absolute paths)\n## add_local_file\n```python\ndef add_local_file(\n    self, local_path: Union[Path, str], remote_path: Optional[Union[str, PurePosixPath, None]] = None\n):\n```\n## add_local_dir\n```python\ndef add_local_dir(\n    self,\n    local_path: Union[Path, str],\n    remote_path: Optional[Union[str, PurePosixPath, None]] = None,\n):\n```\n## listdir\n```python\ndef listdir(self, path: str) -> List[api_pb2.SharedVolumeListFilesEntry]:\n```\nList all files in a directory in the network file system.\n* Passing a directory path lists all files in the directory (names are relative to the directory)\n* Passing a file path returns a list containing only that file's listing description\n* Passing a glob path (including at least one * or ** sequence) returns all files matching that glob path (using absolute paths)\n## remove_file\n```python\ndef remove_file(self, path: str, recursive=False):\n```\nRemove a file in a network file system.\n"}
{"text": "# modal.wsgi_app\n```python\n@typechecked\ndef wsgi_app(\n    label: Optional[str] = None,  # Label for created endpoint. Final subdomain will be <workspace>--<label>.modal.run.\n    wait_for_response: bool = True,  # Whether requests should wait for and return the function response.\n    custom_domains: Optional[\n        Iterable[str]\n    ] = None,  # Create an endpoint using a custom domain fully-qualified domain name.\n) -> Callable[[Callable[..., Any]], _PartialFunction]:\n```\nDecorator for registering a WSGI app with a Modal function.\nWeb Server Gateway Interface (WSGI) is a standard for synchronous Python web apps.\nIt has been [succeeded by the ASGI interface](https://asgi.readthedocs.io/en/latest/introduction.html#wsgi-compatibility) which is compatible with ASGI and supports\nadditional functionality such as web sockets. Modal supports ASGI via [`asgi_app`](/docs/reference/modal.asgi_app).\n**Usage:**\n```python\nfrom typing import Callable\n@stub.function()\n@modal.wsgi_app()\ndef create_wsgi() -> Callable:\n    ...\n```\nTo learn how to use this decorator with popular web frameworks, see the\n[guide on web endpoints](https://modal.com/docs/guide/webhooks).\nFor documentation on this decorator's arguments see [`asgi_app`](/docs/reference/modal.asgi_app).\n"}
{"text": "# modal.Mount\n```python\nclass Mount(modal.object.Object)\n```\nCreate a mount for a local directory or file that can be attached\nto one or more Modal functions.\n**Usage**\n```python\nimport modal\nimport os\nstub = modal.Stub()\n@stub.function(mounts=[modal.Mount.from_local_dir(\"~/foo\", remote_path=\"/root/foo\")])\ndef f():\n    # `/root/foo` has the contents of `~/foo`.\n    print(os.listdir(\"/root/foo/\"))\n```\nModal syncs the contents of the local directory every time the app runs, but uses the hash of\nthe file's contents to skip uploading files that have been uploaded before.\n```python\ndef __init__(self):\n```\n## from_id\n```python\n@classmethod\ndef from_id(cls: Type[O], object_id: str, client: Optional[_Client] = None) -> O:\n```\nGet an object of this type from a unique object id (retrieved from `obj.object_id`)\n## object_id\n```python\n@property\ndef object_id(self):\n```\n## is_hydrated\n```python\ndef is_hydrated(self) -> bool:\n```\n## persist\n```python\ndef persist(\n    self, label: str, namespace=api_pb2.DEPLOYMENT_NAMESPACE_WORKSPACE, environment_name: Optional[str] = None\n):\n```\n`Object.persist` is deprecated for generic objects. See `NetworkFileSystem.persisted` or `Dict.persisted`.\n## from_name\n```python\n@classmethod\ndef from_name(\n    cls: Type[O],\n    app_name: str,\n    tag: Optional[str] = None,\n    namespace=api_pb2.DEPLOYMENT_NAMESPACE_WORKSPACE,\n    environment_name: Optional[str] = None,\n) -> O:\n```\nReturns a reference to an Modal object of any type\nUseful for referring to already created/deployed objects, e.g., Secrets\n```python\nimport modal\nstub = modal.Stub()\n@stub.function(secret=modal.Secret.from_name(\"my-secret-name\"))\ndef some_function():\n    pass\n```\n## lookup\n```python\n@classmethod\ndef lookup(\n    cls: Type[O],\n    app_name: str,\n    tag: Optional[str] = None,\n    namespace=api_pb2.DEPLOYMENT_NAMESPACE_WORKSPACE,\n    client: Optional[_Client] = None,\n    environment_name: Optional[str] = None,\n) -> O:\n```\nGeneral purpose method to retrieve Modal objects such as functions, network file systems, and secrets.\n```python notest\nimport modal\nsquare = modal.Function.lookup(\"my-shared-app\", \"square\")\nassert square(3) == 9\nnfs = modal.NetworkFileSystem.lookup(\"my-nfs\")\nfor chunk in nfs.read_file(\"my_db_dump.csv\"):\n    ...\n```\n## new\n```python\n@staticmethod\ndef new() -> \"_Mount\":\n```\n## entries\n```python\n@property\ndef entries(self):\n```\n## add_local_dir\n```python\n@typechecked\ndef add_local_dir(\n    self,\n    local_path: Union[str, Path],\n    *,\n    remote_path: Union[str, PurePosixPath, None] = None,  # Where the directory is placed within in the mount\n    condition: Optional[\n        Callable[[str], bool]\n    ] = None,  # Filter function for file selection, default to include all files\n    recursive: bool = True,  # add files from subdirectories as well\n) -> \"_Mount\":\n```\n## from_local_dir\n```python\n@staticmethod\n@typechecked\ndef from_local_dir(\n    local_path: Union[str, Path],\n    *,\n    remote_path: Union[str, PurePosixPath, None] = None,  # Where the directory is placed within in the mount\n    condition: Optional[Callable[[str], bool]] = None,  # Filter function for file selection - default all files\n    recursive: bool = True,  # add files from subdirectories as well\n) -> \"_Mount\":\n```\n## add_local_file\n```python\n@typechecked\ndef add_local_file(\n    self, local_path: Union[str, Path], remote_path: Union[str, PurePosixPath, None] = None\n) -> \"_Mount\":\n```\n## from_local_file\n```python\n@staticmethod\n@typechecked\ndef from_local_file(local_path: Union[str, Path], remote_path: Union[str, PurePosixPath, None] = None) -> \"_Mount\":\n```\n## from_local_python_packages\n```python\n@staticmethod\ndef from_local_python_packages(*module_names: str) -> \"_Mount\":\n```\nReturns a `modal.Mount` that makes local modules listed in `module_names` available inside the container.\nThis works by mounting the local path of each module's package to a directory inside the container that's on `PYTHONPATH`.\n**Usage**\n```python notest\nimport modal\nimport my_local_module\nstub = modal.Stub()\n@stub.function(mounts=[\n    modal.Mount.from_local_python_packages(\"my_local_module\", \"my_other_module\"),\n])\ndef f():\n    my_local_module.do_stuff()\n```\n"}
{"text": "# modal.gpu\n**GPU configuration shortcodes**\nThe following are the valid `str` values for the `gpu` parameter of [`@stub.function`](/docs/reference/modal.Stub#function).\n- \"t4\" \u2192 `GPU(T4, count=1)`\n- \"a100\" \u2192 `GPU(A100-40GB, count=1)`\n- \"a100-20g\" \u2192 `GPU(A100-20GB, count=1)`\n- \"a10g\" \u2192 `GPU(A10G, count=1)`\n- \"inf2\" \u2192 `GPU(INFERENTIA2, count=1)`\n- \"any\" \u2192 `GPU(Any, count=1)`\nOther configurations can be created using the constructors documented below.\n## modal.gpu.A100\n```python\nclass A100(modal.gpu._GPUConfig)\n```\n[NVIDIA A100 Tensor Core](https://www.nvidia.com/en-us/data-center/a100/) GPU class.\nThe most powerful GPU available in the cloud. Available in 20GiB and 40GiB GPU memory configurations.\n```python\ndef __init__(\n    self,\n    *,\n    count: int = 1,  # Number of GPUs per container. Defaults to 1. Useful if you have very large models that don't fit on a single GPU.\n    memory: int = 0,  # Set this to 20 if you want to use the 20GB version (with half as many cores)\n):\n```\n## modal.gpu.A10G\n```python\nclass A10G(modal.gpu._GPUConfig)\n```\n[NVIDIA A10G Tensor Core](https://www.nvidia.com/en-us/data-center/products/a10-gpu/) GPU class.\nA10G GPUs deliver up to 3.3x better ML training performance, 3x better ML inference performance,\nand 3x better graphics performance, in comparison to NVIDIA T4 GPUs.\n```python\ndef __init__(\n    self,\n    *,\n    count: int = 1,  # Number of GPUs per container. Defaults to 1. Useful if you have very large models that don't fit on a single GPU.\n):\n```\n## modal.gpu.Any\n```python\nclass Any(modal.gpu._GPUConfig)\n```\nSelects any one of the GPU classes available within Modal, according to availability.\n```python\ndef __init__(self, *, count: int = 1):\n```\n## modal.gpu.T4\n```python\nclass T4(modal.gpu._GPUConfig)\n```\n[NVIDIA T4](https://www.nvidia.com/en-us/data-center/tesla-t4/) GPU class.\nLow-cost GPU option, providing 16GiB of GPU memory.\n```python\ndef __init__(\n    self,\n    count: int = 1,  # Number of GPUs per container. Defaults to 1. Useful if you have very large models that don't fit on a single GPU.\n):\n```\n"}
{"text": "# modal.dict\n## modal.dict.Dict\n```python\nclass Dict(modal.object.Object)\n```\nA distributed dictionary available to Modal apps.\nKeys and values can be essentially any object, so long as it can be\nserialized by `cloudpickle`, including Modal objects.\n**Lifetime of dictionary and its items**\nA `Dict`'s lifetime matches the lifetime of the app it's attached to, but invididual keys expire after 30 days.\nBecause of this, `Dict`s are best used as a cache and not relied on for persistent storage.\nOn app completion or after stopping an app any associated `Dict` objects are cleaned up.\n**Usage**\nThis is the constructor object, used only to attach a `DictHandle` to an app.\nTo interact with `Dict` contents, use `DictHandle` objects that are attached\nto the live app once an app is running.\n```python\nimport modal\nstub = modal.Stub()\nstub.some_dict = modal.Dict.new()\n# stub.some_dict[\"message\"] = \"hello world\" # TypeError!\nif __name__ == \"__main__\":\n    with stub.run() as app:\n        handle = app.some_dict\n        handle[\"message\"] = \"hello world\"  # OK \u2714\ufe0f\n```\n```python\ndef __init__(self, data={}):\n```\n`Dict({...})` is deprecated. Please use `Dict.new({...})` instead.\n### from_id\n```python\n@classmethod\ndef from_id(cls: Type[O], object_id: str, client: Optional[_Client] = None) -> O:\n```\nGet an object of this type from a unique object id (retrieved from `obj.object_id`)\n### object_id\n```python\n@property\ndef object_id(self):\n```\n### is_hydrated\n```python\ndef is_hydrated(self) -> bool:\n```\n### persist\n```python\ndef persist(\n    self, label: str, namespace=api_pb2.DEPLOYMENT_NAMESPACE_WORKSPACE, environment_name: Optional[str] = None\n) -> \"_Dict\":\n```\n`Dict().persist(\"my-dict\")` is deprecated. Use `Dict.persisted(\"my-dict\")` instead.\n### from_name\n```python\n@classmethod\ndef from_name(\n    cls: Type[O],\n    app_name: str,\n    tag: Optional[str] = None,\n    namespace=api_pb2.DEPLOYMENT_NAMESPACE_WORKSPACE,\n    environment_name: Optional[str] = None,\n) -> O:\n```\nReturns a reference to an Modal object of any type\nUseful for referring to already created/deployed objects, e.g., Secrets\n```python\nimport modal\nstub = modal.Stub()\n@stub.function(secret=modal.Secret.from_name(\"my-secret-name\"))\ndef some_function():\n    pass\n```\n### lookup\n```python\n@classmethod\ndef lookup(\n    cls: Type[O],\n    app_name: str,\n    tag: Optional[str] = None,\n    namespace=api_pb2.DEPLOYMENT_NAMESPACE_WORKSPACE,\n    client: Optional[_Client] = None,\n    environment_name: Optional[str] = None,\n) -> O:\n```\nGeneral purpose method to retrieve Modal objects such as functions, network file systems, and secrets.\n```python notest\nimport modal\nsquare = modal.Function.lookup(\"my-shared-app\", \"square\")\nassert square(3) == 9\nnfs = modal.NetworkFileSystem.lookup(\"my-nfs\")\nfor chunk in nfs.read_file(\"my_db_dump.csv\"):\n    ...\n```\n### new\n```python\n@typechecked\n@staticmethod\ndef new(data={}) -> \"_Dict\":\n```\nCreate a new dictionary, optionally filled with initial data.\n### persisted\n```python\n@staticmethod\ndef persisted(\n    label: str, namespace=api_pb2.DEPLOYMENT_NAMESPACE_WORKSPACE, environment_name: Optional[str] = None\n) -> \"_Dict\":\n```\nSee `NetworkFileSystem.persisted`.\n### get\n```python\ndef get(self, key: Any) -> Any:\n```\nGet the value associated with the key.\nRaises `KeyError` if the key does not exist.\n### contains\n```python\ndef contains(self, key: Any) -> bool:\n```\nCheck if the key exists.\n### len\n```python\ndef len(self) -> int:\n```\nReturns the length of the dictionary, including any expired keys.\n### update\n```python\ndef update(self, **kwargs) -> None:\n```\nUpdate the dictionary with additional items.\n### put\n```python\ndef put(self, key: Any, value: Any) -> None:\n```\nAdd a specific key-value pair in the dictionary.\n### pop\n```python\ndef pop(self, key: Any) -> Any:\n```\nRemove a key from the dictionary, returning the value if it exists.\n"}
{"text": "# modal.call_graph\n## modal.call_graph.InputInfo\n```python\nclass InputInfo(object)\n```\nSimple data structure storing information about a function input.\n```python\ndef __init__(self, input_id: str, task_id: str, status: modal.call_graph.InputStatus, function_name: str, module_name: str, children: List[ForwardRef('InputInfo')]) -> None\n```\n## modal.call_graph.InputStatus\n```python\nclass InputStatus(enum.IntEnum)\n```\nEnum representing status of a function input.\nThe possible values are:\n* `PENDING`\n* `SUCCESS`\n* `FAILURE`\n* `TERMINATED`\n* `TIMEOUT`\n"}
{"text": "# modal.asgi_app\n```python\n@typechecked\ndef asgi_app(\n    label: Optional[str] = None,  # Label for created endpoint. Final subdomain will be <workspace>--<label>.modal.run.\n    wait_for_response: bool = True,  # Whether requests should wait for and return the function response.\n    custom_domains: Optional[\n        Iterable[str]\n    ] = None,  # Create an endpoint using a custom domain fully-qualified domain name.\n) -> Callable[[Callable[..., Any]], _PartialFunction]:\n```\nDecorator for registering an ASGI app with a Modal function.\nAsynchronous Server Gateway Interface (ASGI) is a standard for Python\nsynchronous and asynchronous apps, supported by all popular Python web\nlibraries. This is an advanced decorator that gives full flexibility in\ndefining one or more web endpoints on Modal.\n**Usage:**\n```python\nfrom typing import Callable\n@stub.function()\n@modal.asgi_app()\ndef create_asgi() -> Callable:\n    ...\n```\nTo learn how to use Modal with popular web frameworks, see the\n[guide on web endpoints](https://modal.com/docs/guide/webhooks).\nThe two `wait_for_response` modes for webhooks are as follows:\n* wait_for_response=True - tries to fulfill the request on the original URL, but returns a 302 redirect after ~150s to a result URL (original URL with an added `__modal_function_id=fc-1234abcd` query parameter)\n* wait_for_response=False - immediately returns a 202 ACCEPTED response with a JSON payload: `{\"result_url\": \"...\"}` containing the result \"redirect\" url from above (which in turn redirects to itself every 150s)\n"}
{"text": "# modal.Volume\n```python\nclass Volume(modal.object.Object)\n```\nA writeable volume that can be used to share files between one or more Modal functions.\nThe contents of a volume is exposed as a filesystem. You can use it to share data between different functions, or\nto persist durable state across several instances of the same function.\nUnlike a networked filesystem, you need to explicitly reload the volume to see changes made since it was mounted.\nSimilarly, you need to explicitly commit any changes you make to the volume for the changes to become visible\noutside the current container.\nConcurrent modification is supported, but concurrent modifications of the same files should be avoided! Last write\nwins in case of concurrent modification of the same file - any data the last writer didn't have when committing\nchanges will be lost!\nAs a result, volumes are typically not a good fit for use cases where you need to make concurrent modifications to\nthe same file (nor is distributed file locking supported).\nVolumes can only be committed and reloaded if there are no open files for the volume - attempting to reload or\ncommit with open files will result in an error.\n**Usage**\n```python\nimport modal\nstub = modal.Stub()\nstub.volume = modal.Volume.new()\n@stub.function(volumes={\"/root/foo\": stub.volume})\ndef f():\n    with open(\"/root/foo/bar.txt\", \"w\") as f:\n        f.write(\"hello\")\n    stub.app.volume.commit()  # Persist changes\n@stub.function(volumes={\"/root/foo\": stub.volume})\ndef g():\n    stub.app.volume.reload()  # Fetch latest changes\n    with open(\"/root/foo/bar.txt\", \"r\") as f:\n        print(f.read())\n```\n```python\ndef __init__(self):\n```\n## from_id\n```python\n@classmethod\ndef from_id(cls: Type[O], object_id: str, client: Optional[_Client] = None) -> O:\n```\nGet an object of this type from a unique object id (retrieved from `obj.object_id`)\n## object_id\n```python\n@property\ndef object_id(self):\n```\n## is_hydrated\n```python\ndef is_hydrated(self) -> bool:\n```\n## persist\n```python\ndef persist(\n    self, label: str, namespace=api_pb2.DEPLOYMENT_NAMESPACE_WORKSPACE, environment_name: Optional[str] = None\n):\n```\n`Object.persist` is deprecated for generic objects. See `NetworkFileSystem.persisted` or `Dict.persisted`.\n## from_name\n```python\n@classmethod\ndef from_name(\n    cls: Type[O],\n    app_name: str,\n    tag: Optional[str] = None,\n    namespace=api_pb2.DEPLOYMENT_NAMESPACE_WORKSPACE,\n    environment_name: Optional[str] = None,\n) -> O:\n```\nReturns a reference to an Modal object of any type\nUseful for referring to already created/deployed objects, e.g., Secrets\n```python\nimport modal\nstub = modal.Stub()\n@stub.function(secret=modal.Secret.from_name(\"my-secret-name\"))\ndef some_function():\n    pass\n```\n## lookup\n```python\n@classmethod\ndef lookup(\n    cls: Type[O],\n    app_name: str,\n    tag: Optional[str] = None,\n    namespace=api_pb2.DEPLOYMENT_NAMESPACE_WORKSPACE,\n    client: Optional[_Client] = None,\n    environment_name: Optional[str] = None,\n) -> O:\n```\nGeneral purpose method to retrieve Modal objects such as functions, network file systems, and secrets.\n```python notest\nimport modal\nsquare = modal.Function.lookup(\"my-shared-app\", \"square\")\nassert square(3) == 9\nnfs = modal.NetworkFileSystem.lookup(\"my-nfs\")\nfor chunk in nfs.read_file(\"my_db_dump.csv\"):\n    ...\n```\n## new\n```python\n@staticmethod\ndef new() -> \"_Volume\":\n```\nConstruct a new volume, which is empty by default.\n## persisted\n```python\n@staticmethod\ndef persisted(\n    label: str,\n    namespace=api_pb2.DEPLOYMENT_NAMESPACE_WORKSPACE,\n    environment_name: Optional[str] = None,\n) -> \"_Volume\":\n```\nDeploy a Modal app containing this object. This object can then be imported from other apps using\nthe returned reference, or by calling `modal.Volume.from_name(label)` (or the equivalent method\non respective class).\n**Example Usage**\n```python\nimport modal\nvolume = modal.Volume.persisted(\"my-volume\")\nstub = modal.Stub()\n# Volume refers to the same object, even across instances of `stub`.\n@stub.function(volumes={\"/vol\": volume})\ndef f():\n    pass\n```\n## commit\n```python\ndef commit(self):\n```\nCommit changes to the volume and fetch any other changes made to the volume by other containers.\nCommitting always triggers a reload after saving changes.\nIf successful, the changes made are now persisted in durable storage and available to other containers accessing the volume.\nCommitting will fail if there are open files for the volume.\n## reload\n```python\ndef reload(self):\n```\nMake latest committed state of volume available in the running container.\nUncommitted changes to the volume, such as new or modified files, will be preserved during reload. Uncommitted\nchanges will shadow any changes made by other writers - e.g. if you have an uncommitted modified a file that was\nalso updated by another writer you will not see the other change.\nReloading will fail if there are open files for the volume.\n## iterdir\n```python\ndef iterdir(self, path: str) -> AsyncIterator[api_pb2.VolumeListFilesEntry]:\n```\nIterate over all files in a directory in the volume.\n* Passing a directory path lists all files in the directory (names are relative to the directory)\n* Passing a file path returns a list containing only that file's listing description\n* Passing a glob path (including at least one * or ** sequence) returns all files matching that glob path (using absolute paths)\n## listdir\n```python\ndef listdir(self, path: str) -> List[api_pb2.VolumeListFilesEntry]:\n```\nList all files under a path prefix in the modal.Volume.\n* Passing a directory path lists all files in the directory\n* Passing a file path returns a list containing only that file's listing description\n* Passing a glob path (including at least one * or ** sequence) returns all files matching that glob path (using absolute paths)\n"}
{"text": "# modal.sandbox\n## modal.sandbox.LogsReader\n```python\nclass LogsReader(object)\n```\nProvides an interface to buffer and fetch logs from a sandbox stream (`stdout` or `stderr`).\n### read\n```python\ndef read(self) -> str:\n```\nFetch and return contents of the entire stream.\n**Usage**\n```python\nsandbox = stub.app.spawn_sandbox(\"echo\", \"hello\")\nsandbox.wait()\nprint(sandbox.stdout.read())\n```\n## modal.sandbox.Sandbox\n```python\nclass Sandbox(modal.object.Object)\n```\nA `Sandbox` object lets you interact with a running sandbox. This API is similar to Python's\n[asyncio.subprocess.Process](https://docs.python.org/3/library/asyncio-subprocess.html#asyncio.subprocess.Process).\nRefer to the [guide](/docs/guide/sandbox) on how to spawn and use sandboxes.\n```python\ndef __init__(self):\n```\n### from_id\n```python\n@classmethod\ndef from_id(cls: Type[O], object_id: str, client: Optional[_Client] = None) -> O:\n```\nGet an object of this type from a unique object id (retrieved from `obj.object_id`)\n### object_id\n```python\n@property\ndef object_id(self):\n```\n### is_hydrated\n```python\ndef is_hydrated(self) -> bool:\n```\n### persist\n```python\ndef persist(\n    self, label: str, namespace=api_pb2.DEPLOYMENT_NAMESPACE_WORKSPACE, environment_name: Optional[str] = None\n):\n```\n`Object.persist` is deprecated for generic objects. See `NetworkFileSystem.persisted` or `Dict.persisted`.\n### from_name\n```python\n@classmethod\ndef from_name(\n    cls: Type[O],\n    app_name: str,\n    tag: Optional[str] = None,\n    namespace=api_pb2.DEPLOYMENT_NAMESPACE_WORKSPACE,\n    environment_name: Optional[str] = None,\n) -> O:\n```\nReturns a reference to an Modal object of any type\nUseful for referring to already created/deployed objects, e.g., Secrets\n```python\nimport modal\nstub = modal.Stub()\n@stub.function(secret=modal.Secret.from_name(\"my-secret-name\"))\ndef some_function():\n    pass\n```\n### lookup\n```python\n@classmethod\ndef lookup(\n    cls: Type[O],\n    app_name: str,\n    tag: Optional[str] = None,\n    namespace=api_pb2.DEPLOYMENT_NAMESPACE_WORKSPACE,\n    client: Optional[_Client] = None,\n    environment_name: Optional[str] = None,\n) -> O:\n```\nGeneral purpose method to retrieve Modal objects such as functions, network file systems, and secrets.\n```python notest\nimport modal\nsquare = modal.Function.lookup(\"my-shared-app\", \"square\")\nassert square(3) == 9\nnfs = modal.NetworkFileSystem.lookup(\"my-nfs\")\nfor chunk in nfs.read_file(\"my_db_dump.csv\"):\n    ...\n```\n### wait\n```python\ndef wait(self):\n```\nWait for the sandbox to finish running.\n### stdout\n```python\n@property\ndef stdout(self) -> _LogsReader:\n```\n`LogsReader` for the sandbox's stdout stream.\n### stderr\n```python\n@property\ndef stderr(self) -> _LogsReader:\n```\n`LogsReader` for the sandbox's stderr stream.\n### returncode\n```python\n@property\ndef returncode(self) -> Optional[int]:\n```\nReturn code of the sandbox process if it has finished running, else `None`.\n"}
{"text": "# modal.Period\n```python\nclass Period(modal.schedule.Schedule)\n```\nCreate a schedule that runs every given time interval.\n**Usage**\n```python\nimport modal\nstub = modal.Stub()\n@stub.function(schedule=modal.Period(days=1))\ndef f():\n    print(\"This function will run every day\")\nmodal.Period(hours=4)          # runs every 4 hours\nmodal.Period(minutes=15)       # runs every 15 minutes\nmodal.Period(seconds=math.pi)  # runs every 3.141592653589793 seconds\n```\nOnly `seconds` can be a float. All other arguments are integers.\nNote that `days=1` will trigger the function the same time every day.\nThis is not have the same behavior as `seconds=84000` since days have\ndifferent lengths due to daylight savings and leap seconds. Similarly,\nusing `months=1` will trigger the function on the same day each month.\nThis behaves similar to the\n[dateutil](https://dateutil.readthedocs.io/en/latest/relativedelta.html)\npackage.\n```python\ndef __init__(\n    self,\n    years: int = 0,\n    months: int = 0,\n    weeks: int = 0,\n    days: int = 0,\n    hours: int = 0,\n    minutes: int = 0,\n    seconds: float = 0,\n) -> None:\n```\n"}
{"text": "# modal.method\n```python\ndef method(\n    *,\n    # Set this to True if it's a non-generator function returning\n    # a [sync/async] generator object\n    is_generator: Optional[bool] = None,\n) -> Callable[[Callable[..., Any]], _PartialFunction]:\n```\nDecorator for methods that should be transformed into a Modal Function registered against this class's stub.\n**Usage:**\n```python\n@stub.cls(cpu=8)\nclass MyCls:\n    @modal.method()\n    def f(self):\n        ...\n```\n"}
{"text": "# modal.secret\n## modal.secret.Secret\n```python\nclass Secret(modal.object.Object)\n```\nSecrets provide a dictionary of environment variables for images.\nSecrets are a secure way to add credentials and other sensitive information\nto the containers your functions run in. You can create and edit secrets on\n[the dashboard](/secrets), or programmatically from Python code.\nSee [the secrets guide page](/docs/guide/secrets) for more information.\n```python\ndef __init__(self):\n```\n### from_id\n```python\n@classmethod\ndef from_id(cls: Type[O], object_id: str, client: Optional[_Client] = None) -> O:\n```\nGet an object of this type from a unique object id (retrieved from `obj.object_id`)\n### object_id\n```python\n@property\ndef object_id(self):\n```\n### is_hydrated\n```python\ndef is_hydrated(self) -> bool:\n```\n### persist\n```python\ndef persist(\n    self, label: str, namespace=api_pb2.DEPLOYMENT_NAMESPACE_WORKSPACE, environment_name: Optional[str] = None\n):\n```\n`Object.persist` is deprecated for generic objects. See `NetworkFileSystem.persisted` or `Dict.persisted`.\n### from_name\n```python\n@classmethod\ndef from_name(\n    cls: Type[O],\n    app_name: str,\n    tag: Optional[str] = None,\n    namespace=api_pb2.DEPLOYMENT_NAMESPACE_WORKSPACE,\n    environment_name: Optional[str] = None,\n) -> O:\n```\nReturns a reference to an Modal object of any type\nUseful for referring to already created/deployed objects, e.g., Secrets\n```python\nimport modal\nstub = modal.Stub()\n@stub.function(secret=modal.Secret.from_name(\"my-secret-name\"))\ndef some_function():\n    pass\n```\n### lookup\n```python\n@classmethod\ndef lookup(\n    cls: Type[O],\n    app_name: str,\n    tag: Optional[str] = None,\n    namespace=api_pb2.DEPLOYMENT_NAMESPACE_WORKSPACE,\n    client: Optional[_Client] = None,\n    environment_name: Optional[str] = None,\n) -> O:\n```\nGeneral purpose method to retrieve Modal objects such as functions, network file systems, and secrets.\n```python notest\nimport modal\nsquare = modal.Function.lookup(\"my-shared-app\", \"square\")\nassert square(3) == 9\nnfs = modal.NetworkFileSystem.lookup(\"my-nfs\")\nfor chunk in nfs.read_file(\"my_db_dump.csv\"):\n    ...\n```\n### from_dict\n```python\n@typechecked\n@staticmethod\ndef from_dict(\n    env_dict: Dict[\n        str, str\n    ] = {},  # dict of entries to be inserted as environment variables in functions using the secret\n    template_type=\"\",  # internal use only\n):\n```\nCreate a secret from a str-str dictionary.\nUsage:\n```python\n@stub.function(secret=modal.Secret.from_dict({\"FOO\": \"bar\"})\ndef run():\n    print(os.environ[\"FOO\"])\n```\n### from_dotenv\n```python\n@staticmethod\ndef from_dotenv(path=None):\n```\nCreate secrets from a .env file automatically.\nIf no argument is provided, it will use the current working directory as the starting\npoint for finding a `.env` file. Note that it does not use the location of the module\ncalling `Secret.from_dotenv`.\nIf called with an argument, it will use that as a starting point for finding `.env` files.\nIn particular, you can call it like this:\n```python\n@stub.function(secret=modal.Secret.from_dotenv(__file__))\ndef run():\n    print(os.environ[\"USERNAME\"])  # Assumes USERNAME is defined in your .env file\n```\nThis will use the location of the script calling `modal.Secret.from_dotenv` as a\nstarting point for finding the `.env` file.\n"}
{"text": "# modal.web_endpoint\n```python\n@typechecked\ndef web_endpoint(\n    method: str = \"GET\",  # REST method for the created endpoint.\n    label: Optional[str] = None,  # Label for created endpoint. Final subdomain will be <workspace>--<label>.modal.run.\n    wait_for_response: bool = True,  # Whether requests should wait for and return the function response.\n    custom_domains: Optional[\n        Iterable[str]\n    ] = None,  # Create an endpoint using a custom domain fully-qualified domain name.\n) -> Callable[[Callable[..., Any]], _PartialFunction]:\n```\nRegister a basic web endpoint with this application.\nThis is the simple way to create a web endpoint on Modal. The function\nbehaves as a [FastAPI](https://fastapi.tiangolo.com/) handler and should\nreturn a response object to the caller.\nEndpoints created with `@stub.web_endpoint` are meant to be simple, single\nrequest handlers and automatically have\n[CORS](https://developer.mozilla.org/en-US/docs/Web/HTTP/CORS) enabled.\nFor more flexibility, use `@stub.asgi_app`.\nTo learn how to use Modal with popular web frameworks, see the\n[guide on web endpoints](https://modal.com/docs/guide/webhooks).\nAll webhook requests have a 150s maximum request time for the HTTP request itself. However, the underlying functions can\nrun for longer and return results to the caller on completion.\nThe two `wait_for_response` modes for webhooks are as follows:\n* `wait_for_response=True` - tries to fulfill the request on the original URL, but returns a 302 redirect after ~150s to a result URL (original URL with an added `__modal_function_id=...` query parameter)\n* `wait_for_response=False` - immediately returns a 202 ACCEPTED response with a JSON payload: `{\"result_url\": \"...\"}` containing the result \"redirect\" URL from above (which in turn redirects to itself every ~150s)\n"}
{"text": "# modal.Error\n```python\nclass Error(Exception)\n```\nBase error class for all Modal errors.\n**Usage**\n```python notest\nimport modal\ntry:\n    with stub.run():\n        f.call()\nexcept modal.Error:\n    # Catch any exception raised by Modal's systems.\n    print(\"Responding to error...\")\n```\n"}
